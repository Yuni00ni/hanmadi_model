# ğŸ§‘â€ğŸ« KoBERT ë‰´ìŠ¤ ìš”ì•½ í‰ê°€ ì½”ë“œ ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ í•œ ì¤„ì”© ì„¤ëª…!
# (ì‹¤ì œ ì½”ë“œì™€ ì„¤ëª…, ì˜ˆì‹œë¥¼ í•¨ê»˜ ì ì–´ë‘¡ë‹ˆë‹¤)

# 1. í•„ìš”í•œ ë„êµ¬(ë¼ì´ë¸ŒëŸ¬ë¦¬) ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd  # í‘œì²˜ëŸ¼ ìƒê¸´ ë°ì´í„°ë¥¼ ì‰½ê²Œ ë‹¤ë£¨ëŠ” ë„êµ¬ì˜ˆìš”. (ì˜ˆ: ì—‘ì…€ì²˜ëŸ¼)
import numpy as np   # ìˆ«ì ê³„ì‚°ì„ ì‰½ê²Œ í•´ì£¼ëŠ” ë„êµ¬ì˜ˆìš”. (ì˜ˆ: í‰ê· , í•©ê³„ ë“±)
import os            # ì»´í“¨í„° íŒŒì¼ì„ ë‹¤ë£¨ëŠ” ë„êµ¬ì˜ˆìš”. (ì˜ˆ: í´ë”, íŒŒì¼ ì°¾ê¸°)
import re            # ê¸€ìì—ì„œ ê·œì¹™ì„ ì°¾ì•„ ë°”ê¿”ì£¼ëŠ” ë„êµ¬ì˜ˆìš”. (ì˜ˆ: ëŠë‚Œí‘œ ì—†ì• ê¸°)
from sklearn.feature_extraction.text import TfidfVectorizer  # ê¸€ì—ì„œ ì¤‘ìš”í•œ ë‹¨ì–´ë¥¼ ë½‘ì•„ì£¼ëŠ” ë„êµ¬ì˜ˆìš”.
from sklearn.metrics.pairwise import cosine_similarity       # ë‘ ê¸€ì´ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ ì•Œë ¤ì£¼ëŠ” ë„êµ¬ì˜ˆìš”.
from collections import Counter  # ë‹¨ì–´ê°€ ëª‡ ë²ˆ ë‚˜ì™”ëŠ”ì§€ ì„¸ì£¼ëŠ” ë„êµ¬ì˜ˆìš”.
from datetime import datetime    # ì˜¤ëŠ˜ ë‚ ì§œì™€ ì‹œê°„ì„ ì•Œë ¤ì£¼ëŠ” ë„êµ¬ì˜ˆìš”.
from tqdm import tqdm            # ì‘ì—…ì´ ì–¼ë§ˆë‚˜ ì§„í–‰ëëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ë„êµ¬ì˜ˆìš”.
import warnings                  # ê²½ê³  ë©”ì‹œì§€ë¥¼ ì•ˆ ë³´ì´ê²Œ í•´ì£¼ëŠ” ë„êµ¬ì˜ˆìš”.

warnings.filterwarnings("ignore")  # ê²½ê³  ë©”ì‹œì§€ëŠ” ì•ˆ ë³´ì´ê²Œ í•´ìš”. (ì˜ˆ: "ì´ê±´ ìœ„í—˜í•´ìš”!" ê°™ì€ ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°)

print("ğŸš€ KoBERT ë‰´ìŠ¤ ìš”ì•½ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!")  # í”„ë¡œê·¸ë¨ì´ ì‹œì‘ëë‹¤ê³  ì•Œë ¤ì¤˜ìš”.
print("=" * 60)  # =ë¥¼ 60ë²ˆ ë°˜ë³µí•´ì„œ ì¤„ì„ ê·¸ì–´ìš”.

# 2. íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜ ë§Œë“¤ê¸°
# ì˜ˆì‹œ: 'data.csv'ë¼ëŠ” íŒŒì¼ì´ ìˆìœ¼ë©´ í‘œë¡œ ì½ì–´ì™€ìš”

def safe_read_csv(file_path):  # safe_read_csvë¼ëŠ” ì´ë¦„ì˜ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ìš”.
    """
    íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜ì˜ˆìš”.
    ë§Œì•½ ê¸€ìê°€ ê¹¨ì§€ë©´ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œë„ ì½ì–´ë´ìš”.
    """
    try:  # ë¨¼ì € utf-8-sig ë°©ì‹ìœ¼ë¡œ ì½ì–´ë´ìš”.
        return pd.read_csv(file_path, encoding="utf-8-sig")
    except:  # ë§Œì•½ ê¸€ìê°€ ê¹¨ì§€ë©´
        for encoding in ["utf-8", "cp949", "euc-kr"]:  # ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œë„ ì½ì–´ë´ìš”.
            try:
                return pd.read_csv(file_path, encoding=encoding)
            except:
                continue  # ë˜ ì•ˆ ë˜ë©´ ë‹¤ìŒ ë°©ì‹ìœ¼ë¡œ ë„˜ì–´ê°€ìš”.
        raise ValueError(f"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path}")  # ë‹¤ ì•ˆ ë˜ë©´ ì—ëŸ¬ë¥¼ ë‚´ìš”.

# ì˜ˆì‹œ: ì‹¤ì œë¡œ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì•„ë˜ì²˜ëŸ¼ ì‹¤í–‰í•˜ë©´ ì—ëŸ¬ê°€ ë‚˜ìš”
# df = safe_read_csv('data.csv')

# 3. ê¸€ìë¥¼ ì˜ˆì˜ê²Œ ë‹¤ë“¬ëŠ” í•¨ìˆ˜
# ì˜ˆì‹œ: "ì•ˆë…•!!   ë‚˜ëŠ”  ì½”ë”©ì„  ì¢‹ì•„í•´!!" -> "ì•ˆë…• ë‚˜ëŠ” ì½”ë”©ì„ ì¢‹ì•„í•´"
def preprocess_text(text):  # preprocess_textë¼ëŠ” ì´ë¦„ì˜ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ìš”.
    if pd.isna(text) or text == "":  # ë§Œì•½ ê¸€ì´ ì—†ê±°ë‚˜ ë¹„ì–´ ìˆìœ¼ë©´
        return ""  # ë¹ˆ ë¬¸ìì—´ì„ ëŒë ¤ì¤˜ìš”.
    text = str(text).strip()  # ê¸€ì„ ë¬¸ìì—´ë¡œ ë°”ê¾¸ê³ , ì•ë’¤ ë¹ˆì¹¸ì„ ì—†ì• ìš”.
    text = re.sub(r"\s+", " ", text)  # ì—¬ëŸ¬ ì¹¸ ë„ì–´ì“°ê¸°ë¥¼ í•˜ë‚˜ë¡œ ë°”ê¿”ìš”.
    text = re.sub(r"[^\w\sê°€-í£]", "", text)  # í•œê¸€, ì˜ì–´, ìˆ«ì ë¹¼ê³  ë‹¤ ì—†ì• ìš”.
    return text  # ë‹¤ë“¬ì–´ì§„ ê¸€ì„ ëŒë ¤ì¤˜ìš”.

# 4. ë°ì´í„° íŒŒì¼ì—ì„œ ì›ë³¸ ê¸°ì‚¬ì™€ ìš”ì•½ë¬¸ì„ ì½ì–´ì™€ìš”
print("ğŸ“ ë°ì´í„° íŒŒì¼ì—ì„œ ê¸°ì‚¬ì™€ ìš”ì•½ë¬¸ì„ ì½ì–´ì™€ìš”!")  # ì§€ê¸ˆë¶€í„° íŒŒì¼ì„ ì½ëŠ”ë‹¤ê³  ì•Œë ¤ì¤˜ìš”.
try:
    original_df = safe_read_csv("data/crawling_origin.csv")  # ì›ë³¸ ê¸°ì‚¬ íŒŒì¼ì„ ì½ì–´ìš”.
    summary_df = safe_read_csv("data/crawling_origin_with_summary.csv")  # ìš”ì•½ë¬¸ íŒŒì¼ì„ ì½ì–´ìš”.
    print(f"âœ… ë°ì´í„° ì½ê¸° ì„±ê³µ! ì›ë³¸ {len(original_df)}ê°œ, ìš”ì•½ {len(summary_df)}ê°œ")  # ëª‡ ê°œ ì½ì—ˆëŠ”ì§€ ì•Œë ¤ì¤˜ìš”.
except Exception as e:
    print(f"âŒ ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {e}")  # íŒŒì¼ì„ ëª» ì½ìœ¼ë©´ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë³´ì—¬ì¤˜ìš”.
    raise

# 5. ê¸°ì‚¬ì™€ ìš”ì•½ë¬¸ì„ ì˜ˆì˜ê²Œ ë‹¤ë“¬ì–´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ìš”
originals = []  # ì›ë³¸ ê¸°ì‚¬ë“¤ì„ ë‹´ì„ ë¹ˆ ìƒìë¥¼ ë§Œë“¤ì–´ìš”.
summaries = []  # ìš”ì•½ë¬¸ë“¤ì„ ë‹´ì„ ë¹ˆ ìƒìë¥¼ ë§Œë“¤ì–´ìš”.
min_len = min(len(original_df), len(summary_df))  # ë‘˜ ì¤‘ì— ë” ì§§ì€ ê°œìˆ˜ë§Œí¼ë§Œ ë¹„êµí•´ìš”

for i in tqdm(range(min_len), desc="ì „ì²˜ë¦¬"):  # 0ë¶€í„° min_len-1ê¹Œì§€ ë°˜ë³µí•´ìš”. (ì§„í–‰ìƒí™©ë„ ë³´ì—¬ì¤˜ìš”)
    try:
        orig_text = preprocess_text(original_df.iloc[i][0])  # ì›ë³¸ ê¸°ì‚¬ í•œ ì¤„ì„ ì˜ˆì˜ê²Œ ë‹¤ë“¬ì–´ìš”.
        summ_text = preprocess_text(summary_df.iloc[i][0])   # ìš”ì•½ë¬¸ í•œ ì¤„ë„ ì˜ˆì˜ê²Œ ë‹¤ë“¬ì–´ìš”.
        if len(orig_text) >= 10 and len(summ_text) >= 5:  # ë„ˆë¬´ ì§§ì€ ê±´ ë¹¼ê³ 
            originals.append(orig_text)  # ì›ë³¸ ê¸°ì‚¬ ìƒìì— ë„£ì–´ìš”.
            summaries.append(summ_text)  # ìš”ì•½ë¬¸ ìƒìì— ë„£ì–´ìš”.
    except:
        continue  # ì—ëŸ¬ê°€ ë‚˜ë©´ ê·¸ëƒ¥ ë„˜ì–´ê°€ìš”.
print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! {len(originals)}ê°œ ë°ì´í„° ì¤€ë¹„")  # ëª‡ ê°œ ì¤€ë¹„ëëŠ”ì§€ ì•Œë ¤ì¤˜ìš”.

# 6. TF-IDFë¡œ ê¸°ì‚¬ì™€ ìš”ì•½ë¬¸ì´ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ ê³„ì‚°í•´ìš”
print("ğŸ”„ TF-IDFë¡œ ë¹„ìŠ·í•œ ì •ë„ ê³„ì‚° ì¤‘...")
vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2))  # ì¤‘ìš”í•œ ë‹¨ì–´ë¥¼ ë½‘ì•„ì£¼ëŠ” ë„êµ¬ë¥¼ ë§Œë“¤ì–´ìš”.
all_texts = originals + summaries  # ì›ë³¸ê³¼ ìš”ì•½ë¬¸ì„ í•©ì³ì„œ í•œ ì¤„ë¡œ ë§Œë“¤ì–´ìš”.
# ì˜ˆì‹œ: ["ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆë‹¤", "ë°¥ì„ ë¨¹ì—ˆë‹¤ ë‚˜ëŠ”"]
tfidf_matrix = vectorizer.fit_transform(all_texts)  # ê¸€ì„ ìˆ«ìë¡œ ë°”ê¿”ì¤˜ìš”.
orig_matrix = tfidf_matrix[: len(originals)]  # ì›ë³¸ ê¸°ì‚¬ ë¶€ë¶„ë§Œ ì˜ë¼ìš”.
summ_matrix = tfidf_matrix[len(originals) :]  # ìš”ì•½ë¬¸ ë¶€ë¶„ë§Œ ì˜ë¼ìš”.

tfidf_scores = []  # ì ìˆ˜ë¥¼ ë‹´ì„ ë¹ˆ ìƒìë¥¼ ë§Œë“¤ì–´ìš”.
for i in tqdm(range(len(originals)), desc="TF-IDF"):  # ì›ë³¸ ê¸°ì‚¬ ê°œìˆ˜ë§Œí¼ ë°˜ë³µí•´ìš”.
    sim = cosine_similarity(orig_matrix[i], summ_matrix[i])[0][0]  # ië²ˆì§¸ ê¸°ì‚¬ì™€ ìš”ì•½ë¬¸ì´ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ ê³„ì‚°í•´ìš”.
    tfidf_scores.append(sim)  # ì ìˆ˜ë¥¼ ìƒìì— ë„£ì–´ìš”.
tfidf_scores = np.array(tfidf_scores)  # ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆ«ì ë°°ì—´ë¡œ ë°”ê¿”ìš”.
print(f"TF-IDF í‰ê·  ì ìˆ˜: {np.mean(tfidf_scores):.4f}")  # í‰ê·  ì ìˆ˜ë¥¼ ë³´ì—¬ì¤˜ìš”.

# 7. Jaccard ìœ ì‚¬ë„: ê¸°ì‚¬ì™€ ìš”ì•½ë¬¸ì— ê°™ì€ ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ ìˆëŠ”ì§€ ê³„ì‚°í•´ìš”
print("ğŸ”„ Jaccard ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
jaccard_scores = []  # ì ìˆ˜ë¥¼ ë‹´ì„ ë¹ˆ ìƒìë¥¼ ë§Œë“¤ì–´ìš”.
for orig, summ in tqdm(zip(originals, summaries), total=len(originals), desc="Jaccard"):  # ì›ë³¸ê³¼ ìš”ì•½ë¬¸ì„ í•œ ìŒì”© êº¼ë‚´ì„œ
    orig_words = set(orig.split())  # ì›ë³¸ ê¸°ì‚¬ë¥¼ ë‹¨ì–´ë³„ë¡œ ë‚˜ëˆ ì„œ ì¤‘ë³µ ì—†ì´ ëª¨ì•„ìš”.
    summ_words = set(summ.split())  # ìš”ì•½ë¬¸ë„ ë‹¨ì–´ë³„ë¡œ ë‚˜ëˆ ì„œ ì¤‘ë³µ ì—†ì´ ëª¨ì•„ìš”.
    if len(orig_words) == 0 and len(summ_words) == 0:  # ë‘˜ ë‹¤ ë‹¨ì–´ê°€ ì—†ìœ¼ë©´
        jaccard_scores.append(1.0)  # ì™„ì „íˆ ê°™ë‹¤ê³  í•´ìš”.
    elif len(orig_words) == 0 or len(summ_words) == 0:  # í•œìª½ë§Œ ì—†ìœ¼ë©´
        jaccard_scores.append(0.0)  # ì™„ì „íˆ ë‹¤ë¥´ë‹¤ê³  í•´ìš”.
    else:
        intersection = len(orig_words & summ_words)  # ê²¹ì¹˜ëŠ” ë‹¨ì–´ ê°œìˆ˜
        union = len(orig_words | summ_words)         # ì „ì²´ ë‹¨ì–´ ê°œìˆ˜
        jaccard_scores.append(intersection / union)  # ê²¹ì¹˜ëŠ” ë¹„ìœ¨ì„ ì ìˆ˜ë¡œ ë„£ì–´ìš”.
jaccard_scores = np.array(jaccard_scores)  # ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆ«ì ë°°ì—´ë¡œ ë°”ê¿”ìš”.
print(f"Jaccard í‰ê·  ì ìˆ˜: {np.mean(jaccard_scores):.4f}")  # í‰ê·  ì ìˆ˜ë¥¼ ë³´ì—¬ì¤˜ìš”.

# 8. í‚¤ì›Œë“œ ìœ ì‚¬ë„: ê¸°ì‚¬ì™€ ìš”ì•½ë¬¸ì—ì„œ ìì£¼ ë‚˜ì˜¨ ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€ ê³„ì‚°í•´ìš”
print("ğŸ”„ í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
keyword_scores = []  # ì ìˆ˜ë¥¼ ë‹´ì„ ë¹ˆ ìƒìë¥¼ ë§Œë“¤ì–´ìš”.
for orig, summ in tqdm(zip(originals, summaries), total=len(originals), desc="í‚¤ì›Œë“œ"):  # ì›ë³¸ê³¼ ìš”ì•½ë¬¸ì„ í•œ ìŒì”© êº¼ë‚´ì„œ
    orig_words = Counter(orig.split())  # ì›ë³¸ ê¸°ì‚¬ì—ì„œ ë‹¨ì–´ê°€ ëª‡ ë²ˆ ë‚˜ì™”ëŠ”ì§€ ì„¸ìš”.
    summ_words = Counter(summ.split())  # ìš”ì•½ë¬¸ë„ ë‹¨ì–´ê°€ ëª‡ ë²ˆ ë‚˜ì™”ëŠ”ì§€ ì„¸ìš”.
    orig_top = set([word for word, _ in orig_words.most_common(10)])  # ì›ë³¸ì—ì„œ ë§ì´ ë‚˜ì˜¨ 10ê°œ ë‹¨ì–´
    summ_top = set([word for word, _ in summ_words.most_common(10)])  # ìš”ì•½ë¬¸ì—ì„œ ë§ì´ ë‚˜ì˜¨ 10ê°œ ë‹¨ì–´
    if len(orig_top) == 0 and len(summ_top) == 0:  # ë‘˜ ë‹¤ ë‹¨ì–´ê°€ ì—†ìœ¼ë©´
        keyword_scores.append(1.0)  # ì™„ì „íˆ ê°™ë‹¤ê³  í•´ìš”.
    elif len(orig_top) == 0 or len(summ_top) == 0:  # í•œìª½ë§Œ ì—†ìœ¼ë©´
        keyword_scores.append(0.0)  # ì™„ì „íˆ ë‹¤ë¥´ë‹¤ê³  í•´ìš”.
    else:
        intersection = len(orig_top & summ_top)  # ê²¹ì¹˜ëŠ” ë‹¨ì–´ ê°œìˆ˜
        union = len(orig_top | summ_top)         # ì „ì²´ ë‹¨ì–´ ê°œìˆ˜
        keyword_scores.append(intersection / union)  # ê²¹ì¹˜ëŠ” ë¹„ìœ¨ì„ ì ìˆ˜ë¡œ ë„£ì–´ìš”.
keyword_scores = np.array(keyword_scores)  # ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆ«ì ë°°ì—´ë¡œ ë°”ê¿”ìš”.
print(f"í‚¤ì›Œë“œ í‰ê·  ì ìˆ˜: {np.mean(keyword_scores):.4f}")  # í‰ê·  ì ìˆ˜ë¥¼ ë³´ì—¬ì¤˜ìš”.

# 9. ê²°ê³¼ë¥¼ í‘œë¡œ ë§Œë“¤ì–´ ì €ì¥í•´ìš”
import datetime  # ë‚ ì§œì™€ ì‹œê°„ì„ ë‹¤ë£¨ëŠ” ë„êµ¬ë¥¼ í•œ ë²ˆ ë” ë¶ˆëŸ¬ì™€ìš”.
results_df = pd.DataFrame({  # í‘œë¥¼ ë§Œë“¤ì–´ìš”.
    'original_text': originals,  # ì›ë³¸ ê¸°ì‚¬
    'summary_text': summaries,   # ìš”ì•½ë¬¸
    'tfidf_score': tfidf_scores,  # TF-IDF ì ìˆ˜
    'jaccard_score': jaccard_scores,  # Jaccard ì ìˆ˜
    'keyword_score': keyword_scores   # í‚¤ì›Œë“œ ì ìˆ˜
})
filename = f"kobert_eval_easy_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"  # íŒŒì¼ ì´ë¦„ì„ ë‚ ì§œì™€ í•¨ê»˜ ë§Œë“¤ì–´ìš”.
results_df.to_csv(filename, index=False, encoding='utf-8-sig')  # í‘œë¥¼ íŒŒì¼ë¡œ ì €ì¥í•´ìš”.
print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ! íŒŒì¼ëª…: {filename}")  # ì €ì¥ì´ ëë‚¬ë‹¤ê³  ì•Œë ¤ì¤˜ìš”.

# 10. ì˜ˆì‹œë¡œ ê²°ê³¼ 3ê°œë§Œ ë³´ì—¬ì£¼ê¸°
print("\nì˜ˆì‹œ ê²°ê³¼ 3ê°œ!")  # ì´ì œ ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤„ ê±°ì˜ˆìš”.
for i in range(3):  # 0, 1, 2 ì„¸ ë²ˆ ë°˜ë³µí•´ìš”.
    print(f"\nì›ë³¸: {originals[i][:50]}...")  # ì›ë³¸ ê¸°ì‚¬ ì• 50ê¸€ìë§Œ ë³´ì—¬ì¤˜ìš”.
    print(f"ìš”ì•½: {summaries[i][:50]}...")    # ìš”ì•½ë¬¸ ì• 50ê¸€ìë§Œ ë³´ì—¬ì¤˜ìš”.
    print(f"TF-IDF: {tfidf_scores[i]:.2f}, Jaccard: {jaccard_scores[i]:.2f}, í‚¤ì›Œë“œ: {keyword_scores[i]:.2f}")  # ì ìˆ˜ë„ ê°™ì´ ë³´ì—¬ì¤˜ìš”.
