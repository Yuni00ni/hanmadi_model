{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab89b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torch as it is not installed.\n",
      "WARNING: Skipping torchvision as it is not installed.\n",
      "WARNING: Skipping torchaudio as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "ERROR: No matching distribution found for torch\n"
     ]
    }
   ],
   "source": [
    "# âš¡ï¸ PyTorch GPU(CUDA) ë²„ì „ ì„¤ì¹˜ ì•ˆë‚´\n",
    "\n",
    "# ì•„ëž˜ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì„¸ìš”. ì„¤ì¹˜ í›„ ë°˜ë“œì‹œ ì»¤ë„ì„ ìž¬ì‹œìž‘í•´ì•¼ GPU ë²„ì „ì´ ì ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "# 1. ê¸°ì¡´ PyTorch, torchvision, torchaudio ì‚­ì œ\n",
    "# 2. CUDA 12.1ìš© PyTorch ì„¤ì¹˜ (ë³¸ì¸ CUDA ë²„ì „ì— ë§žê²Œ)\n",
    "# 3. ì»¤ë„ ìž¬ì‹œìž‘ (í•„ìˆ˜!)\n",
    "# 4. torch import í›„ GPU ì¸ì‹ í™•ì¸\n",
    "\n",
    "# - CUDA ë“œë¼ì´ë²„ì™€ Toolkitì´ ì„¤ì¹˜ë˜ì–´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "# - ì„¤ì¹˜ í›„ì—ë„ CPUë¡œë§Œ ë™ìž‘í•˜ë©´, ë“œë¼ì´ë²„/í™˜ê²½ ë¬¸ì œì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "# - `nvidia-smi` ëª…ë ¹ì–´ë¡œ GPU ì¸ì‹ ì—¬ë¶€ë¥¼ cmdì—ì„œ í™•ì¸í•˜ì„¸ìš”.\n",
    "\n",
    "%pip uninstall -y torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a13cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "ERROR: No matching distribution found for torch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”§ PyTorch ì„¤ì¹˜ ì˜µì…˜ë“¤ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•„ëž˜ ì˜µì…˜ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹œë„)\n",
    "\n",
    "# ì˜µì…˜ 1: CUDA 12.1 (ìµœì‹ )\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# ì˜µì…˜ 2: CUDA 11.8 (ì•ˆì • ë²„ì „) - ìœ„ ì˜µì…˜ì´ ì•ˆ ë˜ë©´ ì´ê±¸ ì‹œë„\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# ì˜µì…˜ 3: CPU ì „ìš© (GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš°)\n",
    "# %pip install torch torchvision torchaudio\n",
    "\n",
    "# ì˜µì…˜ 4: conda ì‚¬ìš© (pipì´ ì•ˆ ë˜ëŠ” ê²½ìš°)\n",
    "# conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "\n",
    "print(\"ì„¤ì¹˜ ì™„ë£Œ í›„ ë°˜ë“œì‹œ ì»¤ë„ì„ ìž¬ì‹œìž‘í•˜ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9cfc88",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ì•„ëž˜ ì…€ë¡œ GPU ì¸ì‹ í™•ì¸\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA ì‚¬ìš© ê°€ëŠ¥:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ì•„ëž˜ ì…€ë¡œ GPU ì¸ì‹ í™•ì¸\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA ì‚¬ìš© ê°€ëŠ¥:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU ì´ë¦„:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDAê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9820cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ì¡´ PyTorch, torchvision, torchaudio ì‚­ì œ\n",
    "%pip uninstall -y torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA 12.1ìš© PyTorch ì„¤ì¹˜ (ê¶Œìž¥)\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA ì‚¬ìš© ê°€ëŠ¥:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU ì´ë¦„:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDAê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4118b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')\n",
    "\n",
    "# CSV íŒŒì¼ ë¡œë“œ\n",
    "df = pd.read_csv('./data\\crawling_tuned_data.csv')\n",
    "\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
    "print(f\"ì»¬ëŸ¼ëª…: {list(df.columns)}\")\n",
    "print(\"\\nì²« ë²ˆì§¸ ê¸°ì‚¬ ì œëª©:\")\n",
    "print(df['title'].iloc[0])\n",
    "print(\"\\nì²« ë²ˆì§¸ ê¸°ì‚¬ ë³¸ë¬¸ ì¼ë¶€:\")\n",
    "print(df['text'].iloc[0][:200] + \"...\")\n",
    "\n",
    "text = \"ê³¼ê±°ë¥¼ ë– ì˜¬ë ¤ë³´ìž. ë°©ì†¡ì„ ë³´ë˜ ìš°ë¦¬ì˜ ëª¨ìŠµì„. ë…ë³´ì ì¸ ë§¤ì²´ëŠ” TVì˜€ë‹¤. ì˜¨ ê°€ì¡±ì´ ë‘˜ëŸ¬ì•‰ì•„ TVë¥¼ ë´¤ë‹¤. ê°„í˜¹ ê°€ì¡±ë“¤ë¼ë¦¬ ë‰´ìŠ¤ì™€ ë“œë¼ë§ˆ, ì˜ˆëŠ¥ í”„ë¡œê·¸ëž¨ì„ ë‘˜ëŸ¬ì‹¸ê³  ë¦¬ëª¨ì»¨ ìŸíƒˆì „ì´ ë²Œì–´ì§€ê¸°ë„  í–ˆë‹¤. ê°ìž ì„ í˜¸í•˜ëŠ” í”„ë¡œê·¸ëž¨ì„ â€˜ë³¸ë°©â€™ìœ¼ë¡œ ë³´ê¸° ìœ„í•œ ì‹¸ì›€ì´ì—ˆë‹¤. TVê°€ í•œ ëŒ€ì¸ì§€ ë‘ ëŒ€ì¸ì§€ ì—¬ë¶€ë„ ê·¸ëž˜ì„œ ì¤‘ìš”í–ˆë‹¤. ì§€ê¸ˆì€ ì–´ë–¤ê°€. â€˜ì•ˆë°©ê·¹ìž¥â€™ì´ë¼ëŠ” ë§ì€ ì˜›ë§ì´ ëë‹¤. TVê°€ ì—†ëŠ” ì§‘ë„ ë§Žë‹¤. ë¯¸ë””ì–´ì˜ í˜œ íƒì„ ëˆ„ë¦´ ìˆ˜ ìžˆëŠ” ë°©ë²•ì€ ëŠ˜ì–´ë‚¬ë‹¤. ê°ìžì˜ ë°©ì—ì„œ ê°ìžì˜ íœ´ëŒ€í°ìœ¼ë¡œ, ë…¸íŠ¸ë¶ìœ¼ë¡œ, íƒœë¸”ë¦¿ìœ¼ë¡œ ì½˜í…ì¸  ë¥¼ ì¦ê¸´ë‹¤.\"\n",
    "\n",
    "raw_input_ids = tokenizer.encode(text)\n",
    "input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n",
    "\n",
    "summary_ids = model.generate(torch.tensor([input_ids]))\n",
    "tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf97885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or len(text.strip()) == 0:\n",
    "        return \"í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì¼ë¶€ë§Œ ì‚¬ìš© (í† í° ì œí•œ ë•Œë¬¸)\n",
    "    if len(text) > 2000:\n",
    "        text = text[:2000]\n",
    "    \n",
    "    try:\n",
    "        # í† í°í™”\n",
    "        raw_input_ids = tokenizer.encode(text)\n",
    "        input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n",
    "        \n",
    "        # ìš”ì•½ ìƒì„±\n",
    "        summary_ids = model.generate(torch.tensor([input_ids]), max_length=150, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "        \n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        return f\"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
    "\n",
    "# ì²« ë²ˆì§¸ ê¸°ì‚¬ ìš”ì•½ í…ŒìŠ¤íŠ¸\n",
    "first_article = df['title'].iloc[0]\n",
    "print(\"ì›ë³¸ ê¸°ì‚¬:\")\n",
    "print(first_article[:500] + \"...\")\n",
    "print(\"\\nìš”ì•½:\")\n",
    "summary = summarize_text(first_article)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f143e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œ ë°ì´í„°ë¡œ ì—¬ëŸ¬ ê¸°ì‚¬ ìš”ì•½ (ì²˜ìŒ 5ê°œ ê¸°ì‚¬)\n",
    "sample_size = 5\n",
    "sample_df = df.head(sample_size).copy()\n",
    "\n",
    "print(f\"{sample_size}ê°œ ê¸°ì‚¬ ìš”ì•½ ì¤‘...\")\n",
    "summaries = []\n",
    "\n",
    "for idx, row in sample_df.iterrows():\n",
    "    print(f\"ê¸°ì‚¬ {idx + 1} ìš”ì•½ ì¤‘...\")\n",
    "    title = row['title']\n",
    "    content = row['text']\n",
    "    summary = summarize_text(content)\n",
    "    \n",
    "    summaries.append({\n",
    "        'ì›ë³¸_ì œëª©': title,\n",
    "        'ìš”ì•½': summary\n",
    "    })\n",
    "\n",
    "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "summary_df = pd.DataFrame(summaries)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for i, row in summary_df.iterrows():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ê¸°ì‚¬ {i+1}\")\n",
    "    print(f\"ì œëª©: {row['ì›ë³¸_ì œëª©']}\")\n",
    "    print(f\"ìš”ì•½: {row['ìš”ì•½']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "# ìš”ì•½ ê²°ê³¼ë¥¼ CSVë¡œ ì €ìž¥\n",
    "summary_df.to_csv('./data/summaries_news.csv', index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nìš”ì•½ ê²°ê³¼ê°€ 'news_summaries.csv' íŒŒì¼ë¡œ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2665a1",
   "metadata": {},
   "source": [
    "# ì „ì²´ ë°ì´í„° ìš”ì•½ ì‹¤í–‰\n",
    "\n",
    "í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìœ¼ë‹ˆ ì´ì œ ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ ìš”ì•½ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "- GPU ì‚¬ìš© ìµœì í™”\n",
    "- ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ\n",
    "- ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§\n",
    "- ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e108a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU í™•ì¸ ë° ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "print(\"ðŸ” ì‹œìŠ¤í…œ í™˜ê²½ í™•ì¸\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"ðŸš€ GPU ì‚¬ìš©: {gpu_name}\")\n",
    "    print(f\"ðŸ’¾ GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"ðŸŽ Apple MPS GPU ì‚¬ìš©\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    cpu_count = psutil.cpu_count()\n",
    "    ram_gb = psutil.virtual_memory().total / 1024**3\n",
    "    print(f\"ðŸ’» CPU ì‚¬ìš©: {cpu_count}ì½”ì–´\")\n",
    "    print(f\"ðŸ’¾ RAM: {ram_gb:.1f}GB\")\n",
    "\n",
    "# ëª¨ë¸ì„ ì„ íƒëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "print(f\"\\nðŸ“¦ ëª¨ë¸ì„ {device}ë¡œ ì´ë™ ì¤‘...\")\n",
    "model = model.to(device)\n",
    "print(\"âœ… ëª¨ë¸ ì´ë™ ì™„ë£Œ\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸\n",
    "if device.type == 'cuda':\n",
    "    allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
    "    print(f\"ðŸ’¾ í˜„ìž¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {allocated:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7699e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ìµœì í™”ëœ ìš”ì•½ í•¨ìˆ˜\n",
    "from tqdm import tqdm  # ì§„í–‰ í‘œì‹œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "\n",
    "def summarize_batch(texts, batch_size=8, max_length=150):\n",
    "    \"\"\"\n",
    "    ë°°ì¹˜ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ìš”ì•½ ì²˜ë¦¬ (GPU ìµœì í™”)\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    print(f\"ðŸ“Š ì´ {len(texts)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œìž‘\")\n",
    "    print(f\"âš™ï¸ ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "    print(f\"ðŸŽ¯ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"ìš”ì•½ ì§„í–‰\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_summaries = []\n",
    "        \n",
    "        for text in batch_texts:\n",
    "            try:\n",
    "                # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "                if pd.isna(text) or len(str(text).strip()) == 0:\n",
    "                    batch_summaries.append(\"í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                    failed_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ\n",
    "                text = str(text)\n",
    "                if len(text) > 2000:\n",
    "                    text = text[:2000]\n",
    "                \n",
    "                # í† í°í™” ë° ë””ë°”ì´ìŠ¤ ì´ë™\n",
    "                raw_input_ids = tokenizer.encode(text, max_length=1024, truncation=True)\n",
    "                input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n",
    "                input_tensor = torch.tensor([input_ids]).to(device)\n",
    "                \n",
    "                # ìš”ì•½ ìƒì„±\n",
    "                with torch.no_grad():  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "                    summary_ids = model.generate(\n",
    "                        input_tensor,\n",
    "                        max_length=max_length,\n",
    "                        min_length=30,\n",
    "                        num_beams=4,\n",
    "                        early_stopping=True,\n",
    "                        no_repeat_ngram_size=2,\n",
    "                        length_penalty=1.0\n",
    "                    )\n",
    "                \n",
    "                # ë””ì½”ë”©\n",
    "                summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "                batch_summaries.append(summary)\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)[:100]}\"\n",
    "                batch_summaries.append(error_msg)\n",
    "                failed_count += 1\n",
    "                print(f\"âš ï¸ ê¸°ì‚¬ {i + len(batch_summaries)} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        \n",
    "        summaries.extend(batch_summaries)\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë§¤ 10ë°°ì¹˜ë§ˆë‹¤)\n",
    "        if i % (batch_size * 10) == 0:\n",
    "            gc.collect()\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "                allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
    "                print(f\"ðŸ’¾ ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ GPU ì‚¬ìš©ëŸ‰: {allocated:.2f}GB\")\n",
    "        \n",
    "        # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë§¤ 50ë°°ì¹˜ë§ˆë‹¤)\n",
    "        if (i // batch_size + 1) % 50 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            processed = min(i + batch_size, len(texts))\n",
    "            rate = processed / elapsed\n",
    "            remaining = (len(texts) - processed) / rate / 60\n",
    "            print(f\"ðŸ“ˆ ì§„í–‰: {processed}/{len(texts)} ({processed/len(texts)*100:.1f}%) - {rate:.1f}ê°œ/ì´ˆ - ë‚¨ì€ ì‹œê°„: {remaining:.1f}ë¶„\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nâœ… ìš”ì•½ ì™„ë£Œ!\")\n",
    "    print(f\"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time/60:.1f}ë¶„\")\n",
    "    print(f\"âš¡ í‰ê·  ì²˜ë¦¬ ì†ë„: {len(texts)/total_time:.1f}ê°œ/ì´ˆ\")\n",
    "    print(f\"âŒ ì‹¤íŒ¨í•œ ê¸°ì‚¬: {failed_count}ê°œ\")\n",
    "    print(f\"âœ… ì„±ê³µë¥ : {(len(texts)-failed_count)/len(texts)*100:.1f}%\")\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "print(\"ðŸ› ï¸ ë°°ì¹˜ ìš”ì•½ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ (tqdm í¬í•¨)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ë°ì´í„° í™•ì¸ ë° ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
    "print(\"ðŸ“Š ì „ì²´ ë°ì´í„° ë¶„ì„\")\n",
    "print(f\"ì´ ê¸°ì‚¬ ìˆ˜: {len(df):,}ê°œ\")\n",
    "\n",
    "# ìœ íš¨í•œ ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "valid_data = df.dropna(subset=['text']).copy()\n",
    "valid_data = valid_data[valid_data['text'].str.len() > 50]  # ìµœì†Œ 50ìž ì´ìƒ\n",
    "\n",
    "print(f\"ìœ íš¨í•œ ê¸°ì‚¬: {len(valid_data):,}ê°œ\")\n",
    "print(f\"ì œê±°ëœ ê¸°ì‚¬: {len(df) - len(valid_data):,}ê°œ\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„\n",
    "text_lengths = valid_data['text'].str.len()\n",
    "print(f\"\\\\nðŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„:\")\n",
    "print(f\"í‰ê· : {text_lengths.mean():.0f}ìž\")\n",
    "print(f\"ì¤‘ê°„ê°’: {text_lengths.median():.0f}ìž\")\n",
    "print(f\"ìµœëŒ€: {text_lengths.max():,}ìž\")\n",
    "print(f\"ìµœì†Œ: {text_lengths.min():,}ìž\")\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ë³„ ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
    "if device.type == 'cuda':\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    if gpu_memory_gb >= 16:\n",
    "        batch_size = 16\n",
    "    elif gpu_memory_gb >= 8:\n",
    "        batch_size = 12\n",
    "    else:\n",
    "        batch_size = 8\n",
    "elif device.type == 'mps':\n",
    "    batch_size = 6  # MPSëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ\n",
    "else:\n",
    "    batch_size = 4  # CPUëŠ” ìž‘ì€ ë°°ì¹˜\n",
    "\n",
    "print(f\"\\\\nâš™ï¸ ì„¤ì •ëœ ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "\n",
    "# ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°\n",
    "estimated_time = len(valid_data) / batch_size * 2  # ë°°ì¹˜ë‹¹ ì•½ 2ì´ˆ ê°€ì •\n",
    "print(f\"ðŸ“… ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„: {estimated_time/60:.1f}ë¶„\")\n",
    "\n",
    "# ìµœì¢… í™•ì¸\n",
    "print(f\"\\\\nðŸŽ¯ ì²˜ë¦¬ ëŒ€ìƒ: {len(valid_data):,}ê°œ ê¸°ì‚¬\")\n",
    "print(f\"ðŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "print(f\"ðŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„\n",
    "df_to_process = valid_data.copy()\n",
    "print(\"\\\\nâœ… ì „ì²´ ë°ì´í„° ì²˜ë¦¬ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ë°ì´í„° ìš”ì•½ ì‹¤í–‰ (GPU ìµœì í™” ë° ê¸¸ì´ ë¶ˆì¼ì¹˜ ë°©ì§€)\n",
    "print(\"ðŸš€ ì „ì²´ ë°ì´í„° ìš”ì•½ ì‹œìž‘!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ì‹œìž‘ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"ðŸ’» CUDA GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}\")\n",
    "    initial_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
    "    print(f\"ðŸ’¾ ì‹œìž‘ ì „ GPU ë©”ëª¨ë¦¬: {initial_memory:.2f}GB\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"ðŸŽ Apple MPS GPU ì‚¬ìš©\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ CUDA GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì§„í–‰í•©ë‹ˆë‹¤. (Windowsì—ì„œ NVIDIA GPUê°€ ì •ìƒ ì„¤ì¹˜ë˜ì–´ ìžˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”)\")\n",
    "\n",
    "# ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (ìž¬í™•ì¸)\n",
    "model = model.to(device)\n",
    "\n",
    "# ë°ì´í„° ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë° í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "df_to_process = df_to_process.reset_index(drop=True)\n",
    "texts = df_to_process['text'].tolist()\n",
    "\n",
    "# ìš”ì•½ ì‹¤í–‰\n",
    "try:\n",
    "    all_summaries = summarize_batch(\n",
    "        texts=texts,\n",
    "        batch_size=batch_size,\n",
    "        max_length=150\n",
    "    )\n",
    "\n",
    "    print(\"\\nðŸ“Š ìš”ì•½ ê²°ê³¼ ìƒì„± ì¤‘...\")\n",
    "\n",
    "    # ê¸¸ì´ ë¶ˆì¼ì¹˜ ë°©ì§€: ìš”ì•½ ê²°ê³¼ì™€ ë°ì´í„° ê°œìˆ˜ ë§žì¶”ê¸°\n",
    "    if len(all_summaries) != len(df_to_process):\n",
    "        print(f\"â— ìš”ì•½ ê²°ê³¼ ê°œìˆ˜({len(all_summaries)})ì™€ ë°ì´í„° ê°œìˆ˜({len(df_to_process)})ê°€ ë‹¤ë¦…ë‹ˆë‹¤. ë§žì¶°ì„œ ì €ìž¥í•©ë‹ˆë‹¤.\")\n",
    "        min_len = min(len(all_summaries), len(df_to_process))\n",
    "        df_to_process = df_to_process.iloc[:min_len].copy()\n",
    "        all_summaries = all_summaries[:min_len]\n",
    "\n",
    "    # ê²°ê³¼ DataFrame ìƒì„±\n",
    "    result_df = pd.DataFrame({\n",
    "        'ê¸°ì‚¬_ë²ˆí˜¸': range(1, len(df_to_process) + 1),\n",
    "        'ì›ë³¸_ì œëª©': df_to_process['title'].tolist(),\n",
    "        'ì›ë³¸_ë‚´ìš©': df_to_process['text'].tolist(),\n",
    "        'KoBERT_ìš”ì•½': all_summaries,\n",
    "        'ì›ë³¸_ê¸¸ì´': df_to_process['text'].str.len(),\n",
    "        'ìš”ì•½_ê¸¸ì´': pd.Series(all_summaries).str.len(),\n",
    "    })\n",
    "\n",
    "    # ì••ì¶•ë¥  ê³„ì‚°\n",
    "    result_df['ì••ì¶•ë¥ (%)'] = (result_df['ìš”ì•½_ê¸¸ì´'] / result_df['ì›ë³¸_ê¸¸ì´'] * 100).round(1)\n",
    "\n",
    "    # ìš”ì•½ í†µê³„\n",
    "    print(f\"\\nðŸ“ˆ ìš”ì•½ í†µê³„:\")\n",
    "    print(f\"ì´ ì²˜ë¦¬ëœ ê¸°ì‚¬: {len(result_df):,}ê°œ\")\n",
    "    print(f\"í‰ê·  ì••ì¶•ë¥ : {result_df['ì••ì¶•ë¥ (%)'].mean():.1f}%\")\n",
    "    print(f\"í‰ê·  ìš”ì•½ ê¸¸ì´: {result_df['ìš”ì•½_ê¸¸ì´'].mean():.0f}ìž\")\n",
    "\n",
    "    # ì‹¤íŒ¨í•œ ìš”ì•½ í™•ì¸\n",
    "    failed_summaries = result_df[result_df['KoBERT_ìš”ì•½'].str.contains('ì‹¤íŒ¨|ì˜¤ë¥˜|ì—†ìŠµë‹ˆë‹¤')]\n",
    "    print(f\"ì‹¤íŒ¨í•œ ìš”ì•½: {len(failed_summaries)}ê°œ\")\n",
    "\n",
    "    print(\"\\nâœ… ì „ì²´ ë°ì´í„° ìš”ì•½ ì™„ë£Œ!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ìš”ì•½ ê²°ê³¼ ì €ìž¥\n",
    "try:\n",
    "    # íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "    output_path = '/data/kobert_full_news_summaries.csv'\n",
    "    \n",
    "    # CSV íŒŒì¼ë¡œ ì €ìž¥\n",
    "    result_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"ðŸ’¾ ê²°ê³¼ ì €ìž¥ ì™„ë£Œ!\")\n",
    "    print(f\"ðŸ“ íŒŒì¼ ìœ„ì¹˜: {output_path}\")\n",
    "    print(f\"ðŸ“Š ì €ìž¥ëœ ë°ì´í„°: {len(result_df):,}í–‰ Ã— {len(result_df.columns)}ì—´\")\n",
    "    \n",
    "    # íŒŒì¼ í¬ê¸° í™•ì¸\n",
    "    import os\n",
    "    file_size_mb = os.path.getsize(output_path) / 1024 / 1024\n",
    "    print(f\"ðŸ“¦ íŒŒì¼ í¬ê¸°: {file_size_mb:.1f}MB\")\n",
    "    \n",
    "    # ìš”ì•½ í’ˆì§ˆ ë¶„ì„\n",
    "    print(f\"\\\\nðŸ“Š ìš”ì•½ í’ˆì§ˆ ë¶„ì„:\")\n",
    "    \n",
    "    # ì••ì¶•ë¥  ë¶„í¬\n",
    "    compression_ranges = [\n",
    "        (0, 10, \"ë§¤ìš° ë†’ì€ ì••ì¶•\"),\n",
    "        (10, 20, \"ë†’ì€ ì••ì¶•\"), \n",
    "        (20, 30, \"ì ì ˆí•œ ì••ì¶•\"),\n",
    "        (30, 50, \"ë‚®ì€ ì••ì¶•\"),\n",
    "        (50, 100, \"ë§¤ìš° ë‚®ì€ ì••ì¶•\")\n",
    "    ]\n",
    "    \n",
    "    for min_val, max_val, label in compression_ranges:\n",
    "        count = len(result_df[(result_df['ì••ì¶•ë¥ (%)'] >= min_val) & (result_df['ì••ì¶•ë¥ (%)'] < max_val)])\n",
    "        percentage = count / len(result_df) * 100\n",
    "        print(f\"  {label} ({min_val}-{max_val}%): {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\\\nðŸ“ˆ ì „ì²´ í†µê³„:\")\n",
    "    print(result_df[['ì›ë³¸_ê¸¸ì´', 'ìš”ì•½_ê¸¸ì´', 'ì••ì¶•ë¥ (%)']].describe())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ê²°ê³¼ ì €ìž¥ ì¤‘ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b083cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìš”ì•½ ê²°ê³¼ ìƒ˜í”Œ í™•ì¸\n",
    "print(\"ðŸ” ìš”ì•½ ê²°ê³¼ ìƒ˜í”Œ í™•ì¸ (ë¬´ìž‘ìœ„ 3ê°œ)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ë¬´ìž‘ìœ„ë¡œ 3ê°œ ìƒ˜í”Œ ì„ íƒ\n",
    "import random\n",
    "sample_indices = random.sample(range(len(result_df)), min(3, len(result_df)))\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    row = result_df.iloc[idx]\n",
    "    print(f\"\\\\nðŸ“° ìƒ˜í”Œ {i} (ê¸°ì‚¬ ë²ˆí˜¸: {row['ê¸°ì‚¬_ë²ˆí˜¸']})\")\n",
    "    print(f\"ì œëª©: {row['ì›ë³¸_ì œëª©'][:100]}...\")\n",
    "    print(f\"ì›ë³¸ ({row['ì›ë³¸_ê¸¸ì´']}ìž): {row['ì›ë³¸_ë‚´ìš©'][:200]}...\")\n",
    "    print(f\"ìš”ì•½ ({row['ìš”ì•½_ê¸¸ì´']}ìž): {row['KoBERT_ìš”ì•½']}\")\n",
    "    print(f\"ì••ì¶•ë¥ : {row['ì••ì¶•ë¥ (%)']}%\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# ìµœê³ /ìµœì € ì••ì¶•ë¥  ê¸°ì‚¬ í™•ì¸\n",
    "print(f\"\\\\nðŸ† ì••ì¶•ë¥  ê·¹ê°’ ì‚¬ë¡€:\")\n",
    "best_compression = result_df.loc[result_df['ì••ì¶•ë¥ (%)'].idxmin()]\n",
    "worst_compression = result_df.loc[result_df['ì••ì¶•ë¥ (%)'].idxmax()]\n",
    "\n",
    "print(f\"\\\\nðŸ“‰ ìµœê³  ì••ì¶• (ì••ì¶•ë¥ : {best_compression['ì••ì¶•ë¥ (%)']}%):\")\n",
    "print(f\"ì œëª©: {best_compression['ì›ë³¸_ì œëª©'][:100]}...\")\n",
    "print(f\"ìš”ì•½: {best_compression['KoBERT_ìš”ì•½']}\")\n",
    "\n",
    "print(f\"\\\\nðŸ“ˆ ìµœì € ì••ì¶• (ì••ì¶•ë¥ : {worst_compression['ì••ì¶•ë¥ (%)']}%):\")\n",
    "print(f\"ì œëª©: {worst_compression['ì›ë³¸_ì œëª©'][:100]}...\")\n",
    "print(f\"ìš”ì•½: {worst_compression['KoBERT_ìš”ì•½']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02906ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ìž‘ì—… ì™„ë£Œ\n",
    "print(\"\\\\nðŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "gc.collect()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    final_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
    "    max_memory = torch.cuda.max_memory_allocated(device) / 1024**3\n",
    "    print(f\"ðŸ’¾ ìµœì¢… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.2f}GB\")\n",
    "    print(f\"ðŸ’¾ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {max_memory:.2f}GB\")\n",
    "\n",
    "# ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ ì „ì²´ ë‰´ìŠ¤ ë°ì´í„° KoBERT ìš”ì•½ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: {len(result_df):,}ê°œ\")\n",
    "print(f\"ðŸ“ ê²°ê³¼ íŒŒì¼: kobert_full_news_summaries.csv\")\n",
    "print(f\"ðŸ“Š í‰ê·  ì••ì¶•ë¥ : {result_df['ì••ì¶•ë¥ (%)'].mean():.1f}%\")\n",
    "print(f\"â±ï¸ ì‚¬ìš©ëœ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "print(\"\\\\në‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"1. ì €ìž¥ëœ CSV íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”\")\n",
    "print(\"2. í•„ìš”ì‹œ ìš”ì•½ í’ˆì§ˆ í‰ê°€ë¥¼ ì§„í–‰í•˜ì„¸ìš”\")\n",
    "print(\"3. ì›ë³¸ ë°ì´í„°ì™€ ìš”ì•½ ë°ì´í„°ë¥¼ ë¹„êµ ë¶„ì„í•˜ì„¸ìš”\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
