{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3ab89b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.13/site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.13/site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.13/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.13/site-packages (from torchvision) (2.3.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (1.1.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (1.1.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4118b558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: (10952, 3)\n",
      "ì»¬ëŸ¼ëª…: ['original_url', 'title', 'text']\n",
      "\n",
      "ì²« ë²ˆì§¸ ê¸°ì‚¬ ì œëª©:\n",
      "ì´ ëŒ€í†µë ¹, 6ê°œ ì‹œêµ° íŠ¹ë³„ì¬ë‚œ ì§€ì—­ ì„ í¬ â€œì¬ë‚œì— ìŒì£¼ê°€ë¬´ ì—„íˆ ë‹¨ì†â€\n",
      "\n",
      "ì²« ë²ˆì§¸ ê¸°ì‚¬ ë³¸ë¬¸ ì¼ë¶€:\n",
      "ì´ì¬ëª… ëŒ€í†µë ¹ì´ ê²½ê¸°ë„ ê°€í‰ê³¼ ì „ë‚¨ ë‹´ì–‘, ê²½ë‚¨ ì‚°ì²­ ë“± 6ê°œ ì‹œêµ°ì„ íŠ¹ë³„ì¬ë‚œì§€ì—­ìœ¼ë¡œ ì„ í¬í–ˆìŠµë‹ˆë‹¤.   ì´ ëŒ€í†µë ¹ì€ ì¬ë‚œ ìƒí™©ì—ì„œ ë¶€ì ì ˆí•œ ê³µì§ìë“¤ì˜ ì²˜ì‹  ë¬¸ì œë„ ê±°ë¡ í–ˆëŠ”ë°, ì—„í˜¹í•œ í˜„ì¥ì—ì„œ ìŒì£¼ê°€ë¬´ë¥¼ ì¦ê¸°ëŠ” ì •ì‹  ë‚˜ê°„ ê²½ìš°ë„ ìˆì—ˆë‹¤ë©° ì—„íˆ ë‹¨ì†í•˜ë¼ê³  ì£¼ë¬¸í–ˆìŠµë‹ˆë‹¤.   ë³´ë„ì— ì´í¬ì—° ê¸°ìì…ë‹ˆë‹¤.   ë¦¬í¬íŠ¸  ê²½ê¸°ë„ ê°€í‰ê³¼ ì¶©ë‚¨ ì„œì‚° ì˜ˆì‚°.....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TVê°€ ì—†ëŠ” ì§‘ë„ ë§ì•„ì§€ê³  ë¯¸ë””ì–´ì˜ í˜œ íƒì„ ëˆ„ë¦´ ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ëŠ˜ì–´ë‚¬ë‹¤.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')\n",
    "\n",
    "# CSV íŒŒì¼ ë¡œë“œ\n",
    "df = pd.read_csv('/Volumes/1TB/ìŠ¤ì¸ì¬/esgc team project/data/crawling_tuned_data.csv')\n",
    "\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
    "print(f\"ì»¬ëŸ¼ëª…: {list(df.columns)}\")\n",
    "print(\"\\nì²« ë²ˆì§¸ ê¸°ì‚¬ ì œëª©:\")\n",
    "print(df['title'].iloc[0])\n",
    "print(\"\\nì²« ë²ˆì§¸ ê¸°ì‚¬ ë³¸ë¬¸ ì¼ë¶€:\")\n",
    "print(df['text'].iloc[0][:200] + \"...\")\n",
    "\n",
    "text = \"ê³¼ê±°ë¥¼ ë– ì˜¬ë ¤ë³´ì. ë°©ì†¡ì„ ë³´ë˜ ìš°ë¦¬ì˜ ëª¨ìŠµì„. ë…ë³´ì ì¸ ë§¤ì²´ëŠ” TVì˜€ë‹¤. ì˜¨ ê°€ì¡±ì´ ë‘˜ëŸ¬ì•‰ì•„ TVë¥¼ ë´¤ë‹¤. ê°„í˜¹ ê°€ì¡±ë“¤ë¼ë¦¬ ë‰´ìŠ¤ì™€ ë“œë¼ë§ˆ, ì˜ˆëŠ¥ í”„ë¡œê·¸ë¨ì„ ë‘˜ëŸ¬ì‹¸ê³  ë¦¬ëª¨ì»¨ ìŸíƒˆì „ì´ ë²Œì–´ì§€ê¸°ë„  í–ˆë‹¤. ê°ì ì„ í˜¸í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ â€˜ë³¸ë°©â€™ìœ¼ë¡œ ë³´ê¸° ìœ„í•œ ì‹¸ì›€ì´ì—ˆë‹¤. TVê°€ í•œ ëŒ€ì¸ì§€ ë‘ ëŒ€ì¸ì§€ ì—¬ë¶€ë„ ê·¸ë˜ì„œ ì¤‘ìš”í–ˆë‹¤. ì§€ê¸ˆì€ ì–´ë–¤ê°€. â€˜ì•ˆë°©ê·¹ì¥â€™ì´ë¼ëŠ” ë§ì€ ì˜›ë§ì´ ëë‹¤. TVê°€ ì—†ëŠ” ì§‘ë„ ë§ë‹¤. ë¯¸ë””ì–´ì˜ í˜œ íƒì„ ëˆ„ë¦´ ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ëŠ˜ì–´ë‚¬ë‹¤. ê°ìì˜ ë°©ì—ì„œ ê°ìì˜ íœ´ëŒ€í°ìœ¼ë¡œ, ë…¸íŠ¸ë¶ìœ¼ë¡œ, íƒœë¸”ë¦¿ìœ¼ë¡œ ì½˜í…ì¸  ë¥¼ ì¦ê¸´ë‹¤.\"\n",
    "\n",
    "raw_input_ids = tokenizer.encode(text)\n",
    "input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n",
    "\n",
    "summary_ids = model.generate(torch.tensor([input_ids]))\n",
    "tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf97885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ ê¸°ì‚¬:\n",
      "ì´ ëŒ€í†µë ¹, 6ê°œ ì‹œêµ° íŠ¹ë³„ì¬ë‚œ ì§€ì—­ ì„ í¬ â€œì¬ë‚œì— ìŒì£¼ê°€ë¬´ ì—„íˆ ë‹¨ì†â€...\n",
      "\n",
      "ìš”ì•½:\n",
      "ì´ ëŒ€í†µë ¹, 6ê°œ ì‹œêµ° íŠ¹ë³„ì¬ë‚œ ì§€ì—­ ì„ í¬ â€œì¬ë‚œì— ìŒì£¼ê°€ë¬´ ì—„íˆ ë‹¨ì†â€\n",
      "ì´ ëŒ€í†µë ¹, 6ê°œ ì‹œêµ° íŠ¹ë³„ì¬ë‚œ ì§€ì—­ ì„ í¬ â€œì¬ë‚œì— ìŒì£¼ê°€ë¬´ ì—„íˆ ë‹¨ì†â€\n"
     ]
    }
   ],
   "source": [
    "def summarize_text(text):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or len(text.strip()) == 0:\n",
    "        return \"í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì¼ë¶€ë§Œ ì‚¬ìš© (í† í° ì œí•œ ë•Œë¬¸)\n",
    "    if len(text) > 2000:\n",
    "        text = text[:2000]\n",
    "    \n",
    "    try:\n",
    "        # í† í°í™”\n",
    "        raw_input_ids = tokenizer.encode(text)\n",
    "        input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n",
    "        \n",
    "        # ìš”ì•½ ìƒì„±\n",
    "        summary_ids = model.generate(torch.tensor([input_ids]), max_length=150, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "        \n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        return f\"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
    "\n",
    "# ì²« ë²ˆì§¸ ê¸°ì‚¬ ìš”ì•½ í…ŒìŠ¤íŠ¸\n",
    "first_article = df['title'].iloc[0]\n",
    "print(\"ì›ë³¸ ê¸°ì‚¬:\")\n",
    "print(first_article[:500] + \"...\")\n",
    "print(\"\\nìš”ì•½:\")\n",
    "summary = summarize_text(first_article)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85f143e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5ê°œ ê¸°ì‚¬ ìš”ì•½ ì¤‘...\n",
      "ê¸°ì‚¬ 1 ìš”ì•½ ì¤‘...\n",
      "ê¸°ì‚¬ 2 ìš”ì•½ ì¤‘...\n",
      "ê¸°ì‚¬ 2 ìš”ì•½ ì¤‘...\n",
      "ê¸°ì‚¬ 3 ìš”ì•½ ì¤‘...\n",
      "ê¸°ì‚¬ 3 ìš”ì•½ ì¤‘...\n",
      "ê¸°ì‚¬ 4 ìš”ì•½ ì¤‘...\n",
      "ê¸°ì‚¬ 4 ìš”ì•½ ì¤‘...\n",
      "ê¸°ì‚¬ 5 ìš”ì•½ ì¤‘...\n",
      "ê¸°ì‚¬ 5 ìš”ì•½ ì¤‘...\n",
      "\n",
      "==================================================\n",
      "ê¸°ì‚¬ 1\n",
      "ì œëª©: ì´ ëŒ€í†µë ¹, 6ê°œ ì‹œêµ° íŠ¹ë³„ì¬ë‚œ ì§€ì—­ ì„ í¬ â€œì¬ë‚œì— ìŒì£¼ê°€ë¬´ ì—„íˆ ë‹¨ì†â€\n",
      "ìš”ì•½: ì´ì¬ëª… ëŒ€í†µë ¹ì´ ê²½ê¸°ë„ ê°€í‰ê³¼ ì „ë‚¨ ë‹´ì–‘, ê²½ë‚¨ ì‚°ì²­ ë“± 6ê°œ ì‹œêµ°ì„ íŠ¹ë³„ì¬ë‚œì§€ì—­ìœ¼ë¡œ ì„ í¬í•˜ê³  ì—„ì •í•œ ë‹¨ì†ì„ ì£¼ë¬¸í–ˆë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ê¸°ì‚¬ 2\n",
      "ì œëª©: ì‹ ê²½í˜¸ ê²°ì‹¬ ê³µíŒ ì—´ë ¤ ê²€ì°° â€œì§•ì—­ 3ë…„ ìš”êµ¬â€\n",
      "ìš”ì•½: ì‹ ê²½í˜¸ ê°•ì›ë„êµìœ¡ê°ì— ëŒ€í•œ ê²°ì‹¬ê³µíŒì´ ì–´ì œ22ì¼ ì—´ë ¸ëŠ”ë°   ê²€ì°°ì€ ì§•ì—­í˜•ì„ ë‚´ë ¤ì•¼í•œë‹¤ê³  ì£¼ì¥í–ˆê³    ì‹ ê²½í˜¸ ê°•ì›ë„êµìœ¡ê°ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ê¸°ì‚¬ 3\n",
      "ì œëª©: â€˜ê³„ì—„ ì˜¹í˜¸â€™ ê°•ì¤€ìš± ìì§„ ì‚¬í‡´ ìµœë™ì„ â€˜2ì°¨ ê°€í•´â€™ ë°œì–¸ ë“± ë…¼ë€ í™•ì‚°\n",
      "ìš”ì•½: ê°•ì¤€ìš± ëŒ€í†µë ¹ì‹¤ êµ­ë¯¼í†µí•©ë¹„ì„œê´€ì´ ê³„ì—„ ì˜¹í˜¸ ë°œì–¸ ë“±ìœ¼ë¡œ ê²°êµ­ ìì§„ ì‚¬í‡´í–ˆê³ , ìµœë™ì„ ì¸ì‚¬í˜ì‹ ì²˜ì¥ë„ ë°•ì›ìˆœ ì „ ì‹œì¥ ì‚¬ê±´ì„ ë‘ê³  í•œ ê³¼ê±° ë°œì–¸ì´ 2ì°¨ ê°€í•´ ë…¼ë€ì— ì„°ë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ê¸°ì‚¬ 4\n",
      "ì œëª©: ì´ ëŒ€í†µë ¹, ê°•ì„ ìš° ë“± 4ëª… ì²­ë¬¸ë³´ê³ ì„œ ì¬ì†¡ë¶€ ìš”ì²­\n",
      "ìš”ì•½: ì´ì¬ëª… ëŒ€í†µë ¹ì€ êµ­íšŒì— ê°•ì„ ìš° ì—¬ì„±ê°€ì¡±ë¶€ ì¥ê´€ í›„ë³´ìë¥¼ í¬í•¨í•´, êµ­ë°©ë¶€ì™€ êµ­ê°€ë³´í›ˆë¶€, í†µì¼ë¶€ ì¥ê´€ í›„ë³´ìì— ëŒ€í•œ ì¸ì‚¬ì²­ë¬¸ë³´ê³ ì„œë¥¼ ë‚´ì¼ê¹Œì§€ ì¬ì†¡ë¶€í•´ ë‹¬ë¼ê³  ìš”ì²­í–ˆê³   ê°•ìœ ì • ëŒ€í†µë ¹ì‹¤ ëŒ€ë³€ì¸ì€ ë¸Œë¦¬í•‘ì„ í†µí•´ ì´ë²ˆì£¼ ë‚´ ì„ëª…ì„ ë§ˆë¬´ë¦¬í•˜ê³  ì‹ ì†í•œ êµ­ì • ì•ˆì •ì„ ê¾€í•˜ê¸° ìœ„í•´ ê¸°í•œì€ ì˜¤ëŠ” 24ì¼ë¡œ ìš”ì²­í–ˆë‹¤ê³  ë°í˜”ë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ê¸°ì‚¬ 5\n",
      "ì œëª©: 6ê³³ íŠ¹ë³„ì¬ë‚œì§€ì—­ ì„ í¬ â€œê³µì§ì ë¶€ì ì ˆ í–‰ìœ„ ì—„íˆ ë‹¨ì†â€\n",
      "ìš”ì•½: ì¤‘ì•™ì¬ë‚œì•ˆì „ëŒ€ì±…ë³¸ë¶€ëŠ” ê²½ê¸° ê°€í‰, ì¶©ë‚¨ ì„œì‚° ì˜ˆì‚°, ì „ë‚¨ ë‹´ì–‘, ê²½ë‚¨ ì‚°ì²­ í•©ì²œì„ ì§‘ì¤‘í˜¸ìš° í”¼í•´ì§€ì—­ì— ëŒ€í•œ ì‚¬ì „ì¡°ì‚¬ë¥¼ í† ëŒ€ë¡œ ëŒ€í†µë ¹ ì¬ê°€ë¥¼ ë°›ì•„ íŠ¹ë³„ì¬ë‚œì§€ì—­ìœ¼ë¡œ ìš°ì„  ì„ í¬í–ˆë‹¤ê³  ë°í˜”ë‹¤.\n",
      "==================================================\n",
      "\n",
      "ìš”ì•½ ê²°ê³¼ê°€ 'news_summaries.csv' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "ê¸°ì‚¬ 1\n",
      "ì œëª©: ì´ ëŒ€í†µë ¹, 6ê°œ ì‹œêµ° íŠ¹ë³„ì¬ë‚œ ì§€ì—­ ì„ í¬ â€œì¬ë‚œì— ìŒì£¼ê°€ë¬´ ì—„íˆ ë‹¨ì†â€\n",
      "ìš”ì•½: ì´ì¬ëª… ëŒ€í†µë ¹ì´ ê²½ê¸°ë„ ê°€í‰ê³¼ ì „ë‚¨ ë‹´ì–‘, ê²½ë‚¨ ì‚°ì²­ ë“± 6ê°œ ì‹œêµ°ì„ íŠ¹ë³„ì¬ë‚œì§€ì—­ìœ¼ë¡œ ì„ í¬í•˜ê³  ì—„ì •í•œ ë‹¨ì†ì„ ì£¼ë¬¸í–ˆë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ê¸°ì‚¬ 2\n",
      "ì œëª©: ì‹ ê²½í˜¸ ê²°ì‹¬ ê³µíŒ ì—´ë ¤ ê²€ì°° â€œì§•ì—­ 3ë…„ ìš”êµ¬â€\n",
      "ìš”ì•½: ì‹ ê²½í˜¸ ê°•ì›ë„êµìœ¡ê°ì— ëŒ€í•œ ê²°ì‹¬ê³µíŒì´ ì–´ì œ22ì¼ ì—´ë ¸ëŠ”ë°   ê²€ì°°ì€ ì§•ì—­í˜•ì„ ë‚´ë ¤ì•¼í•œë‹¤ê³  ì£¼ì¥í–ˆê³    ì‹ ê²½í˜¸ ê°•ì›ë„êµìœ¡ê°ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ê¸°ì‚¬ 3\n",
      "ì œëª©: â€˜ê³„ì—„ ì˜¹í˜¸â€™ ê°•ì¤€ìš± ìì§„ ì‚¬í‡´ ìµœë™ì„ â€˜2ì°¨ ê°€í•´â€™ ë°œì–¸ ë“± ë…¼ë€ í™•ì‚°\n",
      "ìš”ì•½: ê°•ì¤€ìš± ëŒ€í†µë ¹ì‹¤ êµ­ë¯¼í†µí•©ë¹„ì„œê´€ì´ ê³„ì—„ ì˜¹í˜¸ ë°œì–¸ ë“±ìœ¼ë¡œ ê²°êµ­ ìì§„ ì‚¬í‡´í–ˆê³ , ìµœë™ì„ ì¸ì‚¬í˜ì‹ ì²˜ì¥ë„ ë°•ì›ìˆœ ì „ ì‹œì¥ ì‚¬ê±´ì„ ë‘ê³  í•œ ê³¼ê±° ë°œì–¸ì´ 2ì°¨ ê°€í•´ ë…¼ë€ì— ì„°ë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ê¸°ì‚¬ 4\n",
      "ì œëª©: ì´ ëŒ€í†µë ¹, ê°•ì„ ìš° ë“± 4ëª… ì²­ë¬¸ë³´ê³ ì„œ ì¬ì†¡ë¶€ ìš”ì²­\n",
      "ìš”ì•½: ì´ì¬ëª… ëŒ€í†µë ¹ì€ êµ­íšŒì— ê°•ì„ ìš° ì—¬ì„±ê°€ì¡±ë¶€ ì¥ê´€ í›„ë³´ìë¥¼ í¬í•¨í•´, êµ­ë°©ë¶€ì™€ êµ­ê°€ë³´í›ˆë¶€, í†µì¼ë¶€ ì¥ê´€ í›„ë³´ìì— ëŒ€í•œ ì¸ì‚¬ì²­ë¬¸ë³´ê³ ì„œë¥¼ ë‚´ì¼ê¹Œì§€ ì¬ì†¡ë¶€í•´ ë‹¬ë¼ê³  ìš”ì²­í–ˆê³   ê°•ìœ ì • ëŒ€í†µë ¹ì‹¤ ëŒ€ë³€ì¸ì€ ë¸Œë¦¬í•‘ì„ í†µí•´ ì´ë²ˆì£¼ ë‚´ ì„ëª…ì„ ë§ˆë¬´ë¦¬í•˜ê³  ì‹ ì†í•œ êµ­ì • ì•ˆì •ì„ ê¾€í•˜ê¸° ìœ„í•´ ê¸°í•œì€ ì˜¤ëŠ” 24ì¼ë¡œ ìš”ì²­í–ˆë‹¤ê³  ë°í˜”ë‹¤.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ê¸°ì‚¬ 5\n",
      "ì œëª©: 6ê³³ íŠ¹ë³„ì¬ë‚œì§€ì—­ ì„ í¬ â€œê³µì§ì ë¶€ì ì ˆ í–‰ìœ„ ì—„íˆ ë‹¨ì†â€\n",
      "ìš”ì•½: ì¤‘ì•™ì¬ë‚œì•ˆì „ëŒ€ì±…ë³¸ë¶€ëŠ” ê²½ê¸° ê°€í‰, ì¶©ë‚¨ ì„œì‚° ì˜ˆì‚°, ì „ë‚¨ ë‹´ì–‘, ê²½ë‚¨ ì‚°ì²­ í•©ì²œì„ ì§‘ì¤‘í˜¸ìš° í”¼í•´ì§€ì—­ì— ëŒ€í•œ ì‚¬ì „ì¡°ì‚¬ë¥¼ í† ëŒ€ë¡œ ëŒ€í†µë ¹ ì¬ê°€ë¥¼ ë°›ì•„ íŠ¹ë³„ì¬ë‚œì§€ì—­ìœ¼ë¡œ ìš°ì„  ì„ í¬í–ˆë‹¤ê³  ë°í˜”ë‹¤.\n",
      "==================================================\n",
      "\n",
      "ìš”ì•½ ê²°ê³¼ê°€ 'news_summaries.csv' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ìƒ˜í”Œ ë°ì´í„°ë¡œ ì—¬ëŸ¬ ê¸°ì‚¬ ìš”ì•½ (ì²˜ìŒ 5ê°œ ê¸°ì‚¬)\n",
    "sample_size = 5\n",
    "sample_df = df.head(sample_size).copy()\n",
    "\n",
    "print(f\"{sample_size}ê°œ ê¸°ì‚¬ ìš”ì•½ ì¤‘...\")\n",
    "summaries = []\n",
    "\n",
    "for idx, row in sample_df.iterrows():\n",
    "    print(f\"ê¸°ì‚¬ {idx + 1} ìš”ì•½ ì¤‘...\")\n",
    "    title = row['title']\n",
    "    content = row['text']\n",
    "    summary = summarize_text(content)\n",
    "    \n",
    "    summaries.append({\n",
    "        'ì›ë³¸_ì œëª©': title,\n",
    "        'ìš”ì•½': summary\n",
    "    })\n",
    "\n",
    "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "summary_df = pd.DataFrame(summaries)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for i, row in summary_df.iterrows():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ê¸°ì‚¬ {i+1}\")\n",
    "    print(f\"ì œëª©: {row['ì›ë³¸_ì œëª©']}\")\n",
    "    print(f\"ìš”ì•½: {row['ìš”ì•½']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "# ìš”ì•½ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥\n",
    "summary_df.to_csv('/Volumes/1TB/ìŠ¤ì¸ì¬/esgc team project/news_summaries.csv', index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nìš”ì•½ ê²°ê³¼ê°€ 'news_summaries.csv' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2665a1",
   "metadata": {},
   "source": [
    "# ì „ì²´ ë°ì´í„° ìš”ì•½ ì‹¤í–‰\n",
    "\n",
    "í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìœ¼ë‹ˆ ì´ì œ ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ ìš”ì•½ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "- GPU ì‚¬ìš© ìµœì í™”\n",
    "- ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ\n",
    "- ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§\n",
    "- ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e108a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì‹œìŠ¤í…œ í™˜ê²½ í™•ì¸\n",
      "PyTorch ë²„ì „: 2.7.1\n",
      "CUDA ì‚¬ìš© ê°€ëŠ¥: False\n",
      "ğŸ Apple MPS GPU ì‚¬ìš©\n",
      "\n",
      "ğŸ“¦ ëª¨ë¸ì„ mpsë¡œ ì´ë™ ì¤‘...\n",
      "âœ… ëª¨ë¸ ì´ë™ ì™„ë£Œ\n",
      "âœ… ëª¨ë¸ ì´ë™ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# GPU í™•ì¸ ë° ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "print(\"ğŸ” ì‹œìŠ¤í…œ í™˜ê²½ í™•ì¸\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"ğŸš€ GPU ì‚¬ìš©: {gpu_name}\")\n",
    "    print(f\"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"ğŸ Apple MPS GPU ì‚¬ìš©\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    cpu_count = psutil.cpu_count()\n",
    "    ram_gb = psutil.virtual_memory().total / 1024**3\n",
    "    print(f\"ğŸ’» CPU ì‚¬ìš©: {cpu_count}ì½”ì–´\")\n",
    "    print(f\"ğŸ’¾ RAM: {ram_gb:.1f}GB\")\n",
    "\n",
    "# ëª¨ë¸ì„ ì„ íƒëœ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "print(f\"\\nğŸ“¦ ëª¨ë¸ì„ {device}ë¡œ ì´ë™ ì¤‘...\")\n",
    "model = model.to(device)\n",
    "print(\"âœ… ëª¨ë¸ ì´ë™ ì™„ë£Œ\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸\n",
    "if device.type == 'cuda':\n",
    "    allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
    "    print(f\"ğŸ’¾ í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {allocated:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7699e2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ ë°°ì¹˜ ìš”ì•½ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ (tqdm í¬í•¨)\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²´ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ìµœì í™”ëœ ìš”ì•½ í•¨ìˆ˜\n",
    "from tqdm import tqdm  # ì§„í–‰ í‘œì‹œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "\n",
    "def summarize_batch(texts, batch_size=8, max_length=150):\n",
    "    \"\"\"\n",
    "    ë°°ì¹˜ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ìš”ì•½ ì²˜ë¦¬ (GPU ìµœì í™”)\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    print(f\"ğŸ“Š ì´ {len(texts)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘\")\n",
    "    print(f\"âš™ï¸ ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "    print(f\"ğŸ¯ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"ìš”ì•½ ì§„í–‰\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_summaries = []\n",
    "        \n",
    "        for text in batch_texts:\n",
    "            try:\n",
    "                # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "                if pd.isna(text) or len(str(text).strip()) == 0:\n",
    "                    batch_summaries.append(\"í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                    failed_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ\n",
    "                text = str(text)\n",
    "                if len(text) > 2000:\n",
    "                    text = text[:2000]\n",
    "                \n",
    "                # í† í°í™” ë° ë””ë°”ì´ìŠ¤ ì´ë™\n",
    "                raw_input_ids = tokenizer.encode(text, max_length=1024, truncation=True)\n",
    "                input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n",
    "                input_tensor = torch.tensor([input_ids]).to(device)\n",
    "                \n",
    "                # ìš”ì•½ ìƒì„±\n",
    "                with torch.no_grad():  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "                    summary_ids = model.generate(\n",
    "                        input_tensor,\n",
    "                        max_length=max_length,\n",
    "                        min_length=30,\n",
    "                        num_beams=4,\n",
    "                        early_stopping=True,\n",
    "                        no_repeat_ngram_size=2,\n",
    "                        length_penalty=1.0\n",
    "                    )\n",
    "                \n",
    "                # ë””ì½”ë”©\n",
    "                summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "                batch_summaries.append(summary)\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)[:100]}\"\n",
    "                batch_summaries.append(error_msg)\n",
    "                failed_count += 1\n",
    "                print(f\"âš ï¸ ê¸°ì‚¬ {i + len(batch_summaries)} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        \n",
    "        summaries.extend(batch_summaries)\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë§¤ 10ë°°ì¹˜ë§ˆë‹¤)\n",
    "        if i % (batch_size * 10) == 0:\n",
    "            gc.collect()\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "                allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
    "                print(f\"ğŸ’¾ ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ GPU ì‚¬ìš©ëŸ‰: {allocated:.2f}GB\")\n",
    "        \n",
    "        # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë§¤ 50ë°°ì¹˜ë§ˆë‹¤)\n",
    "        if (i // batch_size + 1) % 50 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            processed = min(i + batch_size, len(texts))\n",
    "            rate = processed / elapsed\n",
    "            remaining = (len(texts) - processed) / rate / 60\n",
    "            print(f\"ğŸ“ˆ ì§„í–‰: {processed}/{len(texts)} ({processed/len(texts)*100:.1f}%) - {rate:.1f}ê°œ/ì´ˆ - ë‚¨ì€ ì‹œê°„: {remaining:.1f}ë¶„\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nâœ… ìš”ì•½ ì™„ë£Œ!\")\n",
    "    print(f\"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time/60:.1f}ë¶„\")\n",
    "    print(f\"âš¡ í‰ê·  ì²˜ë¦¬ ì†ë„: {len(texts)/total_time:.1f}ê°œ/ì´ˆ\")\n",
    "    print(f\"âŒ ì‹¤íŒ¨í•œ ê¸°ì‚¬: {failed_count}ê°œ\")\n",
    "    print(f\"âœ… ì„±ê³µë¥ : {(len(texts)-failed_count)/len(texts)*100:.1f}%\")\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "print(\"ğŸ› ï¸ ë°°ì¹˜ ìš”ì•½ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ (tqdm í¬í•¨)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19ad181b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì „ì²´ ë°ì´í„° ë¶„ì„\n",
      "ì´ ê¸°ì‚¬ ìˆ˜: 10,952ê°œ\n",
      "ìœ íš¨í•œ ê¸°ì‚¬: 10,812ê°œ\n",
      "ì œê±°ëœ ê¸°ì‚¬: 140ê°œ\n",
      "\\nğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„:\n",
      "í‰ê· : 189ì\n",
      "ì¤‘ê°„ê°’: 190ì\n",
      "ìµœëŒ€: 200ì\n",
      "ìµœì†Œ: 52ì\n",
      "\\nâš™ï¸ ì„¤ì •ëœ ë°°ì¹˜ í¬ê¸°: 6\n",
      "ğŸ“… ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„: 60.1ë¶„\n",
      "\\nğŸ¯ ì²˜ë¦¬ ëŒ€ìƒ: 10,812ê°œ ê¸°ì‚¬\n",
      "ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: mps\n",
      "ğŸ“¦ ë°°ì¹˜ í¬ê¸°: 6\n",
      "\\nâœ… ì „ì²´ ë°ì´í„° ì²˜ë¦¬ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ì „ì²´ ë°ì´í„° í™•ì¸ ë° ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
    "print(\"ğŸ“Š ì „ì²´ ë°ì´í„° ë¶„ì„\")\n",
    "print(f\"ì´ ê¸°ì‚¬ ìˆ˜: {len(df):,}ê°œ\")\n",
    "\n",
    "# ìœ íš¨í•œ ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "valid_data = df.dropna(subset=['text']).copy()\n",
    "valid_data = valid_data[valid_data['text'].str.len() > 50]  # ìµœì†Œ 50ì ì´ìƒ\n",
    "\n",
    "print(f\"ìœ íš¨í•œ ê¸°ì‚¬: {len(valid_data):,}ê°œ\")\n",
    "print(f\"ì œê±°ëœ ê¸°ì‚¬: {len(df) - len(valid_data):,}ê°œ\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„\n",
    "text_lengths = valid_data['text'].str.len()\n",
    "print(f\"\\\\nğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„:\")\n",
    "print(f\"í‰ê· : {text_lengths.mean():.0f}ì\")\n",
    "print(f\"ì¤‘ê°„ê°’: {text_lengths.median():.0f}ì\")\n",
    "print(f\"ìµœëŒ€: {text_lengths.max():,}ì\")\n",
    "print(f\"ìµœì†Œ: {text_lengths.min():,}ì\")\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ë³„ ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
    "if device.type == 'cuda':\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    if gpu_memory_gb >= 16:\n",
    "        batch_size = 16\n",
    "    elif gpu_memory_gb >= 8:\n",
    "        batch_size = 12\n",
    "    else:\n",
    "        batch_size = 8\n",
    "elif device.type == 'mps':\n",
    "    batch_size = 6  # MPSëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ\n",
    "else:\n",
    "    batch_size = 4  # CPUëŠ” ì‘ì€ ë°°ì¹˜\n",
    "\n",
    "print(f\"\\\\nâš™ï¸ ì„¤ì •ëœ ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "\n",
    "# ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°\n",
    "estimated_time = len(valid_data) / batch_size * 2  # ë°°ì¹˜ë‹¹ ì•½ 2ì´ˆ ê°€ì •\n",
    "print(f\"ğŸ“… ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„: {estimated_time/60:.1f}ë¶„\")\n",
    "\n",
    "# ìµœì¢… í™•ì¸\n",
    "print(f\"\\\\nğŸ¯ ì²˜ë¦¬ ëŒ€ìƒ: {len(valid_data):,}ê°œ ê¸°ì‚¬\")\n",
    "print(f\"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "print(f\"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„\n",
    "df_to_process = valid_data.copy()\n",
    "print(\"\\\\nâœ… ì „ì²´ ë°ì´í„° ì²˜ë¦¬ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771fdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì „ì²´ ë°ì´í„° ìš”ì•½ ì‹œì‘!\n",
      "============================================================\n",
      "ğŸ“Š ì´ 10812ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘\n",
      "âš™ï¸ ë°°ì¹˜ í¬ê¸°: 6\n",
      "ğŸ¯ ë””ë°”ì´ìŠ¤: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "ìš”ì•½ ì§„í–‰:   1%|          | 20/1802 [04:45<7:42:48, 15.58s/it]"
     ]
    }
   ],
   "source": [
    "# ì „ì²´ ë°ì´í„° ìš”ì•½ ì‹¤í–‰\n",
    "print(\"ğŸš€ ì „ì²´ ë°ì´í„° ìš”ì•½ ì‹œì‘!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ì‹œì‘ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸\n",
    "if device.type == 'cuda':\n",
    "    initial_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
    "    print(f\"ğŸ’¾ ì‹œì‘ ì „ GPU ë©”ëª¨ë¦¬: {initial_memory:.2f}GB\")\n",
    "\n",
    "# ìš”ì•½ ì‹¤í–‰\n",
    "try:\n",
    "    all_summaries = summarize_batch(\n",
    "        texts=df_to_process['text'].tolist(),\n",
    "        batch_size=batch_size,\n",
    "        max_length=150\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\nğŸ“Š ìš”ì•½ ê²°ê³¼ ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    # ê²°ê³¼ DataFrame ìƒì„±\n",
    "    result_df = pd.DataFrame({\n",
    "        'ê¸°ì‚¬_ë²ˆí˜¸': range(1, len(df_to_process) + 1),\n",
    "        'ì›ë³¸_ì œëª©': df_to_process['title'].tolist(),\n",
    "        'ì›ë³¸_ë‚´ìš©': df_to_process['text'].tolist(),\n",
    "        'KoBERT_ìš”ì•½': all_summaries,\n",
    "        'ì›ë³¸_ê¸¸ì´': df_to_process['text'].str.len(),\n",
    "        'ìš”ì•½_ê¸¸ì´': pd.Series(all_summaries).str.len(),\n",
    "    })\n",
    "    \n",
    "    # ì••ì¶•ë¥  ê³„ì‚°\n",
    "    result_df['ì••ì¶•ë¥ (%)'] = (result_df['ìš”ì•½_ê¸¸ì´'] / result_df['ì›ë³¸_ê¸¸ì´'] * 100).round(1)\n",
    "    \n",
    "    # ìš”ì•½ í†µê³„\n",
    "    print(f\"\\\\nğŸ“ˆ ìš”ì•½ í†µê³„:\")\n",
    "    print(f\"ì´ ì²˜ë¦¬ëœ ê¸°ì‚¬: {len(result_df):,}ê°œ\")\n",
    "    print(f\"í‰ê·  ì••ì¶•ë¥ : {result_df['ì••ì¶•ë¥ (%)'].mean():.1f}%\")\n",
    "    print(f\"í‰ê·  ìš”ì•½ ê¸¸ì´: {result_df['ìš”ì•½_ê¸¸ì´'].mean():.0f}ì\")\n",
    "    \n",
    "    # ì‹¤íŒ¨í•œ ìš”ì•½ í™•ì¸\n",
    "    failed_summaries = result_df[result_df['KoBERT_ìš”ì•½'].str.contains('ì‹¤íŒ¨|ì˜¤ë¥˜|ì—†ìŠµë‹ˆë‹¤')]\n",
    "    print(f\"ì‹¤íŒ¨í•œ ìš”ì•½: {len(failed_summaries)}ê°œ\")\n",
    "    \n",
    "    print(\"\\\\nâœ… ì „ì²´ ë°ì´í„° ìš”ì•½ ì™„ë£Œ!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ìš”ì•½ ê²°ê³¼ ì €ì¥\n",
    "try:\n",
    "    # íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "    output_path = '/Volumes/1TB/ìŠ¤ì¸ì¬/esgc team project/kobert_full_news_summaries.csv'\n",
    "    \n",
    "    # CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "    result_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“ íŒŒì¼ ìœ„ì¹˜: {output_path}\")\n",
    "    print(f\"ğŸ“Š ì €ì¥ëœ ë°ì´í„°: {len(result_df):,}í–‰ Ã— {len(result_df.columns)}ì—´\")\n",
    "    \n",
    "    # íŒŒì¼ í¬ê¸° í™•ì¸\n",
    "    import os\n",
    "    file_size_mb = os.path.getsize(output_path) / 1024 / 1024\n",
    "    print(f\"ğŸ“¦ íŒŒì¼ í¬ê¸°: {file_size_mb:.1f}MB\")\n",
    "    \n",
    "    # ìš”ì•½ í’ˆì§ˆ ë¶„ì„\n",
    "    print(f\"\\\\nğŸ“Š ìš”ì•½ í’ˆì§ˆ ë¶„ì„:\")\n",
    "    \n",
    "    # ì••ì¶•ë¥  ë¶„í¬\n",
    "    compression_ranges = [\n",
    "        (0, 10, \"ë§¤ìš° ë†’ì€ ì••ì¶•\"),\n",
    "        (10, 20, \"ë†’ì€ ì••ì¶•\"), \n",
    "        (20, 30, \"ì ì ˆí•œ ì••ì¶•\"),\n",
    "        (30, 50, \"ë‚®ì€ ì••ì¶•\"),\n",
    "        (50, 100, \"ë§¤ìš° ë‚®ì€ ì••ì¶•\")\n",
    "    ]\n",
    "    \n",
    "    for min_val, max_val, label in compression_ranges:\n",
    "        count = len(result_df[(result_df['ì••ì¶•ë¥ (%)'] >= min_val) & (result_df['ì••ì¶•ë¥ (%)'] < max_val)])\n",
    "        percentage = count / len(result_df) * 100\n",
    "        print(f\"  {label} ({min_val}-{max_val}%): {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\\\nğŸ“ˆ ì „ì²´ í†µê³„:\")\n",
    "    print(result_df[['ì›ë³¸_ê¸¸ì´', 'ìš”ì•½_ê¸¸ì´', 'ì••ì¶•ë¥ (%)']].describe())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b083cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìš”ì•½ ê²°ê³¼ ìƒ˜í”Œ í™•ì¸\n",
    "print(\"ğŸ” ìš”ì•½ ê²°ê³¼ ìƒ˜í”Œ í™•ì¸ (ë¬´ì‘ìœ„ 3ê°œ)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ë¬´ì‘ìœ„ë¡œ 3ê°œ ìƒ˜í”Œ ì„ íƒ\n",
    "import random\n",
    "sample_indices = random.sample(range(len(result_df)), min(3, len(result_df)))\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    row = result_df.iloc[idx]\n",
    "    print(f\"\\\\nğŸ“° ìƒ˜í”Œ {i} (ê¸°ì‚¬ ë²ˆí˜¸: {row['ê¸°ì‚¬_ë²ˆí˜¸']})\")\n",
    "    print(f\"ì œëª©: {row['ì›ë³¸_ì œëª©'][:100]}...\")\n",
    "    print(f\"ì›ë³¸ ({row['ì›ë³¸_ê¸¸ì´']}ì): {row['ì›ë³¸_ë‚´ìš©'][:200]}...\")\n",
    "    print(f\"ìš”ì•½ ({row['ìš”ì•½_ê¸¸ì´']}ì): {row['KoBERT_ìš”ì•½']}\")\n",
    "    print(f\"ì••ì¶•ë¥ : {row['ì••ì¶•ë¥ (%)']}%\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# ìµœê³ /ìµœì € ì••ì¶•ë¥  ê¸°ì‚¬ í™•ì¸\n",
    "print(f\"\\\\nğŸ† ì••ì¶•ë¥  ê·¹ê°’ ì‚¬ë¡€:\")\n",
    "best_compression = result_df.loc[result_df['ì••ì¶•ë¥ (%)'].idxmin()]\n",
    "worst_compression = result_df.loc[result_df['ì••ì¶•ë¥ (%)'].idxmax()]\n",
    "\n",
    "print(f\"\\\\nğŸ“‰ ìµœê³  ì••ì¶• (ì••ì¶•ë¥ : {best_compression['ì••ì¶•ë¥ (%)']}%):\")\n",
    "print(f\"ì œëª©: {best_compression['ì›ë³¸_ì œëª©'][:100]}...\")\n",
    "print(f\"ìš”ì•½: {best_compression['KoBERT_ìš”ì•½']}\")\n",
    "\n",
    "print(f\"\\\\nğŸ“ˆ ìµœì € ì••ì¶• (ì••ì¶•ë¥ : {worst_compression['ì••ì¶•ë¥ (%)']}%):\")\n",
    "print(f\"ì œëª©: {worst_compression['ì›ë³¸_ì œëª©'][:100]}...\")\n",
    "print(f\"ìš”ì•½: {worst_compression['KoBERT_ìš”ì•½']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02906ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ì‘ì—… ì™„ë£Œ\n",
    "print(\"\\\\nğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "gc.collect()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    final_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
    "    max_memory = torch.cuda.max_memory_allocated(device) / 1024**3\n",
    "    print(f\"ğŸ’¾ ìµœì¢… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.2f}GB\")\n",
    "    print(f\"ğŸ’¾ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {max_memory:.2f}GB\")\n",
    "\n",
    "# ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ ì „ì²´ ë‰´ìŠ¤ ë°ì´í„° KoBERT ìš”ì•½ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: {len(result_df):,}ê°œ\")\n",
    "print(f\"ğŸ“ ê²°ê³¼ íŒŒì¼: kobert_full_news_summaries.csv\")\n",
    "print(f\"ğŸ“Š í‰ê·  ì••ì¶•ë¥ : {result_df['ì••ì¶•ë¥ (%)'].mean():.1f}%\")\n",
    "print(f\"â±ï¸ ì‚¬ìš©ëœ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "print(\"\\\\në‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"1. ì €ì¥ëœ CSV íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”\")\n",
    "print(\"2. í•„ìš”ì‹œ ìš”ì•½ í’ˆì§ˆ í‰ê°€ë¥¼ ì§„í–‰í•˜ì„¸ìš”\")\n",
    "print(\"3. ì›ë³¸ ë°ì´í„°ì™€ ìš”ì•½ ë°ì´í„°ë¥¼ ë¹„êµ ë¶„ì„í•˜ì„¸ìš”\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
