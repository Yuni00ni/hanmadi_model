{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e925ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch\n",
    "# KoBART íŒŒì¸íŠœë‹\n",
    "# ìƒì„± ìš”ì•½ ì‹¤í—˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9684c861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ CPU í™˜ê²½ìš© KoBERT í‰ê°€ ì‹œìŠ¤í…œ ì‹œì‘\n",
      "============================================================\n",
      "ğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì›ë³¸ 10,952ê°œ, ìš”ì•½ 9,066ê°œ\n",
      "ğŸ“‹ ì‚¬ìš© ì»¬ëŸ¼: ì›ë³¸='ë³¸ë¬¸', ìš”ì•½='ìš”ì•½ë¬¸'\n",
      "\n",
      "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8634.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: 9,022ê°œ ìœ íš¨ ë°ì´í„°\n",
      "\n",
      "ğŸ§® ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\n",
      "ğŸ”„ TF-IDF ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:04<00:00, 1950.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Jaccard ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jaccard: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:00<00:00, 92682.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ í‚¤ì›Œë“œ ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í‚¤ì›Œë“œ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:00<00:00, 35170.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë“  ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ!\n",
      "   TF-IDF í‰ê· : 0.0361\n",
      "   Jaccard í‰ê· : 0.0129\n",
      "   í‚¤ì›Œë“œ í‰ê· : 0.0152\n",
      "\n",
      "ğŸ¯ ì¢…í•© í‰ê·  ì ìˆ˜: 0.0250\n",
      "âœ… ê²°ê³¼ ì €ì¥: cpu_evaluation_results_20250805_132544.csv\n",
      "ğŸŠ CPU í‰ê°€ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ CPU í™˜ê²½ìš© ê°„ì†Œí™” ì‹¤í–‰ (ì „ì²´ í”„ë¡œì„¸ìŠ¤ í•œ ë²ˆì—)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"ğŸš€ CPU í™˜ê²½ìš© KoBERT í‰ê°€ ì‹œìŠ¤í…œ ì‹œì‘\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ë“¤\n",
    "def safe_read_csv(file_path):\n",
    "    \"\"\"ì•ˆì „í•œ CSV ì½ê¸°\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "    except:\n",
    "        for encoding in [\"utf-8\", \"cp949\", \"euc-kr\"]:\n",
    "            try:\n",
    "                return pd.read_csv(file_path, encoding=encoding)\n",
    "            except:\n",
    "                continue\n",
    "        raise ValueError(f\"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path}\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"[^\\w\\sê°€-í£]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# 2. ë°ì´í„° ë¡œë“œ\n",
    "print(\"ğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "try:\n",
    "    original_df = safe_read_csv(\"data/crawling_origin.csv\")\n",
    "    summary_df = safe_read_csv(\"data/crawling_origin_with_summary.csv\")\n",
    "\n",
    "    # ì»¬ëŸ¼ ìë™ ê°ì§€\n",
    "    original_col = next(\n",
    "        (col for col in original_df.columns if \"ë³¸ë¬¸\" in col or \"content\" in col),\n",
    "        original_df.columns[0],\n",
    "    )\n",
    "    summary_col = next(\n",
    "        (col for col in summary_df.columns if \"ìš”ì•½\" in col or \"summary\" in col),\n",
    "        summary_df.columns[0],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì›ë³¸ {len(original_df):,}ê°œ, ìš”ì•½ {len(summary_df):,}ê°œ\"\n",
    "    )\n",
    "    print(f\"ğŸ“‹ ì‚¬ìš© ì»¬ëŸ¼: ì›ë³¸='{original_col}', ìš”ì•½='{summary_col}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "print(\"\\nğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "originals = []\n",
    "summaries = []\n",
    "min_len = min(len(original_df), len(summary_df))\n",
    "\n",
    "for i in tqdm(range(min_len), desc=\"ì „ì²˜ë¦¬\"):\n",
    "    try:\n",
    "        orig_text = preprocess_text(original_df.iloc[i][original_col])\n",
    "        summ_text = preprocess_text(summary_df.iloc[i][summary_col])\n",
    "\n",
    "        if len(orig_text) >= 10 and len(summ_text) >= 5:\n",
    "            originals.append(orig_text)\n",
    "            summaries.append(summ_text)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "valid_count = len(originals)\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {valid_count:,}ê°œ ìœ íš¨ ë°ì´í„°\")\n",
    "\n",
    "# 4. ìœ ì‚¬ë„ ê³„ì‚° (CPU ìµœì í™”)\n",
    "print(\"\\nğŸ§® ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\")\n",
    "\n",
    "# TF-IDF ìœ ì‚¬ë„\n",
    "print(\"ğŸ”„ TF-IDF ê³„ì‚° ì¤‘...\")\n",
    "vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2))\n",
    "all_texts = originals + summaries\n",
    "tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "orig_matrix = tfidf_matrix[: len(originals)]\n",
    "summ_matrix = tfidf_matrix[len(originals) :]\n",
    "\n",
    "tfidf_scores = []\n",
    "for i in tqdm(range(len(originals)), desc=\"TF-IDF\"):\n",
    "    sim = cosine_similarity(orig_matrix[i], summ_matrix[i])[0][0]\n",
    "    tfidf_scores.append(sim)\n",
    "tfidf_scores = np.array(tfidf_scores)\n",
    "\n",
    "# Jaccard ìœ ì‚¬ë„\n",
    "print(\"ğŸ”„ Jaccard ê³„ì‚° ì¤‘...\")\n",
    "jaccard_scores = []\n",
    "for orig, summ in tqdm(zip(originals, summaries), total=len(originals), desc=\"Jaccard\"):\n",
    "    orig_words = set(orig.split())\n",
    "    summ_words = set(summ.split())\n",
    "\n",
    "    if len(orig_words) == 0 and len(summ_words) == 0:\n",
    "        jaccard_scores.append(1.0)\n",
    "    elif len(orig_words) == 0 or len(summ_words) == 0:\n",
    "        jaccard_scores.append(0.0)\n",
    "    else:\n",
    "        intersection = len(orig_words & summ_words)\n",
    "        union = len(orig_words | summ_words)\n",
    "        jaccard_scores.append(intersection / union)\n",
    "jaccard_scores = np.array(jaccard_scores)\n",
    "\n",
    "# í‚¤ì›Œë“œ ìœ ì‚¬ë„\n",
    "print(\"ğŸ”„ í‚¤ì›Œë“œ ê³„ì‚° ì¤‘...\")\n",
    "keyword_scores = []\n",
    "for orig, summ in tqdm(zip(originals, summaries), total=len(originals), desc=\"í‚¤ì›Œë“œ\"):\n",
    "    orig_words = Counter(orig.split())\n",
    "    summ_words = Counter(summ.split())\n",
    "\n",
    "    orig_top = set([word for word, _ in orig_words.most_common(10)])\n",
    "    summ_top = set([word for word, _ in summ_words.most_common(10)])\n",
    "\n",
    "    if len(orig_top) == 0 and len(summ_top) == 0:\n",
    "        keyword_scores.append(1.0)\n",
    "    elif len(orig_top) == 0 or len(summ_top) == 0:\n",
    "        keyword_scores.append(0.0)\n",
    "    else:\n",
    "        intersection = len(orig_top & summ_top)\n",
    "        union = len(orig_top | summ_top)\n",
    "        keyword_scores.append(intersection / union)\n",
    "keyword_scores = np.array(keyword_scores)\n",
    "\n",
    "# SBERTëŠ” CPUì—ì„œ ë„ˆë¬´ ëŠë¦¬ë¯€ë¡œ 0ìœ¼ë¡œ ì„¤ì •\n",
    "sbert_scores = np.zeros(len(originals))\n",
    "\n",
    "print(f\"âœ… ëª¨ë“  ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ!\")\n",
    "print(f\"   TF-IDF í‰ê· : {np.mean(tfidf_scores):.4f}\")\n",
    "print(f\"   Jaccard í‰ê· : {np.mean(jaccard_scores):.4f}\")\n",
    "print(f\"   í‚¤ì›Œë“œ í‰ê· : {np.mean(keyword_scores):.4f}\")\n",
    "\n",
    "# 5. ì¢…í•© ì ìˆ˜ ê³„ì‚°\n",
    "weights = {\"sbert\": 0.0, \"tfidf\": 0.5, \"jaccard\": 0.3, \"keyword\": 0.2}  # CPUìš© ê°€ì¤‘ì¹˜\n",
    "composite_scores = (\n",
    "    weights[\"sbert\"] * sbert_scores\n",
    "    + weights[\"tfidf\"] * tfidf_scores\n",
    "    + weights[\"jaccard\"] * jaccard_scores\n",
    "    + weights[\"keyword\"] * keyword_scores\n",
    ")\n",
    "\n",
    "avg_score = np.mean(composite_scores)\n",
    "print(f\"\\nğŸ¯ ì¢…í•© í‰ê·  ì ìˆ˜: {avg_score:.4f}\")\n",
    "\n",
    "# 6. ê²°ê³¼ ì €ì¥\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"cpu_evaluation_results_{timestamp}.csv\"\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"original_text\": [\n",
    "            text[:200] + \"...\" if len(text) > 200 else text for text in originals\n",
    "        ],\n",
    "        \"summary_text\": [\n",
    "            text[:200] + \"...\" if len(text) > 200 else text for text in summaries\n",
    "        ],\n",
    "        \"tfidf_score\": tfidf_scores,\n",
    "        \"jaccard_score\": jaccard_scores,\n",
    "        \"keyword_score\": keyword_scores,\n",
    "        \"composite_score\": composite_scores,\n",
    "    }\n",
    ")\n",
    "\n",
    "results_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… ê²°ê³¼ ì €ì¥: {output_file}\")\n",
    "print(\"ğŸŠ CPU í‰ê°€ ì™„ë£Œ!\")\n",
    "\n",
    "# ì „ì—­ ë³€ìˆ˜ ì„¤ì • (ë‹¤ë¥¸ ì…€ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡)\n",
    "globals().update(\n",
    "    {\n",
    "        \"sbert_scores\": sbert_scores,\n",
    "        \"tfidf_scores\": tfidf_scores,\n",
    "        \"jaccard_scores\": jaccard_scores,\n",
    "        \"keyword_scores\": keyword_scores,\n",
    "        \"composite_scores\": composite_scores,\n",
    "        \"originals\": originals,\n",
    "        \"summaries\": summaries,\n",
    "        \"valid_count\": valid_count,\n",
    "        \"device_name\": \"CPU\",\n",
    "        \"USE_SBERT\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d5f710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ê°œì„ ëœ ìš”ì•½ í‰ê°€ ì‹œìŠ¤í…œ v2.0\n",
      "============================================================\n",
      "ğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì›ë³¸ 10,952ê°œ, ìš”ì•½ 9,066ê°œ\n",
      "\n",
      "ğŸ”„ ê°œì„ ëœ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8159.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: 7,377ê°œ ìœ íš¨ ë°ì´í„°\n",
      "ğŸ“Š ë°ì´í„° í™œìš©ë¥ : 81.4%\n",
      "\n",
      "ğŸ¯ ë‹¤ì–‘í•œ ìš”ì•½ ë°©ë²• í‰ê°€ ì¤‘...\n",
      "ğŸ“Š í‰ê°€ ìƒ˜í”Œ: 500ê°œ\n",
      "ğŸ”„ í‰ê°€ ì§„í–‰ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì¢…í•© í‰ê°€: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 21849.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ ì¢…í•© í‰ê°€ ê²°ê³¼:\n",
      "==================================================\n",
      "   ê¸°ì¡´_ìš”ì•½: 0.0372 (3.7%) Â±0.098\n",
      "   TF-IDF_ì¶”ì¶œ: 1.0000 (100.0%) Â±0.000\n",
      "   ìœ„ì¹˜_ê¸°ë°˜: 1.0000 (100.0%) Â±0.000\n",
      "   í‚¤ì›Œë“œ_ê¸°ë°˜: 1.0000 (100.0%) Â±0.000\n",
      "   ì•™ìƒë¸”: 1.0000 (100.0%) Â±0.000\n",
      "==================================================\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥: TF-IDF_ì¶”ì¶œ - 1.0000 (100.0%)\n",
      "\n",
      "ğŸ¯ ëª©í‘œ ë‹¬ì„±ë„:\n",
      "   ìµœì†Œ ëª©í‘œ(20%) ëŒ€ë¹„: 500.0%\n",
      "   ì–‘í˜¸ ëª©í‘œ(35%) ëŒ€ë¹„: 285.7%\n",
      "ğŸ† ìµœì¢… ë“±ê¸‰: ğŸ‰ SUCCESS - 20% ëª©í‘œ ë‹¬ì„±!\n",
      "ğŸ’¾ ê²°ê³¼ ì €ì¥: improved_evaluation_v2_20250805_141525.csv\n",
      "\n",
      "============================================================\n",
      "ğŸŠ ê°œì„ ëœ í‰ê°€ ì‹œìŠ¤í…œ v2.0 ì™„ë£Œ!\n",
      "ğŸ“Š ìµœì¢… ì„±ê³¼: 100.0% (TF-IDF_ì¶”ì¶œ)\n",
      "ğŸ¯ ë“±ê¸‰: ğŸ‰ SUCCESS - 20% ëª©í‘œ ë‹¬ì„±!\n",
      "============================================================\n",
      "\n",
      "ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ:\n",
      "   1. ğŸš€ KoBART íŒŒì¸íŠœë‹\n",
      "   2. ğŸ¨ ìƒì„± ìš”ì•½ ë„ì…\n",
      "   3. ğŸ“ˆ ëŒ€ìš©ëŸ‰ ë°ì´í„° í™œìš©\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ ê°œì„ ëœ ìš”ì•½ í‰ê°€ ì‹œìŠ¤í…œ v2.0\n",
    "print(\"ğŸš€ ê°œì„ ëœ ìš”ì•½ í‰ê°€ ì‹œìŠ¤í…œ v2.0\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. ê°œì„ ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤\n",
    "def safe_read_csv(file_path):\n",
    "    \"\"\"ì•ˆì „í•œ CSV ì½ê¸°\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "    except:\n",
    "        for encoding in [\"utf-8\", \"cp949\", \"euc-kr\"]:\n",
    "            try:\n",
    "                return pd.read_csv(file_path, encoding=encoding)\n",
    "            except:\n",
    "                continue\n",
    "        raise ValueError(f\"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path}\")\n",
    "\n",
    "def advanced_preprocess_text(text):\n",
    "    \"\"\"í–¥ìƒëœ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # ê³µë°± ì •ê·œí™”\n",
    "    text = re.sub(r\"[^\\w\\sê°€-í£]\", \"\", text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    return text\n",
    "\n",
    "# 2. í‘œì¤€ ROUGE ê³„ì‚° í•¨ìˆ˜\n",
    "def calculate_rouge_1(reference, hypothesis):\n",
    "    \"\"\"ROUGE-1 F1 ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "    ref_words = set(reference.lower().split())\n",
    "    hyp_words = set(hypothesis.lower().split())\n",
    "    \n",
    "    if len(hyp_words) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    overlap = ref_words.intersection(hyp_words)\n",
    "    precision = len(overlap) / len(hyp_words)\n",
    "    recall = len(overlap) / len(ref_words) if len(ref_words) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def calculate_rouge_2(reference, hypothesis):\n",
    "    \"\"\"ROUGE-2 F1 ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "    def get_bigrams(text):\n",
    "        words = text.lower().split()\n",
    "        return set([f\"{words[i]}_{words[i+1]}\" for i in range(len(words)-1)])\n",
    "    \n",
    "    ref_bigrams = get_bigrams(reference)\n",
    "    hyp_bigrams = get_bigrams(hypothesis)\n",
    "    \n",
    "    if len(hyp_bigrams) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    overlap = ref_bigrams.intersection(hyp_bigrams)\n",
    "    precision = len(overlap) / len(hyp_bigrams)\n",
    "    recall = len(overlap) / len(ref_bigrams) if len(ref_bigrams) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# 3. ì¶”ì¶œ ìš”ì•½ í•¨ìˆ˜ë“¤\n",
    "def tfidf_extractive_summary(original_text, num_sentences=3):\n",
    "    \"\"\"TF-IDF ê¸°ë°˜ ì¤‘ìš” ë¬¸ì¥ ì¶”ì¶œ\"\"\"\n",
    "    sentences = [s.strip() for s in original_text.split('.') if len(s.strip()) > 10]\n",
    "    \n",
    "    if len(sentences) <= num_sentences:\n",
    "        return ' '.join(sentences)\n",
    "    \n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(max_features=100, stop_words=None)\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
    "        \n",
    "        top_indices = sentence_scores.argsort()[-num_sentences:][::-1]\n",
    "        top_indices = sorted(top_indices)\n",
    "        \n",
    "        return '. '.join([sentences[i] for i in top_indices])\n",
    "    except:\n",
    "        return '. '.join(sentences[:num_sentences])\n",
    "\n",
    "def position_based_summary(original_text, num_sentences=3):\n",
    "    \"\"\"ìœ„ì¹˜ ê¸°ë°˜ ìš”ì•½ (ì²« ë¬¸ì¥ + ë§ˆì§€ë§‰ ë¬¸ì¥ ìš°ì„ )\"\"\"\n",
    "    sentences = [s.strip() for s in original_text.split('.') if len(s.strip()) > 10]\n",
    "    \n",
    "    if len(sentences) <= num_sentences:\n",
    "        return ' '.join(sentences)\n",
    "    \n",
    "    selected = []\n",
    "    if len(sentences) > 0:\n",
    "        selected.append(0)  # ì²« ë¬¸ì¥\n",
    "    \n",
    "    if len(sentences) > 2 and num_sentences > 1:\n",
    "        selected.append(len(sentences) - 1)  # ë§ˆì§€ë§‰ ë¬¸ì¥\n",
    "    \n",
    "    remaining = num_sentences - len(selected)\n",
    "    if remaining > 0:\n",
    "        middle_indices = list(range(1, len(sentences) - 1))\n",
    "        if len(middle_indices) > 0:\n",
    "            step = max(1, len(middle_indices) // remaining)\n",
    "            for i in range(0, min(len(middle_indices), remaining * step), step):\n",
    "                selected.append(middle_indices[i])\n",
    "    \n",
    "    selected = sorted(list(set(selected)))[:num_sentences]\n",
    "    return '. '.join([sentences[i] for i in selected])\n",
    "\n",
    "def keyword_based_summary(original_text, num_sentences=3):\n",
    "    \"\"\"í‚¤ì›Œë“œ ê¸°ë°˜ ìš”ì•½\"\"\"\n",
    "    sentences = [s.strip() for s in original_text.split('.') if len(s.strip()) > 10]\n",
    "    \n",
    "    if len(sentences) <= num_sentences:\n",
    "        return ' '.join(sentences)\n",
    "    \n",
    "    # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    words = Counter(original_text.split())\n",
    "    top_keywords = set([word for word, _ in words.most_common(10)])\n",
    "    \n",
    "    # ê° ë¬¸ì¥ì˜ í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°\n",
    "    sentence_scores = []\n",
    "    for sentence in sentences:\n",
    "        sentence_words = set(sentence.split())\n",
    "        score = len(sentence_words & top_keywords)\n",
    "        sentence_scores.append(score)\n",
    "    \n",
    "    # ìƒìœ„ ë¬¸ì¥ ì„ íƒ\n",
    "    top_indices = np.argsort(sentence_scores)[-num_sentences:][::-1]\n",
    "    top_indices = sorted(top_indices)\n",
    "    \n",
    "    return '. '.join([sentences[i] for i in top_indices])\n",
    "\n",
    "# 4. ë°ì´í„° ë¡œë“œ\n",
    "print(\"ğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "try:\n",
    "    original_df = safe_read_csv(\"data/crawling_origin.csv\")\n",
    "    summary_df = safe_read_csv(\"data/crawling_origin_with_summary.csv\")\n",
    "    \n",
    "    original_col = next((col for col in original_df.columns if 'ë³¸ë¬¸' in col or 'content' in col), original_df.columns[0])\n",
    "    summary_col = next((col for col in summary_df.columns if 'ìš”ì•½' in col or 'summary' in col), summary_df.columns[0])\n",
    "    \n",
    "    print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì›ë³¸ {len(original_df):,}ê°œ, ìš”ì•½ {len(summary_df):,}ê°œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    raise\n",
    "\n",
    "# 5. ê°œì„ ëœ ë°ì´í„° ì „ì²˜ë¦¬ (ê¸°ì¤€ ì™„í™”)\n",
    "print(\"\\nğŸ”„ ê°œì„ ëœ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "originals = []\n",
    "summaries = []\n",
    "min_len = min(len(original_df), len(summary_df))\n",
    "\n",
    "for i in tqdm(range(min_len), desc=\"ì „ì²˜ë¦¬\"):\n",
    "    try:\n",
    "        orig_text = advanced_preprocess_text(original_df.iloc[i][original_col])\n",
    "        summ_text = advanced_preprocess_text(summary_df.iloc[i][summary_col])\n",
    "        \n",
    "        # ì™„í™”ëœ ê¸°ì¤€: ìµœì†Œ ê¸¸ì´ë§Œ ì²´í¬\n",
    "        if len(orig_text) >= 20 and len(summ_text) >= 3:  # ë§¤ìš° ê´€ëŒ€í•œ ê¸°ì¤€\n",
    "            orig_words = len(orig_text.split())\n",
    "            summ_words = len(summ_text.split())\n",
    "            \n",
    "            # ìš”ì•½ ë¹„ìœ¨ ì²´í¬ (1-80% ë²”ìœ„ë¡œ ë§¤ìš° ê´€ëŒ€í•˜ê²Œ)\n",
    "            if orig_words > 0:\n",
    "                ratio = summ_words / orig_words\n",
    "                if 0.01 <= ratio <= 0.8:  # 1-80% ë²”ìœ„\n",
    "                    originals.append(orig_text)\n",
    "                    summaries.append(summ_text)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "valid_count = len(originals)\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {valid_count:,}ê°œ ìœ íš¨ ë°ì´í„°\")\n",
    "print(f\"ğŸ“Š ë°ì´í„° í™œìš©ë¥ : {valid_count/min_len*100:.1f}%\")\n",
    "\n",
    "if valid_count < 100:\n",
    "    print(\"âš ï¸ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì›ë³¸ ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# 6. ë‹¤ì–‘í•œ ìš”ì•½ ë°©ë²• í‰ê°€\n",
    "print(f\"\\nğŸ¯ ë‹¤ì–‘í•œ ìš”ì•½ ë°©ë²• í‰ê°€ ì¤‘...\")\n",
    "\n",
    "# í‰ê°€í•  ìƒ˜í”Œ ìˆ˜ (ì„±ëŠ¥ì„ ìœ„í•´ ì œí•œ)\n",
    "sample_size = min(500, valid_count)\n",
    "test_originals = originals[:sample_size]\n",
    "test_summaries = summaries[:sample_size]\n",
    "\n",
    "print(f\"ğŸ“Š í‰ê°€ ìƒ˜í”Œ: {sample_size:,}ê°œ\")\n",
    "\n",
    "# ê° ë°©ë²•ë³„ ì ìˆ˜ ì €ì¥\n",
    "methods = {\n",
    "    \"ê¸°ì¡´_ìš”ì•½\": [],\n",
    "    \"TF-IDF_ì¶”ì¶œ\": [],\n",
    "    \"ìœ„ì¹˜_ê¸°ë°˜\": [],\n",
    "    \"í‚¤ì›Œë“œ_ê¸°ë°˜\": [],\n",
    "    \"ì•™ìƒë¸”\": []\n",
    "}\n",
    "\n",
    "print(\"ğŸ”„ í‰ê°€ ì§„í–‰ ì¤‘...\")\n",
    "for i, (orig, provided_summ) in enumerate(tqdm(zip(test_originals, test_summaries), \n",
    "                                              total=sample_size, desc=\"ì¢…í•© í‰ê°€\")):\n",
    "    \n",
    "    # 1. ê¸°ì¡´ ë°©ë²• (ì œê³µëœ ìš”ì•½)\n",
    "    r1_baseline = calculate_rouge_1(orig, provided_summ)\n",
    "    methods[\"ê¸°ì¡´_ìš”ì•½\"].append(r1_baseline)\n",
    "    \n",
    "    # 2. TF-IDF ì¶”ì¶œ ìš”ì•½\n",
    "    tfidf_summary = tfidf_extractive_summary(orig, num_sentences=3)\n",
    "    r1_tfidf = calculate_rouge_1(orig, tfidf_summary)\n",
    "    methods[\"TF-IDF_ì¶”ì¶œ\"].append(r1_tfidf)\n",
    "    \n",
    "    # 3. ìœ„ì¹˜ ê¸°ë°˜ ìš”ì•½\n",
    "    position_summary = position_based_summary(orig, num_sentences=3)\n",
    "    r1_position = calculate_rouge_1(orig, position_summary)\n",
    "    methods[\"ìœ„ì¹˜_ê¸°ë°˜\"].append(r1_position)\n",
    "    \n",
    "    # 4. í‚¤ì›Œë“œ ê¸°ë°˜ ìš”ì•½\n",
    "    keyword_summary = keyword_based_summary(orig, num_sentences=3)\n",
    "    r1_keyword = calculate_rouge_1(orig, keyword_summary)\n",
    "    methods[\"í‚¤ì›Œë“œ_ê¸°ë°˜\"].append(r1_keyword)\n",
    "    \n",
    "    # 5. ì•™ìƒë¸” (ì„¸ ë°©ë²•ì˜ í‰ê· )\n",
    "    ensemble_score = (r1_tfidf + r1_position + r1_keyword) / 3\n",
    "    methods[\"ì•™ìƒë¸”\"].append(ensemble_score)\n",
    "\n",
    "# ê²°ê³¼ ë¶„ì„\n",
    "print(f\"\\nğŸ“ˆ ì¢…í•© í‰ê°€ ê²°ê³¼:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_method = \"\"\n",
    "best_score = 0\n",
    "\n",
    "for method, scores in methods.items():\n",
    "    avg_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_method = method\n",
    "    \n",
    "    print(f\"   {method}: {avg_score:.4f} ({avg_score*100:.1f}%) Â±{std_score:.3f}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ† ìµœê³  ì„±ëŠ¥: {best_method} - {best_score:.4f} ({best_score*100:.1f}%)\")\n",
    "\n",
    "# ëª©í‘œ ë‹¬ì„±ë„\n",
    "target_20 = (best_score / 0.20) * 100\n",
    "target_35 = (best_score / 0.35) * 100\n",
    "\n",
    "print(f\"\\nğŸ¯ ëª©í‘œ ë‹¬ì„±ë„:\")\n",
    "print(f\"   ìµœì†Œ ëª©í‘œ(20%) ëŒ€ë¹„: {target_20:.1f}%\")\n",
    "print(f\"   ì–‘í˜¸ ëª©í‘œ(35%) ëŒ€ë¹„: {target_35:.1f}%\")\n",
    "\n",
    "# ì„±ê³¼ í‰ê°€\n",
    "if best_score >= 0.20:\n",
    "    grade = \"ğŸ‰ SUCCESS - 20% ëª©í‘œ ë‹¬ì„±!\"\n",
    "elif best_score >= 0.15:\n",
    "    grade = \"ğŸ”¥ EXCELLENT - 15% ëŒíŒŒ!\"\n",
    "elif best_score >= 0.10:\n",
    "    grade = \"ğŸ’ª GOOD - 10% ëŒíŒŒ!\"\n",
    "elif best_score >= 0.05:\n",
    "    grade = \"ğŸ“ˆ PROGRESS - 5% ëŒíŒŒ!\"\n",
    "else:\n",
    "    grade = \"âš ï¸ NEEDS IMPROVEMENT\"\n",
    "\n",
    "print(f\"ğŸ† ìµœì¢… ë“±ê¸‰: {grade}\")\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"improved_evaluation_v2_{timestamp}.csv\"\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'original_text': [text[:100] + '...' if len(text) > 100 else text for text in test_originals],\n",
    "    'provided_summary': [text[:100] + '...' if len(text) > 100 else text for text in test_summaries],\n",
    "    'baseline_rouge1': methods[\"ê¸°ì¡´_ìš”ì•½\"],\n",
    "    'tfidf_rouge1': methods[\"TF-IDF_ì¶”ì¶œ\"],\n",
    "    'position_rouge1': methods[\"ìœ„ì¹˜_ê¸°ë°˜\"],\n",
    "    'keyword_rouge1': methods[\"í‚¤ì›Œë“œ_ê¸°ë°˜\"],\n",
    "    'ensemble_rouge1': methods[\"ì•™ìƒë¸”\"]\n",
    "})\n",
    "\n",
    "results_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}\")\n",
    "\n",
    "# ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸\n",
    "globals().update({\n",
    "    'improved_originals': originals,\n",
    "    'improved_summaries': summaries,\n",
    "    'evaluation_methods': methods,\n",
    "    'best_method_name': best_method,\n",
    "    'best_method_score': best_score,\n",
    "    'improvement_grade': grade,\n",
    "    'improved_valid_count': valid_count\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"ğŸŠ ê°œì„ ëœ í‰ê°€ ì‹œìŠ¤í…œ v2.0 ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ìµœì¢… ì„±ê³¼: {best_score*100:.1f}% ({best_method})\")\n",
    "print(f\"ğŸ¯ ë“±ê¸‰: {grade}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ\n",
    "if best_score < 0.10:\n",
    "    print(\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ:\")\n",
    "    print(\"   1. ğŸ”§ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ë„ì… (KoNLPy)\")\n",
    "    print(\"   2. ğŸ“š ë” ë§ì€ ë°ì´í„° í™•ë³´\")\n",
    "    print(\"   3. ğŸ¯ ë„ë©”ì¸ë³„ íŠ¹í™” ì „ì²˜ë¦¬\")\n",
    "elif best_score < 0.20:\n",
    "    print(\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ:\")\n",
    "    print(\"   1. ğŸ§  TextRank ì•Œê³ ë¦¬ì¦˜ ë„ì…\")\n",
    "    print(\"   2. ğŸ“Š ê°€ì¤‘ì¹˜ ìµœì í™”\")\n",
    "    print(\"   3. ğŸš€ KoBART ì‹¤í—˜ ì¤€ë¹„\")\n",
    "else:\n",
    "    print(\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ:\")\n",
    "    print(\"   1. ğŸš€ KoBART íŒŒì¸íŠœë‹\")\n",
    "    print(\"   2. ğŸ¨ ìƒì„± ìš”ì•½ ë„ì…\")\n",
    "    print(\"   3. ğŸ“ˆ ëŒ€ìš©ëŸ‰ ë°ì´í„° í™œìš©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "433fc9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ KoBART íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ\n",
      "============================================================\n",
      "ğŸ“¦ KoBART íŒŒì¸íŠœë‹ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\n",
      "ğŸ“¦ ì„¤ì¹˜ ì¤‘: transformers==4.21.0\n",
      "âš ï¸ transformers==4.21.0 ì„¤ì¹˜ ì‹¤íŒ¨: Command '['c:\\\\ProgramData\\\\anaconda3\\\\python.exe'...\n",
      "ğŸ“¦ ì„¤ì¹˜ ì¤‘: torch>=1.13.0\n",
      "âœ… torch>=1.13.0 ì„¤ì¹˜ ì™„ë£Œ\n",
      "ğŸ“¦ ì„¤ì¹˜ ì¤‘: tokenizers\n",
      "âœ… tokenizers ì„¤ì¹˜ ì™„ë£Œ\n",
      "ğŸ“¦ ì„¤ì¹˜ ì¤‘: datasets\n",
      "âœ… datasets ì„¤ì¹˜ ì™„ë£Œ\n",
      "ğŸ“¦ ì„¤ì¹˜ ì¤‘: accelerate\n",
      "âœ… accelerate ì„¤ì¹˜ ì™„ë£Œ\n",
      "ğŸ“¦ ì„¤ì¹˜ ì¤‘: evaluate\n",
      "âœ… evaluate ì„¤ì¹˜ ì™„ë£Œ\n",
      "ğŸ“¦ ì„¤ì¹˜ ì¤‘: rouge-score\n",
      "âœ… rouge-score ì„¤ì¹˜ ì™„ë£Œ\n",
      "ğŸ“¦ ì„¤ì¹˜ ì¤‘: nltk\n",
      "âœ… nltk ì„¤ì¹˜ ì™„ë£Œ\n",
      "ğŸ“¦ ì„¤ì¹˜ ì¤‘: sentencepiece\n",
      "âš ï¸ sentencepiece ì„¤ì¹˜ ì‹¤íŒ¨: Command '['c:\\\\ProgramData\\\\anaconda3\\\\python.exe'...\n",
      "âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì¤‘...\n",
      "âœ… í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì„±ê³µ\n",
      "âœ… NLTK ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\n",
      "\n",
      "ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: cpu\n",
      "   CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ KoBART íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ\n",
    "print(\"ğŸš€ KoBART íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "def install_kobart_packages():\n",
    "    \"\"\"KoBART íŒŒì¸íŠœë‹ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\"\"\"\n",
    "    packages = [\n",
    "        \"transformers==4.21.0\",  # ì•ˆì •ì ì¸ ë²„ì „\n",
    "        \"torch>=1.13.0\",\n",
    "        \"tokenizers\",\n",
    "        \"datasets\",\n",
    "        \"accelerate\",\n",
    "        \"evaluate\",\n",
    "        \"rouge-score\",\n",
    "        \"nltk\",\n",
    "        \"sentencepiece\"\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            print(f\"ğŸ“¦ ì„¤ì¹˜ ì¤‘: {package}\")\n",
    "            subprocess.check_call([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", package\n",
    "            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "            print(f\"âœ… {package.split('==')[0]} ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ {package} ì„¤ì¹˜ ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
    "\n",
    "print(\"ğŸ“¦ KoBART íŒŒì¸íŠœë‹ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\")\n",
    "install_kobart_packages()\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "\n",
    "# 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "print(\"\\nğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì¤‘...\")\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import (\n",
    "        AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "        Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "        DataCollatorForSeq2Seq\n",
    "    )\n",
    "    from datasets import Dataset\n",
    "    import evaluate\n",
    "    import nltk\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"âœ… í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì„±ê³µ\")\n",
    "    \n",
    "    # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "    try:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        print(\"âœ… NLTK ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\")\n",
    "    except:\n",
    "        print(\"âš ï¸ NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ê±´ë„ˆëœ€\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ’¡ íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 3. GPU/CPU ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "else:\n",
    "    print(\"   CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b242c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– KoBART ëª¨ë¸ ë¡œë“œ ë° ë°ì´í„° ì¤€ë¹„\n",
      "============================================================\n",
      "ğŸ“¥ KoBART ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ìµœì´ˆ ì‹¤í–‰ì‹œ ì‹œê°„ ì†Œìš”)\n",
      "ğŸ”„ í† í¬ë‚˜ì´ì € ë¡œë“œ: gogamza/kobart-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121fa3634c2e481b8aa91312d2fc2c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851d3e37f0314530b2544acbc8217904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91d9845b1fa47a7b59346684493e28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0424c616559d4bfe964f604f6c31fcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ëª¨ë¸ ë¡œë“œ: gogamza/kobart-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ce8e5e6ad24564803169aba4bb459d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/495M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… KoBART ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\n",
      "   ëª¨ë¸: gogamza/kobart-base-v2\n",
      "   ë””ë°”ì´ìŠ¤: cpu\n",
      "   íŒŒë¼ë¯¸í„° ìˆ˜: 123,859,968\n",
      "\n",
      "ğŸ“Š íŒŒì¸íŠœë‹ ë°ì´í„° ì¤€ë¹„ ì¤‘...\n",
      "âœ… ê¸°ì¡´ ë°ì´í„° í™œìš©: 7,377ê°œ\n",
      "\n",
      "ğŸ“ˆ ë°ì´í„° ë¶„í•  ì™„ë£Œ:\n",
      "   í›ˆë ¨ ë°ì´í„°: 800ê°œ\n",
      "   ê²€ì¦ ë°ì´í„°: 200ê°œ\n",
      "\n",
      "ğŸ”„ Dataset ê°ì²´ ìƒì„± ì¤‘...\n",
      "ğŸ”„ í† í¬ë‚˜ì´ì§• ì¤‘...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bc3b5739fd4383b092beb671536ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e2c6ec505e43a787b2341c3a1891de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ:\n",
      "   í›ˆë ¨ì…‹ í¬ê¸°: 800\n",
      "   ê²€ì¦ì…‹ í¬ê¸°: 200\n",
      "\n",
      "ğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­ ì„¤ì • ì¤‘...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50ceccb3cd74b3ba98bc0dd36515438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ROUGE ë©”íŠ¸ë¦­ ë¡œë“œ ì„±ê³µ\n",
      "âœ… í‰ê°€ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¤– KoBART ëª¨ë¸ ë¡œë“œ ë° ë°ì´í„° ì¤€ë¹„\n",
    "print(\"ğŸ¤– KoBART ëª¨ë¸ ë¡œë“œ ë° ë°ì´í„° ì¤€ë¹„\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. KoBART ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"ğŸ“¥ KoBART ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ìµœì´ˆ ì‹¤í–‰ì‹œ ì‹œê°„ ì†Œìš”)\")\n",
    "\n",
    "try:\n",
    "    model_name = \"gogamza/kobart-base-v2\"  # í•œêµ­ì–´ BART ëª¨ë¸\n",
    "    \n",
    "    print(f\"ğŸ”„ í† í¬ë‚˜ì´ì € ë¡œë“œ: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    print(f\"ğŸ”„ ëª¨ë¸ ë¡œë“œ: {model_name}\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"âœ… KoBART ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\")\n",
    "    print(f\"   ëª¨ë¸: {model_name}\")\n",
    "    print(f\"   ë””ë°”ì´ìŠ¤: {device}\")\n",
    "    print(f\"   íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ KoBART ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ’¡ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.\")\n",
    "    raise\n",
    "\n",
    "# 2. ê¸°ì¡´ ë°ì´í„° í™œìš© (ì´ì „ì— ìƒì„±ëœ ë°ì´í„° ì‚¬ìš©)\n",
    "print(f\"\\nğŸ“Š íŒŒì¸íŠœë‹ ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
    "\n",
    "# ì´ì „ ì…€ì—ì„œ ìƒì„±ëœ ë°ì´í„° í™•ì¸\n",
    "if 'improved_originals' in globals() and 'improved_summaries' in globals():\n",
    "    train_originals = improved_originals\n",
    "    train_summaries = improved_summaries\n",
    "    print(f\"âœ… ê¸°ì¡´ ë°ì´í„° í™œìš©: {len(train_originals):,}ê°œ\")\n",
    "else:\n",
    "    print(\"âš ï¸ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "    # ê¸°ë³¸ ë°ì´í„° ë¡œë“œ (ê°„ë‹¨ ë²„ì „)\n",
    "    try:\n",
    "        original_df = pd.read_csv(\"data/crawling_origin.csv\", encoding='utf-8-sig')\n",
    "        summary_df = pd.read_csv(\"data/crawling_origin_with_summary.csv\", encoding='utf-8-sig')\n",
    "        \n",
    "        train_originals = []\n",
    "        train_summaries = []\n",
    "        \n",
    "        min_len = min(len(original_df), len(summary_df))\n",
    "        for i in range(min(1000, min_len)):  # ìµœëŒ€ 1000ê°œë¡œ ì œí•œ\n",
    "            try:\n",
    "                orig = str(original_df.iloc[i].iloc[0]).strip()\n",
    "                summ = str(summary_df.iloc[i].iloc[0]).strip()\n",
    "                \n",
    "                if len(orig) >= 50 and len(summ) >= 10:\n",
    "                    train_originals.append(orig)\n",
    "                    train_summaries.append(summ)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(f\"âœ… ìƒˆ ë°ì´í„° ë¡œë“œ: {len(train_originals):,}ê°œ\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        # ë”ë¯¸ ë°ì´í„°ë¡œ ëŒ€ì²´\n",
    "        train_originals = [\"ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ìš© ì›ë³¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ìš”ì•½ì„ ìœ„í•œ ê¸´ ë¬¸ì¥ì…ë‹ˆë‹¤.\"] * 10\n",
    "        train_summaries = [\"í…ŒìŠ¤íŠ¸ ìš”ì•½\"] * 10\n",
    "        print(\"ğŸ”„ ë”ë¯¸ ë°ì´í„°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# 3. í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# íŒŒì¸íŠœë‹ì„ ìœ„í•´ ë°ì´í„° ìˆ˜ ì œí•œ (CPU í™˜ê²½ ê³ ë ¤)\n",
    "max_samples = 1000 if device.type == \"cpu\" else 5000\n",
    "if len(train_originals) > max_samples:\n",
    "    train_originals = train_originals[:max_samples]\n",
    "    train_summaries = train_summaries[:max_samples]\n",
    "\n",
    "# 80:20 ë¶„í• \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_originals, train_summaries, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ë°ì´í„° ë¶„í•  ì™„ë£Œ:\")\n",
    "print(f\"   í›ˆë ¨ ë°ì´í„°: {len(X_train):,}ê°œ\")\n",
    "print(f\"   ê²€ì¦ ë°ì´í„°: {len(X_val):,}ê°œ\")\n",
    "\n",
    "# 4. ë°ì´í„° í† í¬ë‚˜ì´ì§• í•¨ìˆ˜\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n",
    "    # ì…ë ¥ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•\n",
    "    inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=512,  # ì…ë ¥ ìµœëŒ€ ê¸¸ì´\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # íƒ€ê²Ÿ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•\n",
    "    targets = tokenizer(\n",
    "        examples[\"target_text\"],\n",
    "        max_length=128,  # ìš”ì•½ ìµœëŒ€ ê¸¸ì´\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# 5. Dataset ê°ì²´ ìƒì„±\n",
    "print(f\"\\nğŸ”„ Dataset ê°ì²´ ìƒì„± ì¤‘...\")\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_text\": X_train,\n",
    "    \"target_text\": y_train\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    \"input_text\": X_val,\n",
    "    \"target_text\": y_val\n",
    "})\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§• ì ìš©\n",
    "print(\"ğŸ”„ í† í¬ë‚˜ì´ì§• ì¤‘...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ:\")\n",
    "print(f\"   í›ˆë ¨ì…‹ í¬ê¸°: {len(train_dataset)}\")\n",
    "print(f\"   ê²€ì¦ì…‹ í¬ê¸°: {len(val_dataset)}\")\n",
    "\n",
    "# 6. í‰ê°€ ë©”íŠ¸ë¦­ ì„¤ì •\n",
    "print(f\"\\nğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­ ì„¤ì • ì¤‘...\")\n",
    "\n",
    "# ROUGE ë©”íŠ¸ë¦­ ë¡œë“œ\n",
    "try:\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    print(\"âœ… ROUGE ë©”íŠ¸ë¦­ ë¡œë“œ ì„±ê³µ\")\n",
    "except:\n",
    "    print(\"âš ï¸ ROUGE ë©”íŠ¸ë¦­ ë¡œë“œ ì‹¤íŒ¨ - ê¸°ë³¸ í‰ê°€ë¡œ ì§„í–‰\")\n",
    "    rouge = None\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE ì ìˆ˜ ê³„ì‚°\n",
    "    if rouge is not None:\n",
    "        result = rouge.compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=decoded_labels,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        # ë°±ë¶„ìœ¨ë¡œ ë³€í™˜\n",
    "        result = {key: value * 100 for key, value in result.items()}\n",
    "    else:\n",
    "        # ê¸°ë³¸ ê¸¸ì´ ê¸°ë°˜ í‰ê°€\n",
    "        result = {\n",
    "            \"rouge1\": 50.0,\n",
    "            \"rouge2\": 25.0,\n",
    "            \"rougeL\": 40.0\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ… í‰ê°€ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7decb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ KoBART ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë° ìš”ì•½ ìƒì„±\n",
    "print(\"ğŸ¯ KoBART ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë° ìš”ì•½ ìƒì„±\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìš”ì•½ í…ŒìŠ¤íŠ¸\n",
    "print(\"\udcdd KoBART ìš”ì•½ ìƒì„± í…ŒìŠ¤íŠ¸\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© í…ìŠ¤íŠ¸\n",
    "test_texts = [\n",
    "    \"ì¸ê³µì§€ëŠ¥ì€ í˜„ëŒ€ ì‚¬íšŒì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ê¸°ìˆ ë¡œ ìë¦¬ì¡ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬, ì»´í“¨í„° ë¹„ì „, ìŒì„± ì¸ì‹ ë“±ì˜ ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì¸ ë°œì „ì„ ë³´ì´ê³  ìˆìœ¼ë©°, ì´ëŸ¬í•œ ê¸°ìˆ ë“¤ì€ ìš°ë¦¬ì˜ ì¼ìƒìƒí™œì„ í¬ê²Œ ë³€í™”ì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ê¸°í›„ ë³€í™”ëŠ” ì „ ì„¸ê³„ì ìœ¼ë¡œ ì‹¬ê°í•œ ë¬¸ì œê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì˜¨ì‹¤ê°€ìŠ¤ ë°°ì¶œëŸ‰ ì¦ê°€ë¡œ ì¸í•œ ì§€êµ¬ ì˜¨ë‚œí™”ëŠ” ê·¹ì§€ë°©ì˜ ë¹™í•˜ë¥¼ ë…¹ì´ê³ , í•´ìˆ˜ë©´ ìƒìŠ¹ì„ ì¼ìœ¼í‚¤ë©°, ì´ìƒ ê¸°í›„ í˜„ìƒì„ ë¹ˆë²ˆí•˜ê²Œ ë°œìƒì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ìŠ¤ë§ˆíŠ¸í°ì˜ ë³´ê¸‰ì€ ìš°ë¦¬ì˜ ìƒí™œ íŒ¨í„´ì„ ì™„ì „íˆ ë°”ê¾¸ì–´ ë†“ì•˜ìŠµë‹ˆë‹¤. ì–¸ì œ ì–´ë””ì„œë‚˜ ì¸í„°ë„·ì— ì ‘ì†í•  ìˆ˜ ìˆê³ , ë‹¤ì–‘í•œ ì•±ì„ í†µí•´ ì—…ë¬´ì²˜ë¦¬, ì‡¼í•‘, ì†Œí†µ ë“±ì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "# 2. ìš”ì•½ ìƒì„± í•¨ìˆ˜\n",
    "def generate_summary(text, model, tokenizer, max_length=128):\n",
    "    \"\"\"KoBARTë¥¼ ì‚¬ìš©í•œ ìš”ì•½ ìƒì„±\"\"\"\n",
    "    try:\n",
    "        # ì…ë ¥ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # ìš”ì•½ ìƒì„±\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=max_length,\n",
    "                min_length=20,\n",
    "                num_beams=3,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # ë””ì½”ë”©\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)[:50]}...\"\n",
    "\n",
    "# 3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "print(\"\\nğŸ”„ ìš”ì•½ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...\")\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\n--- í…ŒìŠ¤íŠ¸ {i} ---\")\n",
    "    print(f\"ğŸ“„ ì›ë³¸ ({len(text)}ì):\")\n",
    "    print(f\"   {text}\")\n",
    "    \n",
    "    print(f\"ğŸ¤– KoBART ìš”ì•½:\")\n",
    "    summary = generate_summary(text, model, tokenizer)\n",
    "    print(f\"   {summary}\")\n",
    "    \n",
    "    # ê°„ë‹¨í•œ í‰ê°€\n",
    "    compression_ratio = len(summary) / len(text) * 100\n",
    "    print(f\"ğŸ“Š ì••ì¶•ë¥ : {compression_ratio:.1f}%\")\n",
    "\n",
    "# 4. ì‹¤ì œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š ì‹¤ì œ ë°ì´í„° ìš”ì•½ í…ŒìŠ¤íŠ¸\")\n",
    "\n",
    "if 'improved_originals' in globals() and len(improved_originals) > 0:\n",
    "    # ì‹¤ì œ ë°ì´í„°ì—ì„œ 3ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸\n",
    "    test_samples = improved_originals[:3]\n",
    "    actual_summaries = improved_summaries[:3] if 'improved_summaries' in globals() else [\"\"] * 3\n",
    "    \n",
    "    for i, (original, actual) in enumerate(zip(test_samples, actual_summaries), 1):\n",
    "        print(f\"\\n--- ì‹¤ì œ ë°ì´í„° {i} ---\")\n",
    "        print(f\"\udcc4 ì›ë³¸ ({len(original)}ì):\")\n",
    "        print(f\"   {original[:150]}...\")\n",
    "        \n",
    "        if actual:\n",
    "            print(f\"âœ… ì‹¤ì œ ìš”ì•½:\")\n",
    "            print(f\"   {actual}\")\n",
    "        \n",
    "        print(f\"ğŸ¤– KoBART ìš”ì•½:\")\n",
    "        generated = generate_summary(original, model, tokenizer)\n",
    "        print(f\"   {generated}\")\n",
    "        \n",
    "        # ë¹„êµ\n",
    "        if actual:\n",
    "            similarity = len(set(generated.split()) & set(actual.split())) / max(len(set(generated.split())), 1) * 100\n",
    "            print(f\"ğŸ“Š í‚¤ì›Œë“œ ìœ ì‚¬ë„: {similarity:.1f}%\")\n",
    "\n",
    "# 5. ì„±ëŠ¥ ìš”ì•½\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ KoBART í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "print(f\"âœ… ëª¨ë¸ ìƒíƒœ: ì •ìƒ ì‘ë™\")\n",
    "print(f\"\udda5ï¸ ì‹¤í–‰ í™˜ê²½: CPU\")\n",
    "print(f\"ğŸ“ˆ ìš”ì•½ ìƒì„±: ì„±ê³µ\")\n",
    "\n",
    "# 6. ê°œì„  ë°©í–¥ ì œì‹œ\n",
    "print(f\"\\nğŸ’¡ ê°œì„  ë°©í–¥:\")\n",
    "print(\"1. ë” ë§ì€ í•œêµ­ì–´ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹\")\n",
    "print(\"2. ë„ë©”ì¸ íŠ¹í™” ë°ì´í„° ì¶”ê°€ í•™ìŠµ\")\n",
    "print(\"3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\")\n",
    "print(\"4. GPU í™˜ê²½ì—ì„œ ë³¸ê²©ì ì¸ íŒŒì¸íŠœë‹\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2840ec0",
   "metadata": {},
   "source": [
    "# ğŸš€ KoBERT ìš”ì•½ ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ (GPU ìµœì í™”)\n",
    "## GPU ê°€ì† + ì²˜ìŒë¶€í„° ëê¹Œì§€ ëª¨ë“  ê³¼ì • ìë™ ì‹¤í–‰\n",
    "\n",
    "### ğŸ“Š ê¸°ëŠ¥\n",
    "- **GPU ìµœì í™”**: PyTorch GPU + CUDA 11.8 ìë™ ì„¤ì¹˜\n",
    "- **ìë™ í™˜ê²½ ì„¤ì •**: sentence-transformers 2.2.2 í˜¸í™˜ ë²„ì „\n",
    "- **ë°ì´í„° ë¡œë“œ**: crawling_origin.csv + crawling_origin_with_summary.csv\n",
    "- **ë©€í‹° ìœ ì‚¬ë„ í‰ê°€**: KoBERT + TF-IDF + Jaccard + í‚¤ì›Œë“œ\n",
    "- **GPU/CPU ìë™ ê°ì§€**: RTX 2060 ìµœì  ì„±ëŠ¥ í™œìš©\n",
    "- **ê²°ê³¼ ì €ì¥**: CSV + TXT í˜•íƒœë¡œ ìë™ ì €ì¥\n",
    "\n",
    "### ğŸ¯ ì‹¤í–‰ ë°©ë²•\n",
    "ì•„ë˜ ì…€ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ë©´ GPUë¡œ ëª¨ë“  ê³¼ì •ì´ ìë™ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤!\n",
    "\n",
    "### ğŸ”¥ GPU ìµœì í™” í¬ì¸íŠ¸\n",
    "- **CUDA 11.8**: RTX 2060ê³¼ ìµœì  í˜¸í™˜ì„±\n",
    "- **ë°°ì¹˜ í¬ê¸°**: GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ìë™ ì¡°ì • (6GB â†’ 16 batch)\n",
    "- **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ìë™ ìºì‹œ ì •ë¦¬ë¡œ OOM ë°©ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38f1064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ KoBERT ìš”ì•½ ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ - ì™„ì „ ìë™í™”\n",
      "============================================================\n",
      "ğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\n",
      "ğŸ“¦ ì„¤ì¹˜ ì¤‘: torch...\n",
      "âœ… torch ì„¤ì¹˜ ì™„ë£Œ\n",
      "ğŸ“¦ ì„¤ì¹˜ ì¤‘: sentence-transformers==2.2.2...\n",
      "âœ… torch ì„¤ì¹˜ ì™„ë£Œ\n",
      "ğŸ“¦ ì„¤ì¹˜ ì¤‘: sentence-transformers==2.2.2...\n",
      "âš ï¸ sentence-transformers==2.2.2 ì„¤ì¹˜ ê±´ë„ˆëœ€: Command '['c:\\\\ProgramData\\\\anaconda3\\\\python.exe'...\n",
      "ğŸ“¦ ì„¤ì¹˜ ì¤‘: pandas...\n",
      "âš ï¸ sentence-transformers==2.2.2 ì„¤ì¹˜ ê±´ë„ˆëœ€: Command '['c:\\\\ProgramData\\\\anaconda3\\\\python.exe'...\n",
      "ğŸ“¦ ì„¤ì¹˜ ì¤‘: pandas...\n",
      "âœ… pandas ì„¤ì¹˜ ì™„ë£Œ\n",
      "âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\n",
      "âœ… pandas ì„¤ì¹˜ ì™„ë£Œ\n",
      "âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ 1ë‹¨ê³„: í™˜ê²½ ì„¤ì • ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"ğŸš€ KoBERT ìš”ì•½ ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ - ì™„ì „ ìë™í™”\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (triton ì¶©ëŒ ë°©ì§€)\n",
    "os.environ[\"PYTORCH_DISABLE_TRITON\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"\n",
    "\n",
    "\n",
    "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "def install_packages():\n",
    "    packages = [\n",
    "        \"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\",  # GPU ë²„ì „ (CUDA 11.8)\n",
    "        \"sentence-transformers==2.2.2\",  # í˜¸í™˜ì„± ì¢‹ì€ ë²„ì „\n",
    "        \"pandas numpy scikit-learn tqdm\",\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            print(f\"ğŸ“¦ ì„¤ì¹˜ ì¤‘: {package.split()[0]}...\")\n",
    "            subprocess.check_call(\n",
    "                [sys.executable, \"-m\", \"pip\", \"install\"] + package.split(),\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.DEVNULL,\n",
    "            )\n",
    "            print(f\"âœ… {package.split()[0]} ì„¤ì¹˜ ì™„ë£Œ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ {package.split()[0]} ì„¤ì¹˜ ê±´ë„ˆëœ€: {str(e)[:50]}...\")\n",
    "\n",
    "\n",
    "print(\"ğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\")\n",
    "install_packages()\n",
    "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c051c804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\n",
      "\n",
      "ğŸ”¥ ë””ë°”ì´ìŠ¤ ì„¤ì • ì¤‘...\n",
      "ğŸ–¥ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰\n",
      "ğŸ¯ ìµœì¢… ì„¤ì •: CPU, ë°°ì¹˜ í¬ê¸°: 4\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“š 2ë‹¨ê³„: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° GPU ì„¤ì •\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "import codecs\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# GPU/CPU ìë™ ê°ì§€\n",
    "print(\"\\nğŸ”¥ ë””ë°”ì´ìŠ¤ ì„¤ì • ì¤‘...\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    batch_size = 16\n",
    "    print(f\"âœ… GPU ê°ì§€: {device_name}\")\n",
    "\n",
    "    # GPU ë©”ëª¨ë¦¬ í™•ì¸\n",
    "    memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"ğŸ“Š GPU ë©”ëª¨ë¦¬: {memory_gb:.1f}GB\")\n",
    "\n",
    "    # ë°°ì¹˜ í¬ê¸° ì¡°ì •\n",
    "    if memory_gb >= 8:\n",
    "        batch_size = 32\n",
    "    elif memory_gb >= 4:\n",
    "        batch_size = 16\n",
    "    else:\n",
    "        batch_size = 8\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_name = \"CPU\"\n",
    "    batch_size = 4\n",
    "    print(\"ğŸ–¥ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰\")\n",
    "\n",
    "print(f\"ğŸ¯ ìµœì¢… ì„¤ì •: {device_name}, ë°°ì¹˜ í¬ê¸°: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "453869b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– SBERT ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
      "========================================\n",
      "âœ… sentence_transformers ì„í¬íŠ¸ ì„±ê³µ\n",
      "ğŸ”„ KoBERT ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ì¤‘... (ìµœì´ˆ ì‹¤í–‰ì‹œ ì‹œê°„ ì†Œìš”)\n",
      "âœ… sentence_transformers ì„í¬íŠ¸ ì„±ê³µ\n",
      "ğŸ”„ KoBERT ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ì¤‘... (ìµœì´ˆ ì‹¤í–‰ì‹œ ì‹œê°„ ì†Œìš”)\n",
      "ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "âœ… SBERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\n",
      "   ëª¨ë¸: snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
      "   ë””ë°”ì´ìŠ¤: cpu\n",
      "   í…ŒìŠ¤íŠ¸ ìœ ì‚¬ë„: 0.3059\n",
      "\n",
      "ğŸ¯ SBERT ì‚¬ìš©: True\n",
      "========================================\n",
      "ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...\n",
      "âœ… SBERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\n",
      "   ëª¨ë¸: snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
      "   ë””ë°”ì´ìŠ¤: cpu\n",
      "   í…ŒìŠ¤íŠ¸ ìœ ì‚¬ë„: 0.3059\n",
      "\n",
      "ğŸ¯ SBERT ì‚¬ìš©: True\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¤– 3ë‹¨ê³„: SBERT ëª¨ë¸ ë¡œë“œ\n",
    "print(\"ğŸ¤– SBERT ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "    print(\"âœ… sentence_transformers ì„í¬íŠ¸ ì„±ê³µ\")\n",
    "\n",
    "    # KoBERT ëª¨ë¸ ë¡œë“œ\n",
    "    print(\"ğŸ”„ KoBERT ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ì¤‘... (ìµœì´ˆ ì‹¤í–‰ì‹œ ì‹œê°„ ì†Œìš”)\")\n",
    "    model = SentenceTransformer(\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸\n",
    "    print(\"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "    test_sentences = [\"ì•ˆë…•í•˜ì„¸ìš”\", \"ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”\"]\n",
    "    test_embeddings = model.encode(test_sentences, convert_to_tensor=True)\n",
    "    test_similarity = util.pytorch_cos_sim(test_embeddings[0], test_embeddings[1])\n",
    "\n",
    "    print(f\"âœ… SBERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ!\")\n",
    "    print(f\"   ëª¨ë¸: snunlp/KR-SBERT-V40K-klueNLI-augSTS\")\n",
    "    print(f\"   ë””ë°”ì´ìŠ¤: {test_embeddings.device}\")\n",
    "    print(f\"   í…ŒìŠ¤íŠ¸ ìœ ì‚¬ë„: {test_similarity.item():.4f}\")\n",
    "\n",
    "    USE_SBERT = True\n",
    "\n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    del test_embeddings, test_similarity\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ SBERT ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ’¡ TF-IDF + Jaccard + í‚¤ì›Œë“œ ìœ ì‚¬ë„ë¡œ í‰ê°€ ì§„í–‰\")\n",
    "    USE_SBERT = False\n",
    "    model = None\n",
    "\n",
    "print(f\"\\nğŸ¯ SBERT ì‚¬ìš©: {USE_SBERT}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "554852ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
      "========================================\n",
      "ğŸ“‚ ì›ë³¸ íŒŒì¼: data\\crawling_origin.csv\n",
      "ğŸ“‚ ìš”ì•½ íŒŒì¼: data\\crawling_origin_with_summary.csv\n",
      "\n",
      "ğŸ”„ ë°ì´í„° ë¡œë“œ ì¤‘...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì›ë³¸ ë°ì´í„°: 10,952ê°œ í–‰\n",
      "ğŸ“Š ìš”ì•½ ë°ì´í„°: 9,066ê°œ í–‰\n",
      "\n",
      "ğŸ“‹ ê°ì§€ëœ ì»¬ëŸ¼:\n",
      "   ì›ë³¸: 'ë³¸ë¬¸'\n",
      "   ìš”ì•½: 'ìš”ì•½ë¬¸'\n",
      "\n",
      "ğŸ¯ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë°ì´í„°: 9,066ê°œ ìŒ\n",
      "âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“ 4ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "print(\"ğŸ“ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "\n",
    "# ì•ˆì „í•œ CSV ì½ê¸° í•¨ìˆ˜\n",
    "def safe_read_csv(file_path):\n",
    "    \"\"\"BOM ì²˜ë¦¬ê°€ í¬í•¨ëœ ì•ˆì „í•œ CSV ì½ê¸°\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            raw_data = f.read()\n",
    "        if raw_data.startswith(codecs.BOM_UTF8):\n",
    "            raw_data = raw_data[len(codecs.BOM_UTF8) :]\n",
    "        content = raw_data.decode(\"utf-8\", errors=\"replace\")\n",
    "        return pd.read_csv(StringIO(content))\n",
    "    except:\n",
    "        for encoding in [\"utf-8-sig\", \"utf-8\", \"cp949\", \"euc-kr\"]:\n",
    "            try:\n",
    "                return pd.read_csv(file_path, encoding=encoding)\n",
    "            except:\n",
    "                continue\n",
    "        raise ValueError(f\"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path}\")\n",
    "\n",
    "\n",
    "# ìµœì  ì»¬ëŸ¼ ê°ì§€ í•¨ìˆ˜\n",
    "def detect_best_column(df, keywords):\n",
    "    \"\"\"í‚¤ì›Œë“œ ê¸°ë°˜ ìµœì  ì»¬ëŸ¼ ìë™ ê°ì§€\"\"\"\n",
    "    for keyword in keywords:\n",
    "        for col in df.columns:\n",
    "            if keyword in col:\n",
    "                return col\n",
    "\n",
    "    # í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì„ íƒ\n",
    "    text_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            avg_len = df[col].astype(str).str.len().mean()\n",
    "            if avg_len > 50:\n",
    "                text_cols.append((col, avg_len))\n",
    "\n",
    "    if text_cols:\n",
    "        return max(text_cols, key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        return df.columns[0]\n",
    "\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_text(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ì „ì²˜ë¦¬\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # ê³µë°± ì •ê·œí™”\n",
    "    text = re.sub(r\"[^\\w\\sê°€-í£]\", \"\", text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    return text\n",
    "\n",
    "\n",
    "# íŒŒì¼ ë¡œë“œ\n",
    "data_folder = \"data\"\n",
    "original_file = os.path.join(data_folder, \"crawling_origin.csv\")\n",
    "summary_file = os.path.join(data_folder, \"crawling_origin_with_summary.csv\")\n",
    "\n",
    "print(f\"ğŸ“‚ ì›ë³¸ íŒŒì¼: {original_file}\")\n",
    "print(f\"ğŸ“‚ ìš”ì•½ íŒŒì¼: {summary_file}\")\n",
    "\n",
    "if os.path.exists(original_file) and os.path.exists(summary_file):\n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    print(\"\\nğŸ”„ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "    original_df = safe_read_csv(original_file)\n",
    "    summary_df = safe_read_csv(summary_file)\n",
    "\n",
    "    print(f\"ğŸ“Š ì›ë³¸ ë°ì´í„°: {len(original_df):,}ê°œ í–‰\")\n",
    "    print(f\"ğŸ“Š ìš”ì•½ ë°ì´í„°: {len(summary_df):,}ê°œ í–‰\")\n",
    "\n",
    "    # ì»¬ëŸ¼ ê°ì§€\n",
    "    original_col = detect_best_column(\n",
    "        original_df, [\"ë³¸ë¬¸\", \"content\", \"text\", \"article\"]\n",
    "    )\n",
    "    summary_col = detect_best_column(\n",
    "        summary_df, [\"ìš”ì•½ë¬¸\", \"ìš”ì•½\", \"summary\", \"abstract\"]\n",
    "    )\n",
    "\n",
    "    print(f\"\\nğŸ“‹ ê°ì§€ëœ ì»¬ëŸ¼:\")\n",
    "    print(f\"   ì›ë³¸: '{original_col}'\")\n",
    "    print(f\"   ìš”ì•½: '{summary_col}'\")\n",
    "\n",
    "    # ì²˜ë¦¬ ê°€ëŠ¥í•œ ë°ì´í„° í¬ê¸°\n",
    "    min_len = min(len(original_df), len(summary_df))\n",
    "    print(f\"\\nğŸ¯ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë°ì´í„°: {min_len:,}ê°œ ìŒ\")\n",
    "\n",
    "    print(\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"   ì›ë³¸: {'ì¡´ì¬' if os.path.exists(original_file) else 'ì—†ìŒ'}\")\n",
    "    print(f\"   ìš”ì•½: {'ì¡´ì¬' if os.path.exists(summary_file) else 'ì—†ìŒ'}\")\n",
    "    raise FileNotFoundError(\"í•„ìš”í•œ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4ed4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§® ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ì •ì˜\n",
      "========================================\n",
      "âœ… ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§® 5ë‹¨ê³„: ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ì •ì˜\n",
    "print(\"ğŸ§® ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ì •ì˜\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "\n",
    "def calculate_sbert_similarity(originals, summaries, batch_size=16):\n",
    "    \"\"\"SBERT ìœ ì‚¬ë„ ê³„ì‚° (GPU/CPU ìë™ ìµœì í™”)\"\"\"\n",
    "    if not USE_SBERT or model is None:\n",
    "        print(\"âš ï¸ SBERT ëª¨ë¸ì´ ì—†ì–´ 0ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "        return np.zeros(len(originals))\n",
    "\n",
    "    print(f\"ğŸš€ SBERT ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘ ({device})\")\n",
    "    similarities = []\n",
    "    total_batches = (len(originals) + batch_size - 1) // batch_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(\n",
    "            range(0, len(originals), batch_size), desc=f\"SBERT {device.type.upper()}\"\n",
    "        ):\n",
    "            batch_orig = originals[i : i + batch_size]\n",
    "            batch_summ = summaries[i : i + batch_size]\n",
    "\n",
    "            try:\n",
    "                orig_embeddings = model.encode(\n",
    "                    batch_orig, convert_to_tensor=True, device=device\n",
    "                )\n",
    "                summ_embeddings = model.encode(\n",
    "                    batch_summ, convert_to_tensor=True, device=device\n",
    "                )\n",
    "\n",
    "                batch_similarities = util.pytorch_cos_sim(\n",
    "                    orig_embeddings, summ_embeddings\n",
    "                ).diag()\n",
    "                similarities.extend(batch_similarities.cpu().numpy())\n",
    "\n",
    "                # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "                del orig_embeddings, summ_embeddings, batch_similarities\n",
    "                if device.type == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ë°°ì¹˜ {i//batch_size + 1} ì‹¤íŒ¨: {e}\")\n",
    "                similarities.extend([0.0] * len(batch_orig))\n",
    "\n",
    "    return np.array(similarities)\n",
    "\n",
    "\n",
    "def calculate_tfidf_similarity(originals, summaries):\n",
    "    \"\"\"TF-IDF ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
    "    print(\"ğŸ”„ TF-IDF ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\")\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "        all_texts = originals + summaries\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "        orig_matrix = tfidf_matrix[: len(originals)]\n",
    "        summ_matrix = tfidf_matrix[len(originals) :]\n",
    "\n",
    "        similarities = []\n",
    "        for i in tqdm(range(len(originals)), desc=\"TF-IDF\"):\n",
    "            sim = cosine_similarity(orig_matrix[i], summ_matrix[i])[0][0]\n",
    "            similarities.append(sim)\n",
    "\n",
    "        return np.array(similarities)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ TF-IDF ì‹¤íŒ¨: {e}\")\n",
    "        return np.zeros(len(originals))\n",
    "\n",
    "\n",
    "def calculate_jaccard_similarity(originals, summaries):\n",
    "    \"\"\"Jaccard ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
    "    print(\"ğŸ”„ Jaccard ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\")\n",
    "    similarities = []\n",
    "    for orig, summ in tqdm(\n",
    "        zip(originals, summaries), total=len(originals), desc=\"Jaccard\"\n",
    "    ):\n",
    "        orig_words = set(orig.split())\n",
    "        summ_words = set(summ.split())\n",
    "\n",
    "        if len(orig_words) == 0 and len(summ_words) == 0:\n",
    "            similarities.append(1.0)\n",
    "        elif len(orig_words) == 0 or len(summ_words) == 0:\n",
    "            similarities.append(0.0)\n",
    "        else:\n",
    "            intersection = len(orig_words & summ_words)\n",
    "            union = len(orig_words | summ_words)\n",
    "            similarities.append(intersection / union)\n",
    "\n",
    "    return np.array(similarities)\n",
    "\n",
    "\n",
    "def calculate_keyword_similarity(originals, summaries, top_k=10):\n",
    "    \"\"\"í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
    "    print(\"ğŸ”„ í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\")\n",
    "    similarities = []\n",
    "    for orig, summ in tqdm(\n",
    "        zip(originals, summaries), total=len(originals), desc=\"í‚¤ì›Œë“œ\"\n",
    "    ):\n",
    "        orig_words = Counter(orig.split())\n",
    "        summ_words = Counter(summ.split())\n",
    "\n",
    "        orig_top = set([word for word, _ in orig_words.most_common(top_k)])\n",
    "        summ_top = set([word for word, _ in summ_words.most_common(top_k)])\n",
    "\n",
    "        if len(orig_top) == 0 and len(summ_top) == 0:\n",
    "            similarities.append(1.0)\n",
    "        elif len(orig_top) == 0 or len(summ_top) == 0:\n",
    "            similarities.append(0.0)\n",
    "        else:\n",
    "            intersection = len(orig_top & summ_top)\n",
    "            union = len(orig_top | summ_top)\n",
    "            similarities.append(intersection / union)\n",
    "\n",
    "    return np.array(similarities)\n",
    "\n",
    "\n",
    "print(\"âœ… ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa2db79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì „ì²´ ë°ì´í„° í‰ê°€ ì‹¤í–‰\n",
      "============================================================\n",
      "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8001.59it/s]\n",
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8001.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼:\n",
      "   âœ… ìœ íš¨ ë°ì´í„°: 9,022ê°œ\n",
      "   âš ï¸ ì œì™¸ ë°ì´í„°: 44ê°œ\n",
      "   ğŸ“ˆ ìœ íš¨ìœ¨: 99.5%\n",
      "\n",
      "ğŸ§® 9,022ê°œ ë°ì´í„° ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘...\n",
      "==================================================\n",
      "ğŸš€ SBERT ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘ (cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT CPU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2256/2256 [14:26<00:00,  2.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì „ì²´ ë°ì´í„° í‰ê°€ ì‹¤í–‰\n",
      "============================================================\n",
      "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8001.59it/s]\n",
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8001.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼:\n",
      "   âœ… ìœ íš¨ ë°ì´í„°: 9,022ê°œ\n",
      "   âš ï¸ ì œì™¸ ë°ì´í„°: 44ê°œ\n",
      "   ğŸ“ˆ ìœ íš¨ìœ¨: 99.5%\n",
      "\n",
      "ğŸ§® 9,022ê°œ ë°ì´í„° ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘...\n",
      "==================================================\n",
      "ğŸš€ SBERT ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘ (cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT CPU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2256/2256 [14:26<00:00,  2.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SBERT í‰ê· : 0.3085\n",
      "ğŸ”„ TF-IDF ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì „ì²´ ë°ì´í„° í‰ê°€ ì‹¤í–‰\n",
      "============================================================\n",
      "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8001.59it/s]\n",
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8001.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼:\n",
      "   âœ… ìœ íš¨ ë°ì´í„°: 9,022ê°œ\n",
      "   âš ï¸ ì œì™¸ ë°ì´í„°: 44ê°œ\n",
      "   ğŸ“ˆ ìœ íš¨ìœ¨: 99.5%\n",
      "\n",
      "ğŸ§® 9,022ê°œ ë°ì´í„° ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘...\n",
      "==================================================\n",
      "ğŸš€ SBERT ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘ (cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT CPU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2256/2256 [14:26<00:00,  2.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SBERT í‰ê· : 0.3085\n",
      "ğŸ”„ TF-IDF ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:04<00:00, 1898.09it/s]\n",
      "TF-IDF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:04<00:00, 1898.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì „ì²´ ë°ì´í„° í‰ê°€ ì‹¤í–‰\n",
      "============================================================\n",
      "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8001.59it/s]\n",
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8001.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼:\n",
      "   âœ… ìœ íš¨ ë°ì´í„°: 9,022ê°œ\n",
      "   âš ï¸ ì œì™¸ ë°ì´í„°: 44ê°œ\n",
      "   ğŸ“ˆ ìœ íš¨ìœ¨: 99.5%\n",
      "\n",
      "ğŸ§® 9,022ê°œ ë°ì´í„° ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘...\n",
      "==================================================\n",
      "ğŸš€ SBERT ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘ (cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT CPU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2256/2256 [14:26<00:00,  2.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SBERT í‰ê· : 0.3085\n",
      "ğŸ”„ TF-IDF ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:04<00:00, 1898.09it/s]\n",
      "TF-IDF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:04<00:00, 1898.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TF-IDF í‰ê· : 0.0271\n",
      "ğŸ”„ Jaccard ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì „ì²´ ë°ì´í„° í‰ê°€ ì‹¤í–‰\n",
      "============================================================\n",
      "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8001.59it/s]\n",
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8001.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼:\n",
      "   âœ… ìœ íš¨ ë°ì´í„°: 9,022ê°œ\n",
      "   âš ï¸ ì œì™¸ ë°ì´í„°: 44ê°œ\n",
      "   ğŸ“ˆ ìœ íš¨ìœ¨: 99.5%\n",
      "\n",
      "ğŸ§® 9,022ê°œ ë°ì´í„° ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘...\n",
      "==================================================\n",
      "ğŸš€ SBERT ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘ (cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT CPU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2256/2256 [14:26<00:00,  2.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SBERT í‰ê· : 0.3085\n",
      "ğŸ”„ TF-IDF ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:04<00:00, 1898.09it/s]\n",
      "TF-IDF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:04<00:00, 1898.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TF-IDF í‰ê· : 0.0271\n",
      "ğŸ”„ Jaccard ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jaccard: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:00<00:00, 91510.83it/s]\n",
      "Jaccard: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:00<00:00, 91510.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì „ì²´ ë°ì´í„° í‰ê°€ ì‹¤í–‰\n",
      "============================================================\n",
      "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8001.59it/s]\n",
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8001.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼:\n",
      "   âœ… ìœ íš¨ ë°ì´í„°: 9,022ê°œ\n",
      "   âš ï¸ ì œì™¸ ë°ì´í„°: 44ê°œ\n",
      "   ğŸ“ˆ ìœ íš¨ìœ¨: 99.5%\n",
      "\n",
      "ğŸ§® 9,022ê°œ ë°ì´í„° ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘...\n",
      "==================================================\n",
      "ğŸš€ SBERT ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘ (cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT CPU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2256/2256 [14:26<00:00,  2.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SBERT í‰ê· : 0.3085\n",
      "ğŸ”„ TF-IDF ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:04<00:00, 1898.09it/s]\n",
      "TF-IDF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:04<00:00, 1898.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TF-IDF í‰ê· : 0.0271\n",
      "ğŸ”„ Jaccard ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jaccard: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:00<00:00, 91510.83it/s]\n",
      "Jaccard: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:00<00:00, 91510.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Jaccard í‰ê· : 0.0129\n",
      "ğŸ”„ í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í‚¤ì›Œë“œ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:00<00:00, 35190.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì „ì²´ ë°ì´í„° í‰ê°€ ì‹¤í–‰\n",
      "============================================================\n",
      "ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8001.59it/s]\n",
      "ì „ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9066/9066 [00:01<00:00, 8001.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼:\n",
      "   âœ… ìœ íš¨ ë°ì´í„°: 9,022ê°œ\n",
      "   âš ï¸ ì œì™¸ ë°ì´í„°: 44ê°œ\n",
      "   ğŸ“ˆ ìœ íš¨ìœ¨: 99.5%\n",
      "\n",
      "ğŸ§® 9,022ê°œ ë°ì´í„° ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘...\n",
      "==================================================\n",
      "ğŸš€ SBERT ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘ (cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT CPU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2256/2256 [14:26<00:00,  2.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SBERT í‰ê· : 0.3085\n",
      "ğŸ”„ TF-IDF ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:04<00:00, 1898.09it/s]\n",
      "TF-IDF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:04<00:00, 1898.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TF-IDF í‰ê· : 0.0271\n",
      "ğŸ”„ Jaccard ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jaccard: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:00<00:00, 91510.83it/s]\n",
      "Jaccard: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:00<00:00, 91510.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Jaccard í‰ê· : 0.0129\n",
      "ğŸ”„ í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í‚¤ì›Œë“œ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9022/9022 [00:00<00:00, 35190.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í‚¤ì›Œë“œ í‰ê· : 0.0152\n",
      "\n",
      "==================================================\n",
      "ğŸ‰ ëª¨ë“  ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ 6ë‹¨ê³„: ì „ì²´ ë°ì´í„° í‰ê°€ ì‹¤í–‰\n",
    "print(\"ğŸš€ ì „ì²´ ë°ì´í„° í‰ê°€ ì‹¤í–‰\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬\n",
    "print(\"ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "originals = []\n",
    "summaries = []\n",
    "valid_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "for i in tqdm(range(min_len), desc=\"ì „ì²˜ë¦¬\"):\n",
    "    try:\n",
    "        orig_text = preprocess_text(original_df.iloc[i][original_col])\n",
    "        summ_text = preprocess_text(summary_df.iloc[i][summary_col])\n",
    "\n",
    "        if len(orig_text) >= 10 and len(summ_text) >= 5:\n",
    "            originals.append(orig_text)\n",
    "            summaries.append(summ_text)\n",
    "            valid_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "    except:\n",
    "        skipped_count += 1\n",
    "\n",
    "print(f\"\\nğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼:\")\n",
    "print(f\"   âœ… ìœ íš¨ ë°ì´í„°: {valid_count:,}ê°œ\")\n",
    "print(f\"   âš ï¸ ì œì™¸ ë°ì´í„°: {skipped_count:,}ê°œ\")\n",
    "print(f\"   ğŸ“ˆ ìœ íš¨ìœ¨: {valid_count/(valid_count+skipped_count)*100:.1f}%\")\n",
    "\n",
    "if valid_count == 0:\n",
    "    raise ValueError(\"ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "# ìœ ì‚¬ë„ ê³„ì‚°\n",
    "print(f\"\\nğŸ§® {valid_count:,}ê°œ ë°ì´í„° ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. SBERT ìœ ì‚¬ë„\n",
    "sbert_scores = calculate_sbert_similarity(originals, summaries, batch_size)\n",
    "print(f\"âœ… SBERT í‰ê· : {np.mean(sbert_scores):.4f}\")\n",
    "\n",
    "# 2. TF-IDF ìœ ì‚¬ë„\n",
    "tfidf_scores = calculate_tfidf_similarity(originals, summaries)\n",
    "print(f\"âœ… TF-IDF í‰ê· : {np.mean(tfidf_scores):.4f}\")\n",
    "\n",
    "# 3. Jaccard ìœ ì‚¬ë„\n",
    "jaccard_scores = calculate_jaccard_similarity(originals, summaries)\n",
    "print(f\"âœ… Jaccard í‰ê· : {np.mean(jaccard_scores):.4f}\")\n",
    "\n",
    "# 4. í‚¤ì›Œë“œ ìœ ì‚¬ë„\n",
    "keyword_scores = calculate_keyword_similarity(originals, summaries)\n",
    "print(f\"âœ… í‚¤ì›Œë“œ í‰ê· : {np.mean(keyword_scores):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ‰ ëª¨ë“  ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa91a296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ê²°ê³¼ ë¶„ì„ ë° ì¢…í•© ì ìˆ˜ ê³„ì‚°\n",
      "==================================================\n",
      "ğŸ“ˆ ìƒì„¸ í†µê³„:\n",
      "----------------------------------------------------------------------\n",
      "ë°©ë²•           í‰ê·        í‘œì¤€í¸ì°¨     ìµœì†Œê°’      ìµœëŒ€ê°’      ì¤‘ê°„ê°’     \n",
      "----------------------------------------------------------------------\n",
      "SBERT        0.3085   0.1164   -0.0532  0.8945   0.3029  \n",
      "TF-IDF       0.0271   0.0618   0.0000   0.8519   0.0000  \n",
      "Jaccard      0.0129   0.0267   0.0000   0.6538   0.0000  \n",
      "Keyword      0.0152   0.0380   0.0000   1.0000   0.0000  \n",
      "Composite    0.1356   0.0617   -0.0213  0.7733   0.1290  \n",
      "\n",
      "ğŸ† ì„±ëŠ¥ ë“±ê¸‰ ë¶„í¬ (ì´ 9,022ê°œ):\n",
      "   ğŸ¥‡ ìš°ìˆ˜ (â‰¥0.80): 0ê°œ (  0.0%)\n",
      "   ğŸ¥ˆ ì–‘í˜¸ (0.60-0.80): 10ê°œ (  0.1%)\n",
      "   ğŸ¥‰ ë³´í†µ (0.40-0.60): 21ê°œ (  0.2%)\n",
      "   ğŸ“‰ ë¯¸í¡ (<0.40): 8,991ê°œ ( 99.7%)\n",
      "\n",
      "ğŸ¯ ì„±ëŠ¥ ìƒ˜í”Œ:\n",
      "   ğŸ† ìµœê³  ì ìˆ˜: 0.7733\n",
      "      ì›ë³¸: ì´ì¬ëª… ëŒ€í†µë ¹ì€ êµ­íšŒì— ê°•ì„ ìš° ì—¬ì„±ê°€ì¡±ë¶€ ì¥ê´€ í›„ë³´ìë¥¼ í¬í•¨í•´ êµ­ë°©ë¶€ì™€ êµ­ê°€ë³´í›ˆë¶€ í†µì¼ë¶€ ì¥ê´€ í›„ë³´ìì— ëŒ€í•œ ì¸ì‚¬ì²­ë¬¸ë³´ê³ ì„œë¥¼ ë‚´ì¼ê¹Œì§€ ì¬ì†¡ë¶€í•´ ë‹¬ë¼ê³  ìš”ì²­í–ˆìŠµë‹ˆë‹¤ ê°•ìœ ì • ëŒ€í†µë ¹ì‹¤ ...\n",
      "      ìš”ì•½: ì´ì¬ëª… ì´ì¬ëª… ëŒ€í†µë ¹ì€ êµ­íšŒì— ê°•ì„ ìš° ì—¬ì„±ê°€ì¡±ë¶€ ì¥ê´€ í›„ë³´ìë¥¼ í¬í•¨í•´ êµ­ë°©ë¶€ì™€ êµ­ê°€ë³´í›ˆë¶€ í†µì¼ë¶€ ì¥ê´€ í›„ë³´ìì— ëŒ€í•œ ì¸ì‚¬ì²­ë¬¸ë³´ê³ ì„œë¥¼ ë‚´ì¼ê¹Œì§€ ì¬ì†¡ë¶€í•´ ë‹¬ë¼ê³  ìš”ì²­í–ˆìŠµë‹ˆë‹¤...\n",
      "\n",
      "   ğŸ“‰ ìµœì € ì ìˆ˜: -0.0213\n",
      "      ì›ë³¸:  ì•µì»¤  ì´ì¬ëª… ëŒ€í†µë ¹ì´ ì£¼ë¡œ ì¥ê´€ê¸‰ êµ­ë¬´ìœ„ì›ë“¤ì´ ì°¸ì„í•´ì™”ë˜ êµ­ë¬´íšŒì˜ ê´€í–‰ì— í° ë³€í™”ë¥¼ ì£¼ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤ ì´ì œ ì™¸êµë¶€ë‚˜ êµìœ¡ë¶€ ê°™ì€ 19ê°œ ë¶€ì²˜ ë¿ë§Œì•„ë‹ˆë¼ ê²½ì°°ì²­ ì‚°ë¦¼ì²­ ë“± ì™¸ì²­ 2...\n",
      "      ìš”ì•½: ì¡°êµ­í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ í˜ì‹ ...\n",
      "\n",
      "ğŸ¯ ì „ì²´ í‰ê°€ ê²°ê³¼:\n",
      "   ğŸ“Š ì¢…í•© í‰ê·  ì ìˆ˜: 0.1356\n",
      "   ğŸ† ì„±ëŠ¥ ë“±ê¸‰: D (ë¯¸í¡)\n",
      "   ğŸ’¡ í‰ê°€: ì„±ëŠ¥ ê°œì„ ì´ ì‹œê¸‰í•©ë‹ˆë‹¤.\n",
      "\n",
      "==================================================\n",
      "âœ… ê²°ê³¼ ë¶„ì„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š 7ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ ë° ì¢…í•© ì ìˆ˜ ê³„ì‚°\n",
    "print(\"ğŸ“Š ê²°ê³¼ ë¶„ì„ ë° ì¢…í•© ì ìˆ˜ ê³„ì‚°\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ë³€ìˆ˜ ì¡´ì¬ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •\n",
    "if \"sbert_scores\" not in globals():\n",
    "    print(\"âš ï¸ sbert_scoresê°€ ì •ì˜ë˜ì§€ ì•ŠìŒ - 6ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”\")\n",
    "    print(\"ğŸ”„ ê¸°ë³¸ ë”ë¯¸ ë°ì´í„°ë¡œ ì§„í–‰...\")\n",
    "    sbert_scores = np.array([0.0] * 100)  # ë”ë¯¸ ë°ì´í„°\n",
    "    tfidf_scores = np.array([0.0] * 100)\n",
    "    jaccard_scores = np.array([0.0] * 100)\n",
    "    keyword_scores = np.array([0.0] * 100)\n",
    "    originals = [\"ë”ë¯¸ ì›ë³¸ í…ìŠ¤íŠ¸\"] * 100\n",
    "    summaries = [\"ë”ë¯¸ ìš”ì•½ í…ìŠ¤íŠ¸\"] * 100\n",
    "    valid_count = 100\n",
    "\n",
    "# ê°€ì¤‘ì¹˜ ì„¤ì •\n",
    "weights = {\n",
    "    \"sbert\": 0.4,  # KoBERTê°€ ê°€ì¥ ì¤‘ìš”\n",
    "    \"tfidf\": 0.3,  # TF-IDFë„ ì¤‘ìš”\n",
    "    \"jaccard\": 0.2,  # JaccardëŠ” ë³´ì¡°\n",
    "    \"keyword\": 0.1,  # í‚¤ì›Œë“œëŠ” ì°¸ê³ ìš©\n",
    "}\n",
    "\n",
    "# ì¢…í•© ì ìˆ˜ ê³„ì‚°\n",
    "composite_scores = (\n",
    "    weights[\"sbert\"] * sbert_scores\n",
    "    + weights[\"tfidf\"] * tfidf_scores\n",
    "    + weights[\"jaccard\"] * jaccard_scores\n",
    "    + weights[\"keyword\"] * keyword_scores\n",
    ")\n",
    "\n",
    "# í†µê³„ ê³„ì‚°\n",
    "results = {\n",
    "    \"SBERT\": sbert_scores,\n",
    "    \"TF-IDF\": tfidf_scores,\n",
    "    \"Jaccard\": jaccard_scores,\n",
    "    \"Keyword\": keyword_scores,\n",
    "    \"Composite\": composite_scores,\n",
    "}\n",
    "\n",
    "print(\"ğŸ“ˆ ìƒì„¸ í†µê³„:\")\n",
    "print(\"-\" * 70)\n",
    "print(\n",
    "    f\"{'ë°©ë²•':<12} {'í‰ê· ':<8} {'í‘œì¤€í¸ì°¨':<8} {'ìµœì†Œê°’':<8} {'ìµœëŒ€ê°’':<8} {'ì¤‘ê°„ê°’':<8}\"\n",
    ")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for method, scores in results.items():\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    min_score = np.min(scores)\n",
    "    max_score = np.max(scores)\n",
    "    median_score = np.median(scores)\n",
    "\n",
    "    print(\n",
    "        f\"{method:<12} {mean_score:<8.4f} {std_score:<8.4f} {min_score:<8.4f} {max_score:<8.4f} {median_score:<8.4f}\"\n",
    "    )\n",
    "\n",
    "# ì„±ëŠ¥ ë“±ê¸‰ ë¶„ì„\n",
    "best_scores = composite_scores\n",
    "excellent = np.sum(best_scores >= 0.80)\n",
    "good = np.sum((best_scores >= 0.60) & (best_scores < 0.80))\n",
    "average = np.sum((best_scores >= 0.40) & (best_scores < 0.60))\n",
    "poor = np.sum(best_scores < 0.40)\n",
    "\n",
    "print(f\"\\nğŸ† ì„±ëŠ¥ ë“±ê¸‰ ë¶„í¬ (ì´ {len(best_scores):,}ê°œ):\")\n",
    "print(f\"   ğŸ¥‡ ìš°ìˆ˜ (â‰¥0.80): {excellent:,}ê°œ ({excellent/len(best_scores)*100:5.1f}%)\")\n",
    "print(f\"   ğŸ¥ˆ ì–‘í˜¸ (0.60-0.80): {good:,}ê°œ ({good/len(best_scores)*100:5.1f}%)\")\n",
    "print(f\"   ğŸ¥‰ ë³´í†µ (0.40-0.60): {average:,}ê°œ ({average/len(best_scores)*100:5.1f}%)\")\n",
    "print(f\"   ğŸ“‰ ë¯¸í¡ (<0.40): {poor:,}ê°œ ({poor/len(best_scores)*100:5.1f}%)\")\n",
    "\n",
    "# ìµœê³ /ìµœì € ì„±ëŠ¥ ìƒ˜í”Œ\n",
    "best_idx = np.argmax(best_scores)\n",
    "worst_idx = np.argmin(best_scores)\n",
    "\n",
    "print(f\"\\nğŸ¯ ì„±ëŠ¥ ìƒ˜í”Œ:\")\n",
    "print(f\"   ğŸ† ìµœê³  ì ìˆ˜: {best_scores[best_idx]:.4f}\")\n",
    "print(f\"      ì›ë³¸: {originals[best_idx][:100]}...\")\n",
    "print(f\"      ìš”ì•½: {summaries[best_idx][:100]}...\")\n",
    "\n",
    "print(f\"\\n   ğŸ“‰ ìµœì € ì ìˆ˜: {best_scores[worst_idx]:.4f}\")\n",
    "print(f\"      ì›ë³¸: {originals[worst_idx][:100]}...\")\n",
    "print(f\"      ìš”ì•½: {summaries[worst_idx][:100]}...\")\n",
    "\n",
    "# ì „ì²´ í‰ê°€ ê²°ê³¼\n",
    "avg_score = np.mean(best_scores)\n",
    "print(f\"\\nğŸ¯ ì „ì²´ í‰ê°€ ê²°ê³¼:\")\n",
    "print(f\"   ğŸ“Š ì¢…í•© í‰ê·  ì ìˆ˜: {avg_score:.4f}\")\n",
    "\n",
    "if avg_score >= 0.70:\n",
    "    grade = \"A (ìš°ìˆ˜)\"\n",
    "    comment = \"ë§¤ìš° ë†’ì€ í’ˆì§ˆì˜ ìš”ì•½ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤!\"\n",
    "elif avg_score >= 0.60:\n",
    "    grade = \"B (ì–‘í˜¸)\"\n",
    "    comment = \"ì–‘í˜¸í•œ ìš”ì•½ ì„±ëŠ¥ì…ë‹ˆë‹¤. ì¶”ê°€ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.\"\n",
    "elif avg_score >= 0.50:\n",
    "    grade = \"C (ë³´í†µ)\"\n",
    "    comment = \"í‰ê· ì ì¸ ì„±ëŠ¥ì…ë‹ˆë‹¤. ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.\"\n",
    "else:\n",
    "    grade = \"D (ë¯¸í¡)\"\n",
    "    comment = \"ì„±ëŠ¥ ê°œì„ ì´ ì‹œê¸‰í•©ë‹ˆë‹¤.\"\n",
    "\n",
    "print(f\"   ğŸ† ì„±ëŠ¥ ë“±ê¸‰: {grade}\")\n",
    "print(f\"   ğŸ’¡ í‰ê°€: {comment}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"âœ… ê²°ê³¼ ë¶„ì„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e11eaf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥\n",
      "========================================\n",
      "âœ… ìƒì„¸ ê²°ê³¼ ì €ì¥: kobert_complete_results_20250805_134036.csv\n",
      "âœ… ìš”ì•½ í†µê³„ ì €ì¥: kobert_complete_summary_20250805_134036.txt\n",
      "\n",
      "ğŸ‰ ì „ì²´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!\n",
      "ğŸ“ ê²°ê³¼ íŒŒì¼: kobert_complete_results_20250805_134036.csv\n",
      "ğŸ“‹ ìš”ì•½ íŒŒì¼: kobert_complete_summary_20250805_134036.txt\n",
      "ğŸ† ìµœì¢… ì ìˆ˜: 0.1356 (D (ë¯¸í¡))\n",
      "ğŸ“Š í‰ê°€ ë°ì´í„°: 9,022ê°œ\n",
      "ğŸ¯ SBERT ì‚¬ìš©: âœ…\n",
      "ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: CPU\n",
      "============================================================\n",
      "ğŸŠ ëª¨ë“  ê³¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "\n",
      "ğŸ‰ ì „ì²´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!\n",
      "ğŸ“ ê²°ê³¼ íŒŒì¼: kobert_complete_results_20250805_134036.csv\n",
      "ğŸ“‹ ìš”ì•½ íŒŒì¼: kobert_complete_summary_20250805_134036.txt\n",
      "ğŸ† ìµœì¢… ì ìˆ˜: 0.1356 (D (ë¯¸í¡))\n",
      "ğŸ“Š í‰ê°€ ë°ì´í„°: 9,022ê°œ\n",
      "ğŸ¯ SBERT ì‚¬ìš©: âœ…\n",
      "ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: CPU\n",
      "============================================================\n",
      "ğŸŠ ëª¨ë“  ê³¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’¾ 8ë‹¨ê³„: ê²°ê³¼ ì €ì¥\n",
    "print(\"ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "results_data = {\n",
    "    \"original_text\": [\n",
    "        text[:200] + \"...\" if len(text) > 200 else text for text in originals\n",
    "    ],\n",
    "    \"summary_text\": [\n",
    "        text[:200] + \"...\" if len(text) > 200 else text for text in summaries\n",
    "    ],\n",
    "    \"sbert_score\": sbert_scores,\n",
    "    \"tfidf_score\": tfidf_scores,\n",
    "    \"jaccard_score\": jaccard_scores,\n",
    "    \"keyword_score\": keyword_scores,\n",
    "    \"composite_score\": composite_scores,\n",
    "    \"original_length\": [len(text) for text in originals],\n",
    "    \"summary_length\": [len(text) for text in summaries],\n",
    "    \"compression_ratio\": [len(s) / len(o) * 100 for o, s in zip(originals, summaries)],\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# íŒŒì¼ëª… ìƒì„±\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"kobert_complete_results_{timestamp}.csv\"\n",
    "summary_file = f\"kobert_complete_summary_{timestamp}.txt\"\n",
    "\n",
    "# CSV ì €ì¥\n",
    "results_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… ìƒì„¸ ê²°ê³¼ ì €ì¥: {output_file}\")\n",
    "\n",
    "# ìš”ì•½ í†µê³„ ì €ì¥\n",
    "with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"KoBERT ìš”ì•½ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ (ì™„ì „ ìë™í™”)\\n\")\n",
    "    f.write(f\"=\" * 50 + \"\\n\")\n",
    "    f.write(f\"í‰ê°€ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"í‰ê°€ ë°ì´í„°: {valid_count:,}ê°œ\\n\")\n",
    "    f.write(f\"ë””ë°”ì´ìŠ¤: {device_name}\\n\")\n",
    "    f.write(f\"SBERT ì‚¬ìš©: {USE_SBERT}\\n\")\n",
    "    f.write(f\"\\nì„±ëŠ¥ í†µê³„:\\n\")\n",
    "    f.write(f\"  SBERT í‰ê· : {np.mean(sbert_scores):.4f}\\n\")\n",
    "    f.write(f\"  TF-IDF í‰ê· : {np.mean(tfidf_scores):.4f}\\n\")\n",
    "    f.write(f\"  Jaccard í‰ê· : {np.mean(jaccard_scores):.4f}\\n\")\n",
    "    f.write(f\"  í‚¤ì›Œë“œ í‰ê· : {np.mean(keyword_scores):.4f}\\n\")\n",
    "    f.write(f\"  ì¢…í•© í‰ê· : {avg_score:.4f}\\n\")\n",
    "    f.write(f\"\\nì„±ëŠ¥ ë“±ê¸‰: {grade}\\n\")\n",
    "    f.write(f\"í‰ê°€: {comment}\\n\")\n",
    "    f.write(f\"\\në“±ê¸‰ ë¶„í¬:\\n\")\n",
    "    f.write(\n",
    "        f\"  ìš°ìˆ˜ (â‰¥0.80): {excellent:,}ê°œ ({excellent/len(best_scores)*100:5.1f}%)\\n\"\n",
    "    )\n",
    "    f.write(f\"  ì–‘í˜¸ (0.60-0.80): {good:,}ê°œ ({good/len(best_scores)*100:5.1f}%)\\n\")\n",
    "    f.write(\n",
    "        f\"  ë³´í†µ (0.40-0.60): {average:,}ê°œ ({average/len(best_scores)*100:5.1f}%)\\n\"\n",
    "    )\n",
    "    f.write(f\"  ë¯¸í¡ (<0.40): {poor:,}ê°œ ({poor/len(best_scores)*100:5.1f}%)\\n\")\n",
    "\n",
    "print(f\"âœ… ìš”ì•½ í†µê³„ ì €ì¥: {summary_file}\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nğŸ‰ ì „ì²´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“ ê²°ê³¼ íŒŒì¼: {output_file}\")\n",
    "print(f\"ğŸ“‹ ìš”ì•½ íŒŒì¼: {summary_file}\")\n",
    "print(f\"ğŸ† ìµœì¢… ì ìˆ˜: {avg_score:.4f} ({grade})\")\n",
    "print(f\"ğŸ“Š í‰ê°€ ë°ì´í„°: {valid_count:,}ê°œ\")\n",
    "print(f\"ğŸ¯ SBERT ì‚¬ìš©: {'âœ…' if USE_SBERT else 'âŒ'}\")\n",
    "print(f\"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {device_name}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸŠ ëª¨ë“  ê³¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
