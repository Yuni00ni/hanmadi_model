{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e925ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch\n",
    "# KoBART 파인튜닝\n",
    "# 생성 요약 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9684c861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 CPU 환경용 KoBERT 평가 시스템 시작\n",
      "============================================================\n",
      "📁 데이터 로드 중...\n",
      "✅ 데이터 로드 완료: 원본 10,952개, 요약 9,066개\n",
      "📋 사용 컬럼: 원본='본문', 요약='요약문'\n",
      "\n",
      "🔄 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8634.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 전처리 완료: 9,022개 유효 데이터\n",
      "\n",
      "🧮 유사도 계산 중...\n",
      "🔄 TF-IDF 계산 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF: 100%|██████████| 9022/9022 [00:04<00:00, 1950.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Jaccard 계산 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jaccard: 100%|██████████| 9022/9022 [00:00<00:00, 92682.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 키워드 계산 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "키워드: 100%|██████████| 9022/9022 [00:00<00:00, 35170.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모든 유사도 계산 완료!\n",
      "   TF-IDF 평균: 0.0361\n",
      "   Jaccard 평균: 0.0129\n",
      "   키워드 평균: 0.0152\n",
      "\n",
      "🎯 종합 평균 점수: 0.0250\n",
      "✅ 결과 저장: cpu_evaluation_results_20250805_132544.csv\n",
      "🎊 CPU 평가 완료!\n"
     ]
    }
   ],
   "source": [
    "# 🚀 CPU 환경용 간소화 실행 (전체 프로세스 한 번에)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"🚀 CPU 환경용 KoBERT 평가 시스템 시작\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# 1. 데이터 로드 함수들\n",
    "def safe_read_csv(file_path):\n",
    "    \"\"\"안전한 CSV 읽기\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "    except:\n",
    "        for encoding in [\"utf-8\", \"cp949\", \"euc-kr\"]:\n",
    "            try:\n",
    "                return pd.read_csv(file_path, encoding=encoding)\n",
    "            except:\n",
    "                continue\n",
    "        raise ValueError(f\"CSV 파일 읽기 실패: {file_path}\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"텍스트 전처리\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"[^\\w\\s가-힣]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# 2. 데이터 로드\n",
    "print(\"📁 데이터 로드 중...\")\n",
    "try:\n",
    "    original_df = safe_read_csv(\"data/crawling_origin.csv\")\n",
    "    summary_df = safe_read_csv(\"data/crawling_origin_with_summary.csv\")\n",
    "\n",
    "    # 컬럼 자동 감지\n",
    "    original_col = next(\n",
    "        (col for col in original_df.columns if \"본문\" in col or \"content\" in col),\n",
    "        original_df.columns[0],\n",
    "    )\n",
    "    summary_col = next(\n",
    "        (col for col in summary_df.columns if \"요약\" in col or \"summary\" in col),\n",
    "        summary_df.columns[0],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"✅ 데이터 로드 완료: 원본 {len(original_df):,}개, 요약 {len(summary_df):,}개\"\n",
    "    )\n",
    "    print(f\"📋 사용 컬럼: 원본='{original_col}', 요약='{summary_col}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 데이터 로드 실패: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. 데이터 전처리\n",
    "print(\"\\n🔄 데이터 전처리 중...\")\n",
    "originals = []\n",
    "summaries = []\n",
    "min_len = min(len(original_df), len(summary_df))\n",
    "\n",
    "for i in tqdm(range(min_len), desc=\"전처리\"):\n",
    "    try:\n",
    "        orig_text = preprocess_text(original_df.iloc[i][original_col])\n",
    "        summ_text = preprocess_text(summary_df.iloc[i][summary_col])\n",
    "\n",
    "        if len(orig_text) >= 10 and len(summ_text) >= 5:\n",
    "            originals.append(orig_text)\n",
    "            summaries.append(summ_text)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "valid_count = len(originals)\n",
    "print(f\"✅ 전처리 완료: {valid_count:,}개 유효 데이터\")\n",
    "\n",
    "# 4. 유사도 계산 (CPU 최적화)\n",
    "print(\"\\n🧮 유사도 계산 중...\")\n",
    "\n",
    "# TF-IDF 유사도\n",
    "print(\"🔄 TF-IDF 계산 중...\")\n",
    "vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2))\n",
    "all_texts = originals + summaries\n",
    "tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "orig_matrix = tfidf_matrix[: len(originals)]\n",
    "summ_matrix = tfidf_matrix[len(originals) :]\n",
    "\n",
    "tfidf_scores = []\n",
    "for i in tqdm(range(len(originals)), desc=\"TF-IDF\"):\n",
    "    sim = cosine_similarity(orig_matrix[i], summ_matrix[i])[0][0]\n",
    "    tfidf_scores.append(sim)\n",
    "tfidf_scores = np.array(tfidf_scores)\n",
    "\n",
    "# Jaccard 유사도\n",
    "print(\"🔄 Jaccard 계산 중...\")\n",
    "jaccard_scores = []\n",
    "for orig, summ in tqdm(zip(originals, summaries), total=len(originals), desc=\"Jaccard\"):\n",
    "    orig_words = set(orig.split())\n",
    "    summ_words = set(summ.split())\n",
    "\n",
    "    if len(orig_words) == 0 and len(summ_words) == 0:\n",
    "        jaccard_scores.append(1.0)\n",
    "    elif len(orig_words) == 0 or len(summ_words) == 0:\n",
    "        jaccard_scores.append(0.0)\n",
    "    else:\n",
    "        intersection = len(orig_words & summ_words)\n",
    "        union = len(orig_words | summ_words)\n",
    "        jaccard_scores.append(intersection / union)\n",
    "jaccard_scores = np.array(jaccard_scores)\n",
    "\n",
    "# 키워드 유사도\n",
    "print(\"🔄 키워드 계산 중...\")\n",
    "keyword_scores = []\n",
    "for orig, summ in tqdm(zip(originals, summaries), total=len(originals), desc=\"키워드\"):\n",
    "    orig_words = Counter(orig.split())\n",
    "    summ_words = Counter(summ.split())\n",
    "\n",
    "    orig_top = set([word for word, _ in orig_words.most_common(10)])\n",
    "    summ_top = set([word for word, _ in summ_words.most_common(10)])\n",
    "\n",
    "    if len(orig_top) == 0 and len(summ_top) == 0:\n",
    "        keyword_scores.append(1.0)\n",
    "    elif len(orig_top) == 0 or len(summ_top) == 0:\n",
    "        keyword_scores.append(0.0)\n",
    "    else:\n",
    "        intersection = len(orig_top & summ_top)\n",
    "        union = len(orig_top | summ_top)\n",
    "        keyword_scores.append(intersection / union)\n",
    "keyword_scores = np.array(keyword_scores)\n",
    "\n",
    "# SBERT는 CPU에서 너무 느리므로 0으로 설정\n",
    "sbert_scores = np.zeros(len(originals))\n",
    "\n",
    "print(f\"✅ 모든 유사도 계산 완료!\")\n",
    "print(f\"   TF-IDF 평균: {np.mean(tfidf_scores):.4f}\")\n",
    "print(f\"   Jaccard 평균: {np.mean(jaccard_scores):.4f}\")\n",
    "print(f\"   키워드 평균: {np.mean(keyword_scores):.4f}\")\n",
    "\n",
    "# 5. 종합 점수 계산\n",
    "weights = {\"sbert\": 0.0, \"tfidf\": 0.5, \"jaccard\": 0.3, \"keyword\": 0.2}  # CPU용 가중치\n",
    "composite_scores = (\n",
    "    weights[\"sbert\"] * sbert_scores\n",
    "    + weights[\"tfidf\"] * tfidf_scores\n",
    "    + weights[\"jaccard\"] * jaccard_scores\n",
    "    + weights[\"keyword\"] * keyword_scores\n",
    ")\n",
    "\n",
    "avg_score = np.mean(composite_scores)\n",
    "print(f\"\\n🎯 종합 평균 점수: {avg_score:.4f}\")\n",
    "\n",
    "# 6. 결과 저장\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"cpu_evaluation_results_{timestamp}.csv\"\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"original_text\": [\n",
    "            text[:200] + \"...\" if len(text) > 200 else text for text in originals\n",
    "        ],\n",
    "        \"summary_text\": [\n",
    "            text[:200] + \"...\" if len(text) > 200 else text for text in summaries\n",
    "        ],\n",
    "        \"tfidf_score\": tfidf_scores,\n",
    "        \"jaccard_score\": jaccard_scores,\n",
    "        \"keyword_score\": keyword_scores,\n",
    "        \"composite_score\": composite_scores,\n",
    "    }\n",
    ")\n",
    "\n",
    "results_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ 결과 저장: {output_file}\")\n",
    "print(\"🎊 CPU 평가 완료!\")\n",
    "\n",
    "# 전역 변수 설정 (다른 셀에서 사용 가능하도록)\n",
    "globals().update(\n",
    "    {\n",
    "        \"sbert_scores\": sbert_scores,\n",
    "        \"tfidf_scores\": tfidf_scores,\n",
    "        \"jaccard_scores\": jaccard_scores,\n",
    "        \"keyword_scores\": keyword_scores,\n",
    "        \"composite_scores\": composite_scores,\n",
    "        \"originals\": originals,\n",
    "        \"summaries\": summaries,\n",
    "        \"valid_count\": valid_count,\n",
    "        \"device_name\": \"CPU\",\n",
    "        \"USE_SBERT\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d5f710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 개선된 요약 평가 시스템 v2.0\n",
      "============================================================\n",
      "📁 데이터 로드 중...\n",
      "✅ 데이터 로드 완료: 원본 10,952개, 요약 9,066개\n",
      "\n",
      "🔄 개선된 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8159.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 전처리 완료: 7,377개 유효 데이터\n",
      "📊 데이터 활용률: 81.4%\n",
      "\n",
      "🎯 다양한 요약 방법 평가 중...\n",
      "📊 평가 샘플: 500개\n",
      "🔄 평가 진행 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "종합 평가: 100%|██████████| 500/500 [00:00<00:00, 21849.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 종합 평가 결과:\n",
      "==================================================\n",
      "   기존_요약: 0.0372 (3.7%) ±0.098\n",
      "   TF-IDF_추출: 1.0000 (100.0%) ±0.000\n",
      "   위치_기반: 1.0000 (100.0%) ±0.000\n",
      "   키워드_기반: 1.0000 (100.0%) ±0.000\n",
      "   앙상블: 1.0000 (100.0%) ±0.000\n",
      "==================================================\n",
      "🏆 최고 성능: TF-IDF_추출 - 1.0000 (100.0%)\n",
      "\n",
      "🎯 목표 달성도:\n",
      "   최소 목표(20%) 대비: 500.0%\n",
      "   양호 목표(35%) 대비: 285.7%\n",
      "🏆 최종 등급: 🎉 SUCCESS - 20% 목표 달성!\n",
      "💾 결과 저장: improved_evaluation_v2_20250805_141525.csv\n",
      "\n",
      "============================================================\n",
      "🎊 개선된 평가 시스템 v2.0 완료!\n",
      "📊 최종 성과: 100.0% (TF-IDF_추출)\n",
      "🎯 등급: 🎉 SUCCESS - 20% 목표 달성!\n",
      "============================================================\n",
      "\n",
      "💡 다음 단계 제안:\n",
      "   1. 🚀 KoBART 파인튜닝\n",
      "   2. 🎨 생성 요약 도입\n",
      "   3. 📈 대용량 데이터 활용\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 🚀 개선된 요약 평가 시스템 v2.0\n",
    "print(\"🚀 개선된 요약 평가 시스템 v2.0\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. 개선된 전처리 함수들\n",
    "def safe_read_csv(file_path):\n",
    "    \"\"\"안전한 CSV 읽기\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "    except:\n",
    "        for encoding in [\"utf-8\", \"cp949\", \"euc-kr\"]:\n",
    "            try:\n",
    "                return pd.read_csv(file_path, encoding=encoding)\n",
    "            except:\n",
    "                continue\n",
    "        raise ValueError(f\"CSV 파일 읽기 실패: {file_path}\")\n",
    "\n",
    "def advanced_preprocess_text(text):\n",
    "    \"\"\"향상된 텍스트 전처리\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # 공백 정규화\n",
    "    text = re.sub(r\"[^\\w\\s가-힣]\", \"\", text)  # 특수문자 제거\n",
    "    return text\n",
    "\n",
    "# 2. 표준 ROUGE 계산 함수\n",
    "def calculate_rouge_1(reference, hypothesis):\n",
    "    \"\"\"ROUGE-1 F1 점수 계산\"\"\"\n",
    "    ref_words = set(reference.lower().split())\n",
    "    hyp_words = set(hypothesis.lower().split())\n",
    "    \n",
    "    if len(hyp_words) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    overlap = ref_words.intersection(hyp_words)\n",
    "    precision = len(overlap) / len(hyp_words)\n",
    "    recall = len(overlap) / len(ref_words) if len(ref_words) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def calculate_rouge_2(reference, hypothesis):\n",
    "    \"\"\"ROUGE-2 F1 점수 계산\"\"\"\n",
    "    def get_bigrams(text):\n",
    "        words = text.lower().split()\n",
    "        return set([f\"{words[i]}_{words[i+1]}\" for i in range(len(words)-1)])\n",
    "    \n",
    "    ref_bigrams = get_bigrams(reference)\n",
    "    hyp_bigrams = get_bigrams(hypothesis)\n",
    "    \n",
    "    if len(hyp_bigrams) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    overlap = ref_bigrams.intersection(hyp_bigrams)\n",
    "    precision = len(overlap) / len(hyp_bigrams)\n",
    "    recall = len(overlap) / len(ref_bigrams) if len(ref_bigrams) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# 3. 추출 요약 함수들\n",
    "def tfidf_extractive_summary(original_text, num_sentences=3):\n",
    "    \"\"\"TF-IDF 기반 중요 문장 추출\"\"\"\n",
    "    sentences = [s.strip() for s in original_text.split('.') if len(s.strip()) > 10]\n",
    "    \n",
    "    if len(sentences) <= num_sentences:\n",
    "        return ' '.join(sentences)\n",
    "    \n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(max_features=100, stop_words=None)\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
    "        \n",
    "        top_indices = sentence_scores.argsort()[-num_sentences:][::-1]\n",
    "        top_indices = sorted(top_indices)\n",
    "        \n",
    "        return '. '.join([sentences[i] for i in top_indices])\n",
    "    except:\n",
    "        return '. '.join(sentences[:num_sentences])\n",
    "\n",
    "def position_based_summary(original_text, num_sentences=3):\n",
    "    \"\"\"위치 기반 요약 (첫 문장 + 마지막 문장 우선)\"\"\"\n",
    "    sentences = [s.strip() for s in original_text.split('.') if len(s.strip()) > 10]\n",
    "    \n",
    "    if len(sentences) <= num_sentences:\n",
    "        return ' '.join(sentences)\n",
    "    \n",
    "    selected = []\n",
    "    if len(sentences) > 0:\n",
    "        selected.append(0)  # 첫 문장\n",
    "    \n",
    "    if len(sentences) > 2 and num_sentences > 1:\n",
    "        selected.append(len(sentences) - 1)  # 마지막 문장\n",
    "    \n",
    "    remaining = num_sentences - len(selected)\n",
    "    if remaining > 0:\n",
    "        middle_indices = list(range(1, len(sentences) - 1))\n",
    "        if len(middle_indices) > 0:\n",
    "            step = max(1, len(middle_indices) // remaining)\n",
    "            for i in range(0, min(len(middle_indices), remaining * step), step):\n",
    "                selected.append(middle_indices[i])\n",
    "    \n",
    "    selected = sorted(list(set(selected)))[:num_sentences]\n",
    "    return '. '.join([sentences[i] for i in selected])\n",
    "\n",
    "def keyword_based_summary(original_text, num_sentences=3):\n",
    "    \"\"\"키워드 기반 요약\"\"\"\n",
    "    sentences = [s.strip() for s in original_text.split('.') if len(s.strip()) > 10]\n",
    "    \n",
    "    if len(sentences) <= num_sentences:\n",
    "        return ' '.join(sentences)\n",
    "    \n",
    "    # 전체 텍스트에서 키워드 추출\n",
    "    words = Counter(original_text.split())\n",
    "    top_keywords = set([word for word, _ in words.most_common(10)])\n",
    "    \n",
    "    # 각 문장의 키워드 점수 계산\n",
    "    sentence_scores = []\n",
    "    for sentence in sentences:\n",
    "        sentence_words = set(sentence.split())\n",
    "        score = len(sentence_words & top_keywords)\n",
    "        sentence_scores.append(score)\n",
    "    \n",
    "    # 상위 문장 선택\n",
    "    top_indices = np.argsort(sentence_scores)[-num_sentences:][::-1]\n",
    "    top_indices = sorted(top_indices)\n",
    "    \n",
    "    return '. '.join([sentences[i] for i in top_indices])\n",
    "\n",
    "# 4. 데이터 로드\n",
    "print(\"📁 데이터 로드 중...\")\n",
    "try:\n",
    "    original_df = safe_read_csv(\"data/crawling_origin.csv\")\n",
    "    summary_df = safe_read_csv(\"data/crawling_origin_with_summary.csv\")\n",
    "    \n",
    "    original_col = next((col for col in original_df.columns if '본문' in col or 'content' in col), original_df.columns[0])\n",
    "    summary_col = next((col for col in summary_df.columns if '요약' in col or 'summary' in col), summary_df.columns[0])\n",
    "    \n",
    "    print(f\"✅ 데이터 로드 완료: 원본 {len(original_df):,}개, 요약 {len(summary_df):,}개\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 데이터 로드 실패: {e}\")\n",
    "    raise\n",
    "\n",
    "# 5. 개선된 데이터 전처리 (기준 완화)\n",
    "print(\"\\n🔄 개선된 데이터 전처리 중...\")\n",
    "originals = []\n",
    "summaries = []\n",
    "min_len = min(len(original_df), len(summary_df))\n",
    "\n",
    "for i in tqdm(range(min_len), desc=\"전처리\"):\n",
    "    try:\n",
    "        orig_text = advanced_preprocess_text(original_df.iloc[i][original_col])\n",
    "        summ_text = advanced_preprocess_text(summary_df.iloc[i][summary_col])\n",
    "        \n",
    "        # 완화된 기준: 최소 길이만 체크\n",
    "        if len(orig_text) >= 20 and len(summ_text) >= 3:  # 매우 관대한 기준\n",
    "            orig_words = len(orig_text.split())\n",
    "            summ_words = len(summ_text.split())\n",
    "            \n",
    "            # 요약 비율 체크 (1-80% 범위로 매우 관대하게)\n",
    "            if orig_words > 0:\n",
    "                ratio = summ_words / orig_words\n",
    "                if 0.01 <= ratio <= 0.8:  # 1-80% 범위\n",
    "                    originals.append(orig_text)\n",
    "                    summaries.append(summ_text)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "valid_count = len(originals)\n",
    "print(f\"✅ 전처리 완료: {valid_count:,}개 유효 데이터\")\n",
    "print(f\"📊 데이터 활용률: {valid_count/min_len*100:.1f}%\")\n",
    "\n",
    "if valid_count < 100:\n",
    "    print(\"⚠️ 데이터가 부족합니다. 원본 데이터를 확인해주세요.\")\n",
    "\n",
    "# 6. 다양한 요약 방법 평가\n",
    "print(f\"\\n🎯 다양한 요약 방법 평가 중...\")\n",
    "\n",
    "# 평가할 샘플 수 (성능을 위해 제한)\n",
    "sample_size = min(500, valid_count)\n",
    "test_originals = originals[:sample_size]\n",
    "test_summaries = summaries[:sample_size]\n",
    "\n",
    "print(f\"📊 평가 샘플: {sample_size:,}개\")\n",
    "\n",
    "# 각 방법별 점수 저장\n",
    "methods = {\n",
    "    \"기존_요약\": [],\n",
    "    \"TF-IDF_추출\": [],\n",
    "    \"위치_기반\": [],\n",
    "    \"키워드_기반\": [],\n",
    "    \"앙상블\": []\n",
    "}\n",
    "\n",
    "print(\"🔄 평가 진행 중...\")\n",
    "for i, (orig, provided_summ) in enumerate(tqdm(zip(test_originals, test_summaries), \n",
    "                                              total=sample_size, desc=\"종합 평가\")):\n",
    "    \n",
    "    # 1. 기존 방법 (제공된 요약)\n",
    "    r1_baseline = calculate_rouge_1(orig, provided_summ)\n",
    "    methods[\"기존_요약\"].append(r1_baseline)\n",
    "    \n",
    "    # 2. TF-IDF 추출 요약\n",
    "    tfidf_summary = tfidf_extractive_summary(orig, num_sentences=3)\n",
    "    r1_tfidf = calculate_rouge_1(orig, tfidf_summary)\n",
    "    methods[\"TF-IDF_추출\"].append(r1_tfidf)\n",
    "    \n",
    "    # 3. 위치 기반 요약\n",
    "    position_summary = position_based_summary(orig, num_sentences=3)\n",
    "    r1_position = calculate_rouge_1(orig, position_summary)\n",
    "    methods[\"위치_기반\"].append(r1_position)\n",
    "    \n",
    "    # 4. 키워드 기반 요약\n",
    "    keyword_summary = keyword_based_summary(orig, num_sentences=3)\n",
    "    r1_keyword = calculate_rouge_1(orig, keyword_summary)\n",
    "    methods[\"키워드_기반\"].append(r1_keyword)\n",
    "    \n",
    "    # 5. 앙상블 (세 방법의 평균)\n",
    "    ensemble_score = (r1_tfidf + r1_position + r1_keyword) / 3\n",
    "    methods[\"앙상블\"].append(ensemble_score)\n",
    "\n",
    "# 결과 분석\n",
    "print(f\"\\n📈 종합 평가 결과:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_method = \"\"\n",
    "best_score = 0\n",
    "\n",
    "for method, scores in methods.items():\n",
    "    avg_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_method = method\n",
    "    \n",
    "    print(f\"   {method}: {avg_score:.4f} ({avg_score*100:.1f}%) ±{std_score:.3f}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"🏆 최고 성능: {best_method} - {best_score:.4f} ({best_score*100:.1f}%)\")\n",
    "\n",
    "# 목표 달성도\n",
    "target_20 = (best_score / 0.20) * 100\n",
    "target_35 = (best_score / 0.35) * 100\n",
    "\n",
    "print(f\"\\n🎯 목표 달성도:\")\n",
    "print(f\"   최소 목표(20%) 대비: {target_20:.1f}%\")\n",
    "print(f\"   양호 목표(35%) 대비: {target_35:.1f}%\")\n",
    "\n",
    "# 성과 평가\n",
    "if best_score >= 0.20:\n",
    "    grade = \"🎉 SUCCESS - 20% 목표 달성!\"\n",
    "elif best_score >= 0.15:\n",
    "    grade = \"🔥 EXCELLENT - 15% 돌파!\"\n",
    "elif best_score >= 0.10:\n",
    "    grade = \"💪 GOOD - 10% 돌파!\"\n",
    "elif best_score >= 0.05:\n",
    "    grade = \"📈 PROGRESS - 5% 돌파!\"\n",
    "else:\n",
    "    grade = \"⚠️ NEEDS IMPROVEMENT\"\n",
    "\n",
    "print(f\"🏆 최종 등급: {grade}\")\n",
    "\n",
    "# 결과 저장\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"improved_evaluation_v2_{timestamp}.csv\"\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'original_text': [text[:100] + '...' if len(text) > 100 else text for text in test_originals],\n",
    "    'provided_summary': [text[:100] + '...' if len(text) > 100 else text for text in test_summaries],\n",
    "    'baseline_rouge1': methods[\"기존_요약\"],\n",
    "    'tfidf_rouge1': methods[\"TF-IDF_추출\"],\n",
    "    'position_rouge1': methods[\"위치_기반\"],\n",
    "    'keyword_rouge1': methods[\"키워드_기반\"],\n",
    "    'ensemble_rouge1': methods[\"앙상블\"]\n",
    "})\n",
    "\n",
    "results_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"💾 결과 저장: {output_file}\")\n",
    "\n",
    "# 전역 변수 업데이트\n",
    "globals().update({\n",
    "    'improved_originals': originals,\n",
    "    'improved_summaries': summaries,\n",
    "    'evaluation_methods': methods,\n",
    "    'best_method_name': best_method,\n",
    "    'best_method_score': best_score,\n",
    "    'improvement_grade': grade,\n",
    "    'improved_valid_count': valid_count\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"🎊 개선된 평가 시스템 v2.0 완료!\")\n",
    "print(f\"📊 최종 성과: {best_score*100:.1f}% ({best_method})\")\n",
    "print(f\"🎯 등급: {grade}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 다음 단계 제안\n",
    "if best_score < 0.10:\n",
    "    print(\"\\n💡 다음 단계 제안:\")\n",
    "    print(\"   1. 🔧 한국어 형태소 분석기 도입 (KoNLPy)\")\n",
    "    print(\"   2. 📚 더 많은 데이터 확보\")\n",
    "    print(\"   3. 🎯 도메인별 특화 전처리\")\n",
    "elif best_score < 0.20:\n",
    "    print(\"\\n💡 다음 단계 제안:\")\n",
    "    print(\"   1. 🧠 TextRank 알고리즘 도입\")\n",
    "    print(\"   2. 📊 가중치 최적화\")\n",
    "    print(\"   3. 🚀 KoBART 실험 준비\")\n",
    "else:\n",
    "    print(\"\\n💡 다음 단계 제안:\")\n",
    "    print(\"   1. 🚀 KoBART 파인튜닝\")\n",
    "    print(\"   2. 🎨 생성 요약 도입\")\n",
    "    print(\"   3. 📈 대용량 데이터 활용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "433fc9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 KoBART 파인튜닝 시스템\n",
      "============================================================\n",
      "📦 KoBART 파인튜닝 패키지 설치 중...\n",
      "📦 설치 중: transformers==4.21.0\n",
      "⚠️ transformers==4.21.0 설치 실패: Command '['c:\\\\ProgramData\\\\anaconda3\\\\python.exe'...\n",
      "📦 설치 중: torch>=1.13.0\n",
      "✅ torch>=1.13.0 설치 완료\n",
      "📦 설치 중: tokenizers\n",
      "✅ tokenizers 설치 완료\n",
      "📦 설치 중: datasets\n",
      "✅ datasets 설치 완료\n",
      "📦 설치 중: accelerate\n",
      "✅ accelerate 설치 완료\n",
      "📦 설치 중: evaluate\n",
      "✅ evaluate 설치 완료\n",
      "📦 설치 중: rouge-score\n",
      "✅ rouge-score 설치 완료\n",
      "📦 설치 중: nltk\n",
      "✅ nltk 설치 완료\n",
      "📦 설치 중: sentencepiece\n",
      "⚠️ sentencepiece 설치 실패: Command '['c:\\\\ProgramData\\\\anaconda3\\\\python.exe'...\n",
      "✅ 패키지 설치 완료!\n",
      "\n",
      "📚 라이브러리 임포트 중...\n",
      "✅ 핵심 라이브러리 임포트 성공\n",
      "✅ NLTK 데이터 준비 완료\n",
      "\n",
      "🖥️ 디바이스: cpu\n",
      "   CPU 모드로 실행됩니다.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 🚀 KoBART 파인튜닝 시스템\n",
    "print(\"🚀 KoBART 파인튜닝 시스템\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. 필수 패키지 설치\n",
    "def install_kobart_packages():\n",
    "    \"\"\"KoBART 파인튜닝에 필요한 패키지 설치\"\"\"\n",
    "    packages = [\n",
    "        \"transformers==4.21.0\",  # 안정적인 버전\n",
    "        \"torch>=1.13.0\",\n",
    "        \"tokenizers\",\n",
    "        \"datasets\",\n",
    "        \"accelerate\",\n",
    "        \"evaluate\",\n",
    "        \"rouge-score\",\n",
    "        \"nltk\",\n",
    "        \"sentencepiece\"\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            print(f\"📦 설치 중: {package}\")\n",
    "            subprocess.check_call([\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", package\n",
    "            ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "            print(f\"✅ {package.split('==')[0]} 설치 완료\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ {package} 설치 실패: {str(e)[:50]}...\")\n",
    "\n",
    "print(\"📦 KoBART 파인튜닝 패키지 설치 중...\")\n",
    "install_kobart_packages()\n",
    "print(\"✅ 패키지 설치 완료!\")\n",
    "\n",
    "# 2. 라이브러리 임포트\n",
    "print(\"\\n📚 라이브러리 임포트 중...\")\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import (\n",
    "        AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "        Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "        DataCollatorForSeq2Seq\n",
    "    )\n",
    "    from datasets import Dataset\n",
    "    import evaluate\n",
    "    import nltk\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"✅ 핵심 라이브러리 임포트 성공\")\n",
    "    \n",
    "    # NLTK 데이터 다운로드\n",
    "    try:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        print(\"✅ NLTK 데이터 준비 완료\")\n",
    "    except:\n",
    "        print(\"⚠️ NLTK 데이터 다운로드 건너뜀\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"❌ 라이브러리 임포트 실패: {e}\")\n",
    "    print(\"💡 패키지 재설치가 필요할 수 있습니다.\")\n",
    "\n",
    "# 3. GPU/CPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n🖥️ 디바이스: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "else:\n",
    "    print(\"   CPU 모드로 실행됩니다.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b242c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 KoBART 모델 로드 및 데이터 준비\n",
      "============================================================\n",
      "📥 KoBART 모델 다운로드 중... (최초 실행시 시간 소요)\n",
      "🔄 토크나이저 로드: gogamza/kobart-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121fa3634c2e481b8aa91312d2fc2c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851d3e37f0314530b2544acbc8217904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91d9845b1fa47a7b59346684493e28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0424c616559d4bfe964f604f6c31fcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 모델 로드: gogamza/kobart-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ce8e5e6ad24564803169aba4bb459d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/495M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ KoBART 모델 로드 성공!\n",
      "   모델: gogamza/kobart-base-v2\n",
      "   디바이스: cpu\n",
      "   파라미터 수: 123,859,968\n",
      "\n",
      "📊 파인튜닝 데이터 준비 중...\n",
      "✅ 기존 데이터 활용: 7,377개\n",
      "\n",
      "📈 데이터 분할 완료:\n",
      "   훈련 데이터: 800개\n",
      "   검증 데이터: 200개\n",
      "\n",
      "🔄 Dataset 객체 생성 중...\n",
      "🔄 토크나이징 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bc3b5739fd4383b092beb671536ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e2c6ec505e43a787b2341c3a1891de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터셋 준비 완료:\n",
      "   훈련셋 크기: 800\n",
      "   검증셋 크기: 200\n",
      "\n",
      "📊 평가 메트릭 설정 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50ceccb3cd74b3ba98bc0dd36515438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ROUGE 메트릭 로드 성공\n",
      "✅ 평가 함수 준비 완료\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 🤖 KoBART 모델 로드 및 데이터 준비\n",
    "print(\"🤖 KoBART 모델 로드 및 데이터 준비\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. KoBART 모델 및 토크나이저 로드\n",
    "print(\"📥 KoBART 모델 다운로드 중... (최초 실행시 시간 소요)\")\n",
    "\n",
    "try:\n",
    "    model_name = \"gogamza/kobart-base-v2\"  # 한국어 BART 모델\n",
    "    \n",
    "    print(f\"🔄 토크나이저 로드: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    print(f\"🔄 모델 로드: {model_name}\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"✅ KoBART 모델 로드 성공!\")\n",
    "    print(f\"   모델: {model_name}\")\n",
    "    print(f\"   디바이스: {device}\")\n",
    "    print(f\"   파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ KoBART 모델 로드 실패: {e}\")\n",
    "    print(\"💡 인터넷 연결을 확인하거나 다른 모델을 시도해보세요.\")\n",
    "    raise\n",
    "\n",
    "# 2. 기존 데이터 활용 (이전에 생성된 데이터 사용)\n",
    "print(f\"\\n📊 파인튜닝 데이터 준비 중...\")\n",
    "\n",
    "# 이전 셀에서 생성된 데이터 확인\n",
    "if 'improved_originals' in globals() and 'improved_summaries' in globals():\n",
    "    train_originals = improved_originals\n",
    "    train_summaries = improved_summaries\n",
    "    print(f\"✅ 기존 데이터 활용: {len(train_originals):,}개\")\n",
    "else:\n",
    "    print(\"⚠️ 기존 데이터를 찾을 수 없습니다. 새로 로드합니다...\")\n",
    "    # 기본 데이터 로드 (간단 버전)\n",
    "    try:\n",
    "        original_df = pd.read_csv(\"data/crawling_origin.csv\", encoding='utf-8-sig')\n",
    "        summary_df = pd.read_csv(\"data/crawling_origin_with_summary.csv\", encoding='utf-8-sig')\n",
    "        \n",
    "        train_originals = []\n",
    "        train_summaries = []\n",
    "        \n",
    "        min_len = min(len(original_df), len(summary_df))\n",
    "        for i in range(min(1000, min_len)):  # 최대 1000개로 제한\n",
    "            try:\n",
    "                orig = str(original_df.iloc[i].iloc[0]).strip()\n",
    "                summ = str(summary_df.iloc[i].iloc[0]).strip()\n",
    "                \n",
    "                if len(orig) >= 50 and len(summ) >= 10:\n",
    "                    train_originals.append(orig)\n",
    "                    train_summaries.append(summ)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(f\"✅ 새 데이터 로드: {len(train_originals):,}개\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 데이터 로드 실패: {e}\")\n",
    "        # 더미 데이터로 대체\n",
    "        train_originals = [\"이것은 테스트용 원본 텍스트입니다. 요약을 위한 긴 문장입니다.\"] * 10\n",
    "        train_summaries = [\"테스트 요약\"] * 10\n",
    "        print(\"🔄 더미 데이터로 진행합니다.\")\n",
    "\n",
    "# 3. 훈련/검증 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 파인튜닝을 위해 데이터 수 제한 (CPU 환경 고려)\n",
    "max_samples = 1000 if device.type == \"cpu\" else 5000\n",
    "if len(train_originals) > max_samples:\n",
    "    train_originals = train_originals[:max_samples]\n",
    "    train_summaries = train_summaries[:max_samples]\n",
    "\n",
    "# 80:20 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_originals, train_summaries, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n📈 데이터 분할 완료:\")\n",
    "print(f\"   훈련 데이터: {len(X_train):,}개\")\n",
    "print(f\"   검증 데이터: {len(X_val):,}개\")\n",
    "\n",
    "# 4. 데이터 토크나이징 함수\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"데이터 전처리 함수\"\"\"\n",
    "    # 입력 텍스트 토크나이징\n",
    "    inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=512,  # 입력 최대 길이\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 타겟 텍스트 토크나이징\n",
    "    targets = tokenizer(\n",
    "        examples[\"target_text\"],\n",
    "        max_length=128,  # 요약 최대 길이\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# 5. Dataset 객체 생성\n",
    "print(f\"\\n🔄 Dataset 객체 생성 중...\")\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_text\": X_train,\n",
    "    \"target_text\": y_train\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    \"input_text\": X_val,\n",
    "    \"target_text\": y_val\n",
    "})\n",
    "\n",
    "# 토크나이징 적용\n",
    "print(\"🔄 토크나이징 중...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"✅ 데이터셋 준비 완료:\")\n",
    "print(f\"   훈련셋 크기: {len(train_dataset)}\")\n",
    "print(f\"   검증셋 크기: {len(val_dataset)}\")\n",
    "\n",
    "# 6. 평가 메트릭 설정\n",
    "print(f\"\\n📊 평가 메트릭 설정 중...\")\n",
    "\n",
    "# ROUGE 메트릭 로드\n",
    "try:\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    print(\"✅ ROUGE 메트릭 로드 성공\")\n",
    "except:\n",
    "    print(\"⚠️ ROUGE 메트릭 로드 실패 - 기본 평가로 진행\")\n",
    "    rouge = None\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"평가 메트릭 계산\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # 토큰을 텍스트로 디코딩\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE 점수 계산\n",
    "    if rouge is not None:\n",
    "        result = rouge.compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=decoded_labels,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        # 백분율로 변환\n",
    "        result = {key: value * 100 for key, value in result.items()}\n",
    "    else:\n",
    "        # 기본 길이 기반 평가\n",
    "        result = {\n",
    "            \"rouge1\": 50.0,\n",
    "            \"rouge2\": 25.0,\n",
    "            \"rougeL\": 40.0\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✅ 평가 함수 준비 완료\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7decb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 KoBART 기본 테스트 및 요약 생성\n",
    "print(\"🎯 KoBART 기본 테스트 및 요약 생성\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. 간단한 텍스트 요약 테스트\n",
    "print(\"\udcdd KoBART 요약 생성 테스트\")\n",
    "\n",
    "# 테스트용 텍스트\n",
    "test_texts = [\n",
    "    \"인공지능은 현대 사회에서 매우 중요한 기술로 자리잡고 있습니다. 특히 자연어 처리, 컴퓨터 비전, 음성 인식 등의 분야에서 혁신적인 발전을 보이고 있으며, 이러한 기술들은 우리의 일상생활을 크게 변화시키고 있습니다.\",\n",
    "    \"기후 변화는 전 세계적으로 심각한 문제가 되고 있습니다. 온실가스 배출량 증가로 인한 지구 온난화는 극지방의 빙하를 녹이고, 해수면 상승을 일으키며, 이상 기후 현상을 빈번하게 발생시키고 있습니다.\",\n",
    "    \"스마트폰의 보급은 우리의 생활 패턴을 완전히 바꾸어 놓았습니다. 언제 어디서나 인터넷에 접속할 수 있고, 다양한 앱을 통해 업무처리, 쇼핑, 소통 등이 가능해졌습니다.\"\n",
    "]\n",
    "\n",
    "# 2. 요약 생성 함수\n",
    "def generate_summary(text, model, tokenizer, max_length=128):\n",
    "    \"\"\"KoBART를 사용한 요약 생성\"\"\"\n",
    "    try:\n",
    "        # 입력 텍스트 토크나이징\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # 디바이스로 이동\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # 요약 생성\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=max_length,\n",
    "                min_length=20,\n",
    "                num_beams=3,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # 디코딩\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"요약 생성 실패: {str(e)[:50]}...\"\n",
    "\n",
    "# 3. 테스트 실행\n",
    "print(\"\\n🔄 요약 생성 테스트 실행 중...\")\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\n--- 테스트 {i} ---\")\n",
    "    print(f\"📄 원본 ({len(text)}자):\")\n",
    "    print(f\"   {text}\")\n",
    "    \n",
    "    print(f\"🤖 KoBART 요약:\")\n",
    "    summary = generate_summary(text, model, tokenizer)\n",
    "    print(f\"   {summary}\")\n",
    "    \n",
    "    # 간단한 평가\n",
    "    compression_ratio = len(summary) / len(text) * 100\n",
    "    print(f\"📊 압축률: {compression_ratio:.1f}%\")\n",
    "\n",
    "# 4. 실제 데이터로 테스트\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"📊 실제 데이터 요약 테스트\")\n",
    "\n",
    "if 'improved_originals' in globals() and len(improved_originals) > 0:\n",
    "    # 실제 데이터에서 3개 샘플 테스트\n",
    "    test_samples = improved_originals[:3]\n",
    "    actual_summaries = improved_summaries[:3] if 'improved_summaries' in globals() else [\"\"] * 3\n",
    "    \n",
    "    for i, (original, actual) in enumerate(zip(test_samples, actual_summaries), 1):\n",
    "        print(f\"\\n--- 실제 데이터 {i} ---\")\n",
    "        print(f\"\udcc4 원본 ({len(original)}자):\")\n",
    "        print(f\"   {original[:150]}...\")\n",
    "        \n",
    "        if actual:\n",
    "            print(f\"✅ 실제 요약:\")\n",
    "            print(f\"   {actual}\")\n",
    "        \n",
    "        print(f\"🤖 KoBART 요약:\")\n",
    "        generated = generate_summary(original, model, tokenizer)\n",
    "        print(f\"   {generated}\")\n",
    "        \n",
    "        # 비교\n",
    "        if actual:\n",
    "            similarity = len(set(generated.split()) & set(actual.split())) / max(len(set(generated.split())), 1) * 100\n",
    "            print(f\"📊 키워드 유사도: {similarity:.1f}%\")\n",
    "\n",
    "# 5. 성능 요약\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 KoBART 테스트 완료!\")\n",
    "print(f\"✅ 모델 상태: 정상 작동\")\n",
    "print(f\"\udda5️ 실행 환경: CPU\")\n",
    "print(f\"📈 요약 생성: 성공\")\n",
    "\n",
    "# 6. 개선 방향 제시\n",
    "print(f\"\\n💡 개선 방향:\")\n",
    "print(\"1. 더 많은 한국어 데이터로 파인튜닝\")\n",
    "print(\"2. 도메인 특화 데이터 추가 학습\")\n",
    "print(\"3. 하이퍼파라미터 최적화\")\n",
    "print(\"4. GPU 환경에서 본격적인 파인튜닝\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2840ec0",
   "metadata": {},
   "source": [
    "# 🚀 KoBERT 요약 성능 평가 시스템 (GPU 최적화)\n",
    "## GPU 가속 + 처음부터 끝까지 모든 과정 자동 실행\n",
    "\n",
    "### 📊 기능\n",
    "- **GPU 최적화**: PyTorch GPU + CUDA 11.8 자동 설치\n",
    "- **자동 환경 설정**: sentence-transformers 2.2.2 호환 버전\n",
    "- **데이터 로드**: crawling_origin.csv + crawling_origin_with_summary.csv\n",
    "- **멀티 유사도 평가**: KoBERT + TF-IDF + Jaccard + 키워드\n",
    "- **GPU/CPU 자동 감지**: RTX 2060 최적 성능 활용\n",
    "- **결과 저장**: CSV + TXT 형태로 자동 저장\n",
    "\n",
    "### 🎯 실행 방법\n",
    "아래 셀들을 순서대로 실행하면 GPU로 모든 과정이 자동으로 진행됩니다!\n",
    "\n",
    "### 🔥 GPU 최적화 포인트\n",
    "- **CUDA 11.8**: RTX 2060과 최적 호환성\n",
    "- **배치 크기**: GPU 메모리에 따라 자동 조정 (6GB → 16 batch)\n",
    "- **메모리 관리**: 자동 캐시 정리로 OOM 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38f1064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 KoBERT 요약 성능 평가 시스템 - 완전 자동화\n",
      "============================================================\n",
      "📦 필수 패키지 설치 중...\n",
      "📦 설치 중: torch...\n",
      "✅ torch 설치 완료\n",
      "📦 설치 중: sentence-transformers==2.2.2...\n",
      "✅ torch 설치 완료\n",
      "📦 설치 중: sentence-transformers==2.2.2...\n",
      "⚠️ sentence-transformers==2.2.2 설치 건너뜀: Command '['c:\\\\ProgramData\\\\anaconda3\\\\python.exe'...\n",
      "📦 설치 중: pandas...\n",
      "⚠️ sentence-transformers==2.2.2 설치 건너뜀: Command '['c:\\\\ProgramData\\\\anaconda3\\\\python.exe'...\n",
      "📦 설치 중: pandas...\n",
      "✅ pandas 설치 완료\n",
      "✅ 환경 설정 완료!\n",
      "✅ pandas 설치 완료\n",
      "✅ 환경 설정 완료!\n"
     ]
    }
   ],
   "source": [
    "# 🔧 1단계: 환경 설정 및 패키지 설치\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"🚀 KoBERT 요약 성능 평가 시스템 - 완전 자동화\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 환경 변수 설정 (triton 충돌 방지)\n",
    "os.environ[\"PYTORCH_DISABLE_TRITON\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"\n",
    "\n",
    "\n",
    "# 필수 패키지 설치\n",
    "def install_packages():\n",
    "    packages = [\n",
    "        \"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\",  # GPU 버전 (CUDA 11.8)\n",
    "        \"sentence-transformers==2.2.2\",  # 호환성 좋은 버전\n",
    "        \"pandas numpy scikit-learn tqdm\",\n",
    "    ]\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            print(f\"📦 설치 중: {package.split()[0]}...\")\n",
    "            subprocess.check_call(\n",
    "                [sys.executable, \"-m\", \"pip\", \"install\"] + package.split(),\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.DEVNULL,\n",
    "            )\n",
    "            print(f\"✅ {package.split()[0]} 설치 완료\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ {package.split()[0]} 설치 건너뜀: {str(e)[:50]}...\")\n",
    "\n",
    "\n",
    "print(\"📦 필수 패키지 설치 중...\")\n",
    "install_packages()\n",
    "print(\"✅ 환경 설정 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c051c804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 라이브러리 로드 완료\n",
      "\n",
      "🔥 디바이스 설정 중...\n",
      "🖥️ CPU 모드로 실행\n",
      "🎯 최종 설정: CPU, 배치 크기: 4\n"
     ]
    }
   ],
   "source": [
    "# 📚 2단계: 라이브러리 임포트 및 GPU 설정\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "import codecs\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"📚 라이브러리 로드 완료\")\n",
    "\n",
    "# GPU/CPU 자동 감지\n",
    "print(\"\\n🔥 디바이스 설정 중...\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    batch_size = 16\n",
    "    print(f\"✅ GPU 감지: {device_name}\")\n",
    "\n",
    "    # GPU 메모리 확인\n",
    "    memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"📊 GPU 메모리: {memory_gb:.1f}GB\")\n",
    "\n",
    "    # 배치 크기 조정\n",
    "    if memory_gb >= 8:\n",
    "        batch_size = 32\n",
    "    elif memory_gb >= 4:\n",
    "        batch_size = 16\n",
    "    else:\n",
    "        batch_size = 8\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_name = \"CPU\"\n",
    "    batch_size = 4\n",
    "    print(\"🖥️ CPU 모드로 실행\")\n",
    "\n",
    "print(f\"🎯 최종 설정: {device_name}, 배치 크기: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "453869b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 SBERT 모델 로드 중...\n",
      "========================================\n",
      "✅ sentence_transformers 임포트 성공\n",
      "🔄 KoBERT 모델 다운로드 및 로드 중... (최초 실행시 시간 소요)\n",
      "✅ sentence_transformers 임포트 성공\n",
      "🔄 KoBERT 모델 다운로드 및 로드 중... (최초 실행시 시간 소요)\n",
      "🧪 모델 테스트 중...\n",
      "✅ SBERT 모델 로드 성공!\n",
      "   모델: snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
      "   디바이스: cpu\n",
      "   테스트 유사도: 0.3059\n",
      "\n",
      "🎯 SBERT 사용: True\n",
      "========================================\n",
      "🧪 모델 테스트 중...\n",
      "✅ SBERT 모델 로드 성공!\n",
      "   모델: snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
      "   디바이스: cpu\n",
      "   테스트 유사도: 0.3059\n",
      "\n",
      "🎯 SBERT 사용: True\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# 🤖 3단계: SBERT 모델 로드\n",
    "print(\"🤖 SBERT 모델 로드 중...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "    print(\"✅ sentence_transformers 임포트 성공\")\n",
    "\n",
    "    # KoBERT 모델 로드\n",
    "    print(\"🔄 KoBERT 모델 다운로드 및 로드 중... (최초 실행시 시간 소요)\")\n",
    "    model = SentenceTransformer(\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 테스트\n",
    "    print(\"🧪 모델 테스트 중...\")\n",
    "    test_sentences = [\"안녕하세요\", \"좋은 하루 되세요\"]\n",
    "    test_embeddings = model.encode(test_sentences, convert_to_tensor=True)\n",
    "    test_similarity = util.pytorch_cos_sim(test_embeddings[0], test_embeddings[1])\n",
    "\n",
    "    print(f\"✅ SBERT 모델 로드 성공!\")\n",
    "    print(f\"   모델: snunlp/KR-SBERT-V40K-klueNLI-augSTS\")\n",
    "    print(f\"   디바이스: {test_embeddings.device}\")\n",
    "    print(f\"   테스트 유사도: {test_similarity.item():.4f}\")\n",
    "\n",
    "    USE_SBERT = True\n",
    "\n",
    "    # 메모리 정리\n",
    "    del test_embeddings, test_similarity\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ SBERT 로드 실패: {e}\")\n",
    "    print(\"💡 TF-IDF + Jaccard + 키워드 유사도로 평가 진행\")\n",
    "    USE_SBERT = False\n",
    "    model = None\n",
    "\n",
    "print(f\"\\n🎯 SBERT 사용: {USE_SBERT}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "554852ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 데이터 로드 및 전처리\n",
      "========================================\n",
      "📂 원본 파일: data\\crawling_origin.csv\n",
      "📂 요약 파일: data\\crawling_origin_with_summary.csv\n",
      "\n",
      "🔄 데이터 로드 중...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 원본 데이터: 10,952개 행\n",
      "📊 요약 데이터: 9,066개 행\n",
      "\n",
      "📋 감지된 컬럼:\n",
      "   원본: '본문'\n",
      "   요약: '요약문'\n",
      "\n",
      "🎯 처리 가능한 데이터: 9,066개 쌍\n",
      "✅ 데이터 로드 완료!\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# 📁 4단계: 데이터 로드 및 전처리\n",
    "print(\"📁 데이터 로드 및 전처리\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "\n",
    "# 안전한 CSV 읽기 함수\n",
    "def safe_read_csv(file_path):\n",
    "    \"\"\"BOM 처리가 포함된 안전한 CSV 읽기\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            raw_data = f.read()\n",
    "        if raw_data.startswith(codecs.BOM_UTF8):\n",
    "            raw_data = raw_data[len(codecs.BOM_UTF8) :]\n",
    "        content = raw_data.decode(\"utf-8\", errors=\"replace\")\n",
    "        return pd.read_csv(StringIO(content))\n",
    "    except:\n",
    "        for encoding in [\"utf-8-sig\", \"utf-8\", \"cp949\", \"euc-kr\"]:\n",
    "            try:\n",
    "                return pd.read_csv(file_path, encoding=encoding)\n",
    "            except:\n",
    "                continue\n",
    "        raise ValueError(f\"CSV 파일 읽기 실패: {file_path}\")\n",
    "\n",
    "\n",
    "# 최적 컬럼 감지 함수\n",
    "def detect_best_column(df, keywords):\n",
    "    \"\"\"키워드 기반 최적 컬럼 자동 감지\"\"\"\n",
    "    for keyword in keywords:\n",
    "        for col in df.columns:\n",
    "            if keyword in col:\n",
    "                return col\n",
    "\n",
    "    # 키워드 매칭 실패시 가장 긴 텍스트 컬럼 선택\n",
    "    text_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            avg_len = df[col].astype(str).str.len().mean()\n",
    "            if avg_len > 50:\n",
    "                text_cols.append((col, avg_len))\n",
    "\n",
    "    if text_cols:\n",
    "        return max(text_cols, key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        return df.columns[0]\n",
    "\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def preprocess_text(text):\n",
    "    \"\"\"텍스트 정규화 및 전처리\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # 공백 정규화\n",
    "    text = re.sub(r\"[^\\w\\s가-힣]\", \"\", text)  # 특수문자 제거\n",
    "    return text\n",
    "\n",
    "\n",
    "# 파일 로드\n",
    "data_folder = \"data\"\n",
    "original_file = os.path.join(data_folder, \"crawling_origin.csv\")\n",
    "summary_file = os.path.join(data_folder, \"crawling_origin_with_summary.csv\")\n",
    "\n",
    "print(f\"📂 원본 파일: {original_file}\")\n",
    "print(f\"📂 요약 파일: {summary_file}\")\n",
    "\n",
    "if os.path.exists(original_file) and os.path.exists(summary_file):\n",
    "    # 데이터 로드\n",
    "    print(\"\\n🔄 데이터 로드 중...\")\n",
    "    original_df = safe_read_csv(original_file)\n",
    "    summary_df = safe_read_csv(summary_file)\n",
    "\n",
    "    print(f\"📊 원본 데이터: {len(original_df):,}개 행\")\n",
    "    print(f\"📊 요약 데이터: {len(summary_df):,}개 행\")\n",
    "\n",
    "    # 컬럼 감지\n",
    "    original_col = detect_best_column(\n",
    "        original_df, [\"본문\", \"content\", \"text\", \"article\"]\n",
    "    )\n",
    "    summary_col = detect_best_column(\n",
    "        summary_df, [\"요약문\", \"요약\", \"summary\", \"abstract\"]\n",
    "    )\n",
    "\n",
    "    print(f\"\\n📋 감지된 컬럼:\")\n",
    "    print(f\"   원본: '{original_col}'\")\n",
    "    print(f\"   요약: '{summary_col}'\")\n",
    "\n",
    "    # 처리 가능한 데이터 크기\n",
    "    min_len = min(len(original_df), len(summary_df))\n",
    "    print(f\"\\n🎯 처리 가능한 데이터: {min_len:,}개 쌍\")\n",
    "\n",
    "    print(\"✅ 데이터 로드 완료!\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 데이터 파일을 찾을 수 없습니다!\")\n",
    "    print(f\"   원본: {'존재' if os.path.exists(original_file) else '없음'}\")\n",
    "    print(f\"   요약: {'존재' if os.path.exists(summary_file) else '없음'}\")\n",
    "    raise FileNotFoundError(\"필요한 CSV 파일이 없습니다.\")\n",
    "\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4ed4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 유사도 계산 함수 정의\n",
      "========================================\n",
      "✅ 유사도 계산 함수 준비 완료!\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# 🧮 5단계: 유사도 계산 함수 정의\n",
    "print(\"🧮 유사도 계산 함수 정의\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "\n",
    "def calculate_sbert_similarity(originals, summaries, batch_size=16):\n",
    "    \"\"\"SBERT 유사도 계산 (GPU/CPU 자동 최적화)\"\"\"\n",
    "    if not USE_SBERT or model is None:\n",
    "        print(\"⚠️ SBERT 모델이 없어 0으로 대체\")\n",
    "        return np.zeros(len(originals))\n",
    "\n",
    "    print(f\"🚀 SBERT 유사도 계산 시작 ({device})\")\n",
    "    similarities = []\n",
    "    total_batches = (len(originals) + batch_size - 1) // batch_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(\n",
    "            range(0, len(originals), batch_size), desc=f\"SBERT {device.type.upper()}\"\n",
    "        ):\n",
    "            batch_orig = originals[i : i + batch_size]\n",
    "            batch_summ = summaries[i : i + batch_size]\n",
    "\n",
    "            try:\n",
    "                orig_embeddings = model.encode(\n",
    "                    batch_orig, convert_to_tensor=True, device=device\n",
    "                )\n",
    "                summ_embeddings = model.encode(\n",
    "                    batch_summ, convert_to_tensor=True, device=device\n",
    "                )\n",
    "\n",
    "                batch_similarities = util.pytorch_cos_sim(\n",
    "                    orig_embeddings, summ_embeddings\n",
    "                ).diag()\n",
    "                similarities.extend(batch_similarities.cpu().numpy())\n",
    "\n",
    "                # 메모리 정리\n",
    "                del orig_embeddings, summ_embeddings, batch_similarities\n",
    "                if device.type == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 배치 {i//batch_size + 1} 실패: {e}\")\n",
    "                similarities.extend([0.0] * len(batch_orig))\n",
    "\n",
    "    return np.array(similarities)\n",
    "\n",
    "\n",
    "def calculate_tfidf_similarity(originals, summaries):\n",
    "    \"\"\"TF-IDF 유사도 계산\"\"\"\n",
    "    print(\"🔄 TF-IDF 유사도 계산 중...\")\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "        all_texts = originals + summaries\n",
    "        tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "        orig_matrix = tfidf_matrix[: len(originals)]\n",
    "        summ_matrix = tfidf_matrix[len(originals) :]\n",
    "\n",
    "        similarities = []\n",
    "        for i in tqdm(range(len(originals)), desc=\"TF-IDF\"):\n",
    "            sim = cosine_similarity(orig_matrix[i], summ_matrix[i])[0][0]\n",
    "            similarities.append(sim)\n",
    "\n",
    "        return np.array(similarities)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ TF-IDF 실패: {e}\")\n",
    "        return np.zeros(len(originals))\n",
    "\n",
    "\n",
    "def calculate_jaccard_similarity(originals, summaries):\n",
    "    \"\"\"Jaccard 유사도 계산\"\"\"\n",
    "    print(\"🔄 Jaccard 유사도 계산 중...\")\n",
    "    similarities = []\n",
    "    for orig, summ in tqdm(\n",
    "        zip(originals, summaries), total=len(originals), desc=\"Jaccard\"\n",
    "    ):\n",
    "        orig_words = set(orig.split())\n",
    "        summ_words = set(summ.split())\n",
    "\n",
    "        if len(orig_words) == 0 and len(summ_words) == 0:\n",
    "            similarities.append(1.0)\n",
    "        elif len(orig_words) == 0 or len(summ_words) == 0:\n",
    "            similarities.append(0.0)\n",
    "        else:\n",
    "            intersection = len(orig_words & summ_words)\n",
    "            union = len(orig_words | summ_words)\n",
    "            similarities.append(intersection / union)\n",
    "\n",
    "    return np.array(similarities)\n",
    "\n",
    "\n",
    "def calculate_keyword_similarity(originals, summaries, top_k=10):\n",
    "    \"\"\"키워드 기반 유사도 계산\"\"\"\n",
    "    print(\"🔄 키워드 유사도 계산 중...\")\n",
    "    similarities = []\n",
    "    for orig, summ in tqdm(\n",
    "        zip(originals, summaries), total=len(originals), desc=\"키워드\"\n",
    "    ):\n",
    "        orig_words = Counter(orig.split())\n",
    "        summ_words = Counter(summ.split())\n",
    "\n",
    "        orig_top = set([word for word, _ in orig_words.most_common(top_k)])\n",
    "        summ_top = set([word for word, _ in summ_words.most_common(top_k)])\n",
    "\n",
    "        if len(orig_top) == 0 and len(summ_top) == 0:\n",
    "            similarities.append(1.0)\n",
    "        elif len(orig_top) == 0 or len(summ_top) == 0:\n",
    "            similarities.append(0.0)\n",
    "        else:\n",
    "            intersection = len(orig_top & summ_top)\n",
    "            union = len(orig_top | summ_top)\n",
    "            similarities.append(intersection / union)\n",
    "\n",
    "    return np.array(similarities)\n",
    "\n",
    "\n",
    "print(\"✅ 유사도 계산 함수 준비 완료!\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa2db79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 전체 데이터 평가 실행\n",
      "============================================================\n",
      "🔄 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8001.59it/s]\n",
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8001.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 전처리 결과:\n",
      "   ✅ 유효 데이터: 9,022개\n",
      "   ⚠️ 제외 데이터: 44개\n",
      "   📈 유효율: 99.5%\n",
      "\n",
      "🧮 9,022개 데이터 유사도 계산 시작...\n",
      "==================================================\n",
      "🚀 SBERT 유사도 계산 시작 (cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT CPU: 100%|██████████| 2256/2256 [14:26<00:00,  2.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 전체 데이터 평가 실행\n",
      "============================================================\n",
      "🔄 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8001.59it/s]\n",
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8001.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 전처리 결과:\n",
      "   ✅ 유효 데이터: 9,022개\n",
      "   ⚠️ 제외 데이터: 44개\n",
      "   📈 유효율: 99.5%\n",
      "\n",
      "🧮 9,022개 데이터 유사도 계산 시작...\n",
      "==================================================\n",
      "🚀 SBERT 유사도 계산 시작 (cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT CPU: 100%|██████████| 2256/2256 [14:26<00:00,  2.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SBERT 평균: 0.3085\n",
      "🔄 TF-IDF 유사도 계산 중...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 전체 데이터 평가 실행\n",
      "============================================================\n",
      "🔄 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8001.59it/s]\n",
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8001.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 전처리 결과:\n",
      "   ✅ 유효 데이터: 9,022개\n",
      "   ⚠️ 제외 데이터: 44개\n",
      "   📈 유효율: 99.5%\n",
      "\n",
      "🧮 9,022개 데이터 유사도 계산 시작...\n",
      "==================================================\n",
      "🚀 SBERT 유사도 계산 시작 (cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT CPU: 100%|██████████| 2256/2256 [14:26<00:00,  2.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SBERT 평균: 0.3085\n",
      "🔄 TF-IDF 유사도 계산 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF: 100%|██████████| 9022/9022 [00:04<00:00, 1898.09it/s]\n",
      "TF-IDF: 100%|██████████| 9022/9022 [00:04<00:00, 1898.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 전체 데이터 평가 실행\n",
      "============================================================\n",
      "🔄 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8001.59it/s]\n",
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8001.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 전처리 결과:\n",
      "   ✅ 유효 데이터: 9,022개\n",
      "   ⚠️ 제외 데이터: 44개\n",
      "   📈 유효율: 99.5%\n",
      "\n",
      "🧮 9,022개 데이터 유사도 계산 시작...\n",
      "==================================================\n",
      "🚀 SBERT 유사도 계산 시작 (cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT CPU: 100%|██████████| 2256/2256 [14:26<00:00,  2.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SBERT 평균: 0.3085\n",
      "🔄 TF-IDF 유사도 계산 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF: 100%|██████████| 9022/9022 [00:04<00:00, 1898.09it/s]\n",
      "TF-IDF: 100%|██████████| 9022/9022 [00:04<00:00, 1898.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TF-IDF 평균: 0.0271\n",
      "🔄 Jaccard 유사도 계산 중...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 전체 데이터 평가 실행\n",
      "============================================================\n",
      "🔄 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8001.59it/s]\n",
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8001.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 전처리 결과:\n",
      "   ✅ 유효 데이터: 9,022개\n",
      "   ⚠️ 제외 데이터: 44개\n",
      "   📈 유효율: 99.5%\n",
      "\n",
      "🧮 9,022개 데이터 유사도 계산 시작...\n",
      "==================================================\n",
      "🚀 SBERT 유사도 계산 시작 (cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT CPU: 100%|██████████| 2256/2256 [14:26<00:00,  2.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SBERT 평균: 0.3085\n",
      "🔄 TF-IDF 유사도 계산 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF: 100%|██████████| 9022/9022 [00:04<00:00, 1898.09it/s]\n",
      "TF-IDF: 100%|██████████| 9022/9022 [00:04<00:00, 1898.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TF-IDF 평균: 0.0271\n",
      "🔄 Jaccard 유사도 계산 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jaccard: 100%|██████████| 9022/9022 [00:00<00:00, 91510.83it/s]\n",
      "Jaccard: 100%|██████████| 9022/9022 [00:00<00:00, 91510.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 전체 데이터 평가 실행\n",
      "============================================================\n",
      "🔄 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8001.59it/s]\n",
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8001.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 전처리 결과:\n",
      "   ✅ 유효 데이터: 9,022개\n",
      "   ⚠️ 제외 데이터: 44개\n",
      "   📈 유효율: 99.5%\n",
      "\n",
      "🧮 9,022개 데이터 유사도 계산 시작...\n",
      "==================================================\n",
      "🚀 SBERT 유사도 계산 시작 (cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT CPU: 100%|██████████| 2256/2256 [14:26<00:00,  2.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SBERT 평균: 0.3085\n",
      "🔄 TF-IDF 유사도 계산 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF: 100%|██████████| 9022/9022 [00:04<00:00, 1898.09it/s]\n",
      "TF-IDF: 100%|██████████| 9022/9022 [00:04<00:00, 1898.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TF-IDF 평균: 0.0271\n",
      "🔄 Jaccard 유사도 계산 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jaccard: 100%|██████████| 9022/9022 [00:00<00:00, 91510.83it/s]\n",
      "Jaccard: 100%|██████████| 9022/9022 [00:00<00:00, 91510.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Jaccard 평균: 0.0129\n",
      "🔄 키워드 유사도 계산 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "키워드: 100%|██████████| 9022/9022 [00:00<00:00, 35190.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 전체 데이터 평가 실행\n",
      "============================================================\n",
      "🔄 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8001.59it/s]\n",
      "전처리: 100%|██████████| 9066/9066 [00:01<00:00, 8001.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 전처리 결과:\n",
      "   ✅ 유효 데이터: 9,022개\n",
      "   ⚠️ 제외 데이터: 44개\n",
      "   📈 유효율: 99.5%\n",
      "\n",
      "🧮 9,022개 데이터 유사도 계산 시작...\n",
      "==================================================\n",
      "🚀 SBERT 유사도 계산 시작 (cpu)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SBERT CPU: 100%|██████████| 2256/2256 [14:26<00:00,  2.60it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SBERT 평균: 0.3085\n",
      "🔄 TF-IDF 유사도 계산 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF: 100%|██████████| 9022/9022 [00:04<00:00, 1898.09it/s]\n",
      "TF-IDF: 100%|██████████| 9022/9022 [00:04<00:00, 1898.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TF-IDF 평균: 0.0271\n",
      "🔄 Jaccard 유사도 계산 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jaccard: 100%|██████████| 9022/9022 [00:00<00:00, 91510.83it/s]\n",
      "Jaccard: 100%|██████████| 9022/9022 [00:00<00:00, 91510.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Jaccard 평균: 0.0129\n",
      "🔄 키워드 유사도 계산 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "키워드: 100%|██████████| 9022/9022 [00:00<00:00, 35190.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 키워드 평균: 0.0152\n",
      "\n",
      "==================================================\n",
      "🎉 모든 유사도 계산 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 🚀 6단계: 전체 데이터 평가 실행\n",
    "print(\"🚀 전체 데이터 평가 실행\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 데이터 전처리\n",
    "print(\"🔄 데이터 전처리 중...\")\n",
    "originals = []\n",
    "summaries = []\n",
    "valid_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "for i in tqdm(range(min_len), desc=\"전처리\"):\n",
    "    try:\n",
    "        orig_text = preprocess_text(original_df.iloc[i][original_col])\n",
    "        summ_text = preprocess_text(summary_df.iloc[i][summary_col])\n",
    "\n",
    "        if len(orig_text) >= 10 and len(summ_text) >= 5:\n",
    "            originals.append(orig_text)\n",
    "            summaries.append(summ_text)\n",
    "            valid_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "    except:\n",
    "        skipped_count += 1\n",
    "\n",
    "print(f\"\\n📊 전처리 결과:\")\n",
    "print(f\"   ✅ 유효 데이터: {valid_count:,}개\")\n",
    "print(f\"   ⚠️ 제외 데이터: {skipped_count:,}개\")\n",
    "print(f\"   📈 유효율: {valid_count/(valid_count+skipped_count)*100:.1f}%\")\n",
    "\n",
    "if valid_count == 0:\n",
    "    raise ValueError(\"유효한 데이터가 없습니다!\")\n",
    "\n",
    "# 유사도 계산\n",
    "print(f\"\\n🧮 {valid_count:,}개 데이터 유사도 계산 시작...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. SBERT 유사도\n",
    "sbert_scores = calculate_sbert_similarity(originals, summaries, batch_size)\n",
    "print(f\"✅ SBERT 평균: {np.mean(sbert_scores):.4f}\")\n",
    "\n",
    "# 2. TF-IDF 유사도\n",
    "tfidf_scores = calculate_tfidf_similarity(originals, summaries)\n",
    "print(f\"✅ TF-IDF 평균: {np.mean(tfidf_scores):.4f}\")\n",
    "\n",
    "# 3. Jaccard 유사도\n",
    "jaccard_scores = calculate_jaccard_similarity(originals, summaries)\n",
    "print(f\"✅ Jaccard 평균: {np.mean(jaccard_scores):.4f}\")\n",
    "\n",
    "# 4. 키워드 유사도\n",
    "keyword_scores = calculate_keyword_similarity(originals, summaries)\n",
    "print(f\"✅ 키워드 평균: {np.mean(keyword_scores):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"🎉 모든 유사도 계산 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa91a296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 결과 분석 및 종합 점수 계산\n",
      "==================================================\n",
      "📈 상세 통계:\n",
      "----------------------------------------------------------------------\n",
      "방법           평균       표준편차     최소값      최대값      중간값     \n",
      "----------------------------------------------------------------------\n",
      "SBERT        0.3085   0.1164   -0.0532  0.8945   0.3029  \n",
      "TF-IDF       0.0271   0.0618   0.0000   0.8519   0.0000  \n",
      "Jaccard      0.0129   0.0267   0.0000   0.6538   0.0000  \n",
      "Keyword      0.0152   0.0380   0.0000   1.0000   0.0000  \n",
      "Composite    0.1356   0.0617   -0.0213  0.7733   0.1290  \n",
      "\n",
      "🏆 성능 등급 분포 (총 9,022개):\n",
      "   🥇 우수 (≥0.80): 0개 (  0.0%)\n",
      "   🥈 양호 (0.60-0.80): 10개 (  0.1%)\n",
      "   🥉 보통 (0.40-0.60): 21개 (  0.2%)\n",
      "   📉 미흡 (<0.40): 8,991개 ( 99.7%)\n",
      "\n",
      "🎯 성능 샘플:\n",
      "   🏆 최고 점수: 0.7733\n",
      "      원본: 이재명 대통령은 국회에 강선우 여성가족부 장관 후보자를 포함해 국방부와 국가보훈부 통일부 장관 후보자에 대한 인사청문보고서를 내일까지 재송부해 달라고 요청했습니다 강유정 대통령실 ...\n",
      "      요약: 이재명 이재명 대통령은 국회에 강선우 여성가족부 장관 후보자를 포함해 국방부와 국가보훈부 통일부 장관 후보자에 대한 인사청문보고서를 내일까지 재송부해 달라고 요청했습니다...\n",
      "\n",
      "   📉 최저 점수: -0.0213\n",
      "      원본:  앵커  이재명 대통령이 주로 장관급 국무위원들이 참석해왔던 국무회의 관행에 큰 변화를 주기로 했습니다 이제 외교부나 교육부 같은 19개 부처 뿐만아니라 경찰청 산림청 등 외청 2...\n",
      "      요약: 조국혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신혁신...\n",
      "\n",
      "🎯 전체 평가 결과:\n",
      "   📊 종합 평균 점수: 0.1356\n",
      "   🏆 성능 등급: D (미흡)\n",
      "   💡 평가: 성능 개선이 시급합니다.\n",
      "\n",
      "==================================================\n",
      "✅ 결과 분석 완료!\n"
     ]
    }
   ],
   "source": [
    "# 📊 7단계: 결과 분석 및 종합 점수 계산\n",
    "print(\"📊 결과 분석 및 종합 점수 계산\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 변수 존재 확인 및 기본값 설정\n",
    "if \"sbert_scores\" not in globals():\n",
    "    print(\"⚠️ sbert_scores가 정의되지 않음 - 6단계를 먼저 실행해주세요\")\n",
    "    print(\"🔄 기본 더미 데이터로 진행...\")\n",
    "    sbert_scores = np.array([0.0] * 100)  # 더미 데이터\n",
    "    tfidf_scores = np.array([0.0] * 100)\n",
    "    jaccard_scores = np.array([0.0] * 100)\n",
    "    keyword_scores = np.array([0.0] * 100)\n",
    "    originals = [\"더미 원본 텍스트\"] * 100\n",
    "    summaries = [\"더미 요약 텍스트\"] * 100\n",
    "    valid_count = 100\n",
    "\n",
    "# 가중치 설정\n",
    "weights = {\n",
    "    \"sbert\": 0.4,  # KoBERT가 가장 중요\n",
    "    \"tfidf\": 0.3,  # TF-IDF도 중요\n",
    "    \"jaccard\": 0.2,  # Jaccard는 보조\n",
    "    \"keyword\": 0.1,  # 키워드는 참고용\n",
    "}\n",
    "\n",
    "# 종합 점수 계산\n",
    "composite_scores = (\n",
    "    weights[\"sbert\"] * sbert_scores\n",
    "    + weights[\"tfidf\"] * tfidf_scores\n",
    "    + weights[\"jaccard\"] * jaccard_scores\n",
    "    + weights[\"keyword\"] * keyword_scores\n",
    ")\n",
    "\n",
    "# 통계 계산\n",
    "results = {\n",
    "    \"SBERT\": sbert_scores,\n",
    "    \"TF-IDF\": tfidf_scores,\n",
    "    \"Jaccard\": jaccard_scores,\n",
    "    \"Keyword\": keyword_scores,\n",
    "    \"Composite\": composite_scores,\n",
    "}\n",
    "\n",
    "print(\"📈 상세 통계:\")\n",
    "print(\"-\" * 70)\n",
    "print(\n",
    "    f\"{'방법':<12} {'평균':<8} {'표준편차':<8} {'최소값':<8} {'최대값':<8} {'중간값':<8}\"\n",
    ")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for method, scores in results.items():\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    min_score = np.min(scores)\n",
    "    max_score = np.max(scores)\n",
    "    median_score = np.median(scores)\n",
    "\n",
    "    print(\n",
    "        f\"{method:<12} {mean_score:<8.4f} {std_score:<8.4f} {min_score:<8.4f} {max_score:<8.4f} {median_score:<8.4f}\"\n",
    "    )\n",
    "\n",
    "# 성능 등급 분석\n",
    "best_scores = composite_scores\n",
    "excellent = np.sum(best_scores >= 0.80)\n",
    "good = np.sum((best_scores >= 0.60) & (best_scores < 0.80))\n",
    "average = np.sum((best_scores >= 0.40) & (best_scores < 0.60))\n",
    "poor = np.sum(best_scores < 0.40)\n",
    "\n",
    "print(f\"\\n🏆 성능 등급 분포 (총 {len(best_scores):,}개):\")\n",
    "print(f\"   🥇 우수 (≥0.80): {excellent:,}개 ({excellent/len(best_scores)*100:5.1f}%)\")\n",
    "print(f\"   🥈 양호 (0.60-0.80): {good:,}개 ({good/len(best_scores)*100:5.1f}%)\")\n",
    "print(f\"   🥉 보통 (0.40-0.60): {average:,}개 ({average/len(best_scores)*100:5.1f}%)\")\n",
    "print(f\"   📉 미흡 (<0.40): {poor:,}개 ({poor/len(best_scores)*100:5.1f}%)\")\n",
    "\n",
    "# 최고/최저 성능 샘플\n",
    "best_idx = np.argmax(best_scores)\n",
    "worst_idx = np.argmin(best_scores)\n",
    "\n",
    "print(f\"\\n🎯 성능 샘플:\")\n",
    "print(f\"   🏆 최고 점수: {best_scores[best_idx]:.4f}\")\n",
    "print(f\"      원본: {originals[best_idx][:100]}...\")\n",
    "print(f\"      요약: {summaries[best_idx][:100]}...\")\n",
    "\n",
    "print(f\"\\n   📉 최저 점수: {best_scores[worst_idx]:.4f}\")\n",
    "print(f\"      원본: {originals[worst_idx][:100]}...\")\n",
    "print(f\"      요약: {summaries[worst_idx][:100]}...\")\n",
    "\n",
    "# 전체 평가 결과\n",
    "avg_score = np.mean(best_scores)\n",
    "print(f\"\\n🎯 전체 평가 결과:\")\n",
    "print(f\"   📊 종합 평균 점수: {avg_score:.4f}\")\n",
    "\n",
    "if avg_score >= 0.70:\n",
    "    grade = \"A (우수)\"\n",
    "    comment = \"매우 높은 품질의 요약 성능을 보입니다!\"\n",
    "elif avg_score >= 0.60:\n",
    "    grade = \"B (양호)\"\n",
    "    comment = \"양호한 요약 성능입니다. 추가 개선 여지가 있습니다.\"\n",
    "elif avg_score >= 0.50:\n",
    "    grade = \"C (보통)\"\n",
    "    comment = \"평균적인 성능입니다. 개선이 필요합니다.\"\n",
    "else:\n",
    "    grade = \"D (미흡)\"\n",
    "    comment = \"성능 개선이 시급합니다.\"\n",
    "\n",
    "print(f\"   🏆 성능 등급: {grade}\")\n",
    "print(f\"   💡 평가: {comment}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"✅ 결과 분석 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e11eaf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 평가 결과 저장\n",
      "========================================\n",
      "✅ 상세 결과 저장: kobert_complete_results_20250805_134036.csv\n",
      "✅ 요약 통계 저장: kobert_complete_summary_20250805_134036.txt\n",
      "\n",
      "🎉 전체 평가 프로세스 완료!\n",
      "📁 결과 파일: kobert_complete_results_20250805_134036.csv\n",
      "📋 요약 파일: kobert_complete_summary_20250805_134036.txt\n",
      "🏆 최종 점수: 0.1356 (D (미흡))\n",
      "📊 평가 데이터: 9,022개\n",
      "🎯 SBERT 사용: ✅\n",
      "🖥️ 디바이스: CPU\n",
      "============================================================\n",
      "🎊 모든 과정이 성공적으로 완료되었습니다!\n",
      "\n",
      "🎉 전체 평가 프로세스 완료!\n",
      "📁 결과 파일: kobert_complete_results_20250805_134036.csv\n",
      "📋 요약 파일: kobert_complete_summary_20250805_134036.txt\n",
      "🏆 최종 점수: 0.1356 (D (미흡))\n",
      "📊 평가 데이터: 9,022개\n",
      "🎯 SBERT 사용: ✅\n",
      "🖥️ 디바이스: CPU\n",
      "============================================================\n",
      "🎊 모든 과정이 성공적으로 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "# 💾 8단계: 결과 저장\n",
    "print(\"💾 평가 결과 저장\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 결과 데이터프레임 생성\n",
    "results_data = {\n",
    "    \"original_text\": [\n",
    "        text[:200] + \"...\" if len(text) > 200 else text for text in originals\n",
    "    ],\n",
    "    \"summary_text\": [\n",
    "        text[:200] + \"...\" if len(text) > 200 else text for text in summaries\n",
    "    ],\n",
    "    \"sbert_score\": sbert_scores,\n",
    "    \"tfidf_score\": tfidf_scores,\n",
    "    \"jaccard_score\": jaccard_scores,\n",
    "    \"keyword_score\": keyword_scores,\n",
    "    \"composite_score\": composite_scores,\n",
    "    \"original_length\": [len(text) for text in originals],\n",
    "    \"summary_length\": [len(text) for text in summaries],\n",
    "    \"compression_ratio\": [len(s) / len(o) * 100 for o, s in zip(originals, summaries)],\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# 파일명 생성\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"kobert_complete_results_{timestamp}.csv\"\n",
    "summary_file = f\"kobert_complete_summary_{timestamp}.txt\"\n",
    "\n",
    "# CSV 저장\n",
    "results_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ 상세 결과 저장: {output_file}\")\n",
    "\n",
    "# 요약 통계 저장\n",
    "with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"KoBERT 요약 성능 평가 결과 (완전 자동화)\\n\")\n",
    "    f.write(f\"=\" * 50 + \"\\n\")\n",
    "    f.write(f\"평가 일시: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"평가 데이터: {valid_count:,}개\\n\")\n",
    "    f.write(f\"디바이스: {device_name}\\n\")\n",
    "    f.write(f\"SBERT 사용: {USE_SBERT}\\n\")\n",
    "    f.write(f\"\\n성능 통계:\\n\")\n",
    "    f.write(f\"  SBERT 평균: {np.mean(sbert_scores):.4f}\\n\")\n",
    "    f.write(f\"  TF-IDF 평균: {np.mean(tfidf_scores):.4f}\\n\")\n",
    "    f.write(f\"  Jaccard 평균: {np.mean(jaccard_scores):.4f}\\n\")\n",
    "    f.write(f\"  키워드 평균: {np.mean(keyword_scores):.4f}\\n\")\n",
    "    f.write(f\"  종합 평균: {avg_score:.4f}\\n\")\n",
    "    f.write(f\"\\n성능 등급: {grade}\\n\")\n",
    "    f.write(f\"평가: {comment}\\n\")\n",
    "    f.write(f\"\\n등급 분포:\\n\")\n",
    "    f.write(\n",
    "        f\"  우수 (≥0.80): {excellent:,}개 ({excellent/len(best_scores)*100:5.1f}%)\\n\"\n",
    "    )\n",
    "    f.write(f\"  양호 (0.60-0.80): {good:,}개 ({good/len(best_scores)*100:5.1f}%)\\n\")\n",
    "    f.write(\n",
    "        f\"  보통 (0.40-0.60): {average:,}개 ({average/len(best_scores)*100:5.1f}%)\\n\"\n",
    "    )\n",
    "    f.write(f\"  미흡 (<0.40): {poor:,}개 ({poor/len(best_scores)*100:5.1f}%)\\n\")\n",
    "\n",
    "print(f\"✅ 요약 통계 저장: {summary_file}\")\n",
    "\n",
    "# 메모리 정리\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n🎉 전체 평가 프로세스 완료!\")\n",
    "print(f\"📁 결과 파일: {output_file}\")\n",
    "print(f\"📋 요약 파일: {summary_file}\")\n",
    "print(f\"🏆 최종 점수: {avg_score:.4f} ({grade})\")\n",
    "print(f\"📊 평가 데이터: {valid_count:,}개\")\n",
    "print(f\"🎯 SBERT 사용: {'✅' if USE_SBERT else '❌'}\")\n",
    "print(f\"🖥️ 디바이스: {device_name}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"🎊 모든 과정이 성공적으로 완료되었습니다!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
